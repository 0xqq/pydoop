#!/usr/bin/env bash

set -euo pipefail
[ -n "${DEBUG:-}" ] && set -x
this="${BASH_SOURCE-$0}"
this_dir=$(cd -P -- "$(dirname -- "${this}")" && pwd -P)
. "${this_dir}/../config.sh"

MODULE="wordcount_minimal"
MZIP="${MODULE}".zip
MPY="${MODULE}".py
JOBNAME="${MODULE}"-job
INPUT="${MODULE}"_input
OUTPUT="${MODULE}"_output
DATA="${this_dir}"/../input/alice.txt
RESULTS="results.txt"
LOGLEVEL="DEBUG"

zip -j "${MZIP}" "${this_dir}/../wordcount/new_api/${MPY}"

"${HADOOP}" fs -rmr "/user/${USER}/${INPUT}" || :
"${HADOOP}" fs -mkdir -p "/user/${USER}/${INPUT}"
"${HADOOP}" fs -rmr "/user/${USER}/${OUTPUT}" || :
"${HADOOP}" fs -put "${DATA}" "${INPUT}"

${PYDOOP} submit \
  --python-zip "${MZIP}" \
  --python-program "${PYTHON}" \
 -D mapreduce.pipes.isjavarecordreader=true \
 -D mapreduce.pipes.isjavarecordwriter=true \
 -D pydoop.hdfs.user="${USER}" \
 --entry-point main \
 --log-level "${LOGLEVEL}" \
 --job-name "${JOBNAME}" \
 "${MODULE}" "${INPUT}" "${OUTPUT}"

${HADOOP} fs -cat "${OUTPUT}"/part'*' > "${RESULTS}"

python <<EOF
import pydoop.test_support as pts

def check_results(data_in, data_out):
    local_wc = pts.LocalWordCount(data_in)
    print "result is:", local_wc.check(open(data_out).read())

print "Checking results"
check_results("${DATA}", "${RESULTS}")
EOF
