#!/bin/sh

# change the following exports to appropriate values for your environment
""":"
#export PATH="/ELS/els5/acdc/opt/bin:$PATH"
#export LD_LIBRARY_PATH="/ELS/els5/acdc/opt/lib:$LD_LIBRARY_PATH"

#--
export LD_LIBRARY_PATH=${HOME}/svn/ac-dc/lib/pydoop/trunk/src/libhdfs:${LD_LIBRARY_PATH}

HADOOP_ROOT=/opt/hadoop
export CLASSPATH=\
${HADOOP_ROOT}/lib/xmlenc-0.52.jar:\
${HADOOP_ROOT}/lib/slf4j-log4j12-1.4.3.jar:\
${HADOOP_ROOT}/lib/slf4j-api-1.4.3.jar:\
${HADOOP_ROOT}/lib/servlet-api.jar:\
${HADOOP_ROOT}/lib/oro-2.0.8.jar:\
${HADOOP_ROOT}/lib/log4j-1.2.15.jar:\
${HADOOP_ROOT}/lib/kfs-0.2.0.jar:\
${HADOOP_ROOT}/lib/junit-3.8.1.jar:\
${HADOOP_ROOT}/lib/jetty-5.1.4.jar:\
${HADOOP_ROOT}/lib/jets3t-0.6.1.jar:\
${HADOOP_ROOT}/lib/hsqldb-1.8.0.10.jar:\
${HADOOP_ROOT}/lib/commons-net-1.4.1.jar:\
${HADOOP_ROOT}/lib/commons-logging-api-1.0.4.jar:\
${HADOOP_ROOT}/lib/commons-logging-1.0.4.jar:\
${HADOOP_ROOT}/lib/commons-httpclient-3.0.1.jar:\
${HADOOP_ROOT}/lib/commons-codec-1.3.jar:\
${HADOOP_ROOT}/lib/commons-cli-2.0-SNAPSHOT.jar:\
${HADOOP_ROOT}/hadoop-0.19.1-core.jar:\
${HADOOP_ROOT}/hadoop-0.19.1-tools.jar:\
${HADOOP_ROOT}/conf

exec python -u -OO $0 ${1+"@"}
":"""


# FIXME this is a local hack that should go away in the real dist.
import sys
import os
pydoop_path = os.path.join(os.environ['HOME'],
                           'svn/ac-dc/lib/pydoop/trunk/examples')
sys.path.append(pydoop_path)

from pydoop.pipes import Mapper, Reducer, Factory, runTask
from pydoop.hdfs  import hdfs
from pydoop.utils import split_hdfs_path, jc_configure, jc_configure_int


import gap_io

IMAGE_LOADER = 'IMAGE_LOADER'
INPUT_TILES      = 'INPUT_TILES'
OUTPUT_BYTES     = 'OUTPUT_BYTES'
OUTPUT_SLICES    = 'OUTPUT_SLICES'


def log(x):
  sys.stderr.write('%s\n' % x)


import Image
import glob
import os

class load_factory(gap_io.factory):
  @staticmethod
  def cmp_cycles(x, y):
    xr, xn = os.path.split(x)
    yr, yn = os.path.split(y)
    assert xr == yr
    xi = int(xn[1:-2])
    yi = int(yn[1:-2])
    return xi - yi
  #--
  def __make_path(self, lane, tile, cycle, channel):
    path = os.path.join(self.root, 'L%03d/C%d.1/s_%d_%d_%c.tif' % \
                        (lane, cycle, lane, tile, channel))
    return path
  #--
  def __init__(self, root):
    self.root = root
    # this is, of course, a despicable thing to do...
    im = Image.open(self.__make_path(1, 1, 1, 'a'))
    xsize, ysize = im.size
    super(load_factory, self).__init__(xsize, ysize)

  #
  def make(self, lane, tile, cycle):
    im = {}
    for c in 'acgt':
      path  = self.__make_path(lane, tile, cycle, c)
      im[c] = Image.open(path).tostring()
    return super(load_factory, self).make(lane, tile, cycle,
                                          im['a'], im['c'], im['g'], im['t'])

#--------------------------------------------------------------------------
class mapper(Mapper):
  #--
  def __get_configuration(self, jc):
    jc_configure(self, jc,
                 'image.loader.source.directory', 'source_dir',
                 '/ELS/els5/acdc/opt/data/images')
    jc_configure(self, jc,
                 'image.loader.target.directory', 'target_dir',
                 'hdfs://localhost/images')
    jc_configure_int(self, jc, 'image.loader.cycles', 'n_cycles', 4)
    self.hdfs_host, self.hdfs_port, self.hdfs_root = \
                    split_hdfs_path(self.target_dir)

  def __init__(self, task_ctx):
    super(mapper, self).__init__()
    log('mapper::init  start')
    log('mapper::task_ctx = %s' % task_ctx)
    self.input_tiles  = task_ctx.getCounter(IMAGE_LOADER, INPUT_TILES)
    self.output_bytes = task_ctx.getCounter(IMAGE_LOADER, OUTPUT_BYTES)
    #--
    jc = task_ctx.getJobConf()
    self.__get_configuration(jc)
    self.img_factory = load_factory(self.source_dir)

    log('mapper::init source_dir = %s' % self.source_dir)
    log('mapper::init target_dir = %s' % self.target_dir)
    log('mapper::init hdfs_host  = %s' % self.hdfs_host)
    log('mapper::init hdfs_port  = %s' % self.hdfs_port)
    log('mapper::init cycles     = %s' % self.n_cycles)
    log('mapper::init  end')
  #-
  def map(self, ctx):
    log('mapper::map self=%s, ctx=%s' % (self, ctx))
    iv = ctx.getInputValue().strip()
    if not iv:
      log('mapper::map empty record')
    else:
      lane, tile = map(int, ctx.getInputValue().split())
      log('mapper::map lane=%d, tile=%d' % (lane, tile))
      ctx.incrementCounter(self.input_tiles, 1)
      #--
      lane_tile_id = 'L%02dT%03d.gap' % (lane, tile)
      path = os.path.join(self.hdfs_root, lane_tile_id)
      fs = hdfs(self.hdfs_host, self.hdfs_port)
      log('mapper::map opened server (%s:%s' % (self.hdfs_host, self.hdfs_port))
      f = fs.open_file(path, os.O_WRONLY, 0, 0, 0)
      log('mapper::map opened hdfs file=%s' % path)
      try:
        for i in range(1, self.n_cycles+1):
          img = self.img_factory.make(lane, tile, i)
          written = f.write(img.data)
          assert written == len(img.data)
          ctx.incrementCounter(self.output_bytes, written)
          ctx.emit(lane_tile_id, '%d' % 4)
      finally:
        f.close()
        fs.close()
    log('mapper::map end')

class reducer(Reducer):
  def __init__(self, task_ctx):
    super(reducer, self).__init__()
    log('reducer::init  start')
    log('reducer::task_ctx = %s' % task_ctx)
    self.outputSlices = task_ctx.getCounter(IMAGE_LOADER, OUTPUT_SLICES)
  #-
  def reduce(self, red_ctx):
    log('reducer::reduce self=%s, ctx=%s' % (self, red_ctx))
    s = 0
    while red_ctx.nextValue():
      s += 1
    k = red_ctx.getInputKey()
    log('reducer::reduce k=%s, s=%d' % (k, s))
    red_ctx.emit(k, str(s))
    red_ctx.incrementCounter(self.outputSlices, s)

def main(argv):
  log('image_loader started')
  runTask(Factory(mapper, reducer))


if __name__ == "__main__":
  main(sys.argv)

# Local Variables: **
# mode: python **
# End: **
