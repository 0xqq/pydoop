#!/usr/bin/env python


import sys
import os
pydoop_path = os.path.join(os.environ['HOME'],
                           'svn/ac-dc/lib/pydoop/trunk/examples')
sys.path.append(pydoop_path)

from pydoop.pipes import Mapper, Reducer, RecordReader, Factory, InputSplit
from pydoop.pipes import runTask

from pydoop.hdfs  import hdfs
from pydoop.utils import split_hdfs_path

from gap_io import image as gap_image

import os


IMAGE_ANALYSIS = 'IMAGE_ANALYSIS'
INPUT_IMAGES  = 'INPUT_IMAGES'
ANALYZED_IMAGES  = 'ANALYZED_IMAGES'
PROCESSED_BYTES  = 'PROCESSED_BYTES'
REDUCED_BYTES    = 'REDUCED_BYTES'


def log(x):
  sys.stderr.write('%s\n' % x)

class record_reader(RecordReader):
  #--
  def _configure(self, jc, k, f, df):
    v = df
    if jc.hasKey(k):
      v = jc.get(k)
    setattr(self, f, v)
  #--
  def _configure_int(self, jc, k, f, df):
    v = df
    if jc.hasKey(k):
      v = jc.getInt(k)
    setattr(self, f, v)
  #--
  def __init__(self, ctx):
    RecordReader.__init__(self)
    log('IA_RecordReader::init  start')
    log('IA_RecordReader::ctx = %s' % ctx)
    isplit = ctx.getInputSplit()
    log('IA_RecordReader::input_split(raw) = >%r<' % isplit)
    #--
    self.ctx = ctx
    self.input_images = ctx.getCounter(IMAGE_ANALYSIS, INPUT_IMAGES)
    #--
    jc = self.ctx.getJobConf()
    self._configure_int(jc, 'image.analysis.xsize', 'xsize', 32)
    self._configure_int(jc, 'image.analysis.ysize', 'ysize', 32)
    self.min_record_size = gap_image.image_size(self.xsize, self.ysize)
    #--
    self.input_split = InputSplit(isplit)
    host, port, fpath = split_hdfs_path(self.input_split.filename)
    log('IA_RecordReader:: host=%r port=%d fpath=%r' % (host, port, fpath))
    self.fs   = hdfs(host, port)
    self.file = self.fs.open_file(fpath, os.O_RDONLY, 0, 0, 0)
    #--
    self.end_reached    = False
    self.position       = self.input_split.offset
    self.bytes_read     = 0
    #--
    log('IA_RecordReader:: xsize=%d ysize=%d min_record_size=%d' %
        (self.xsize, self.ysize, self.min_record_size))
    log('IA_RecordReader::init offset=%d length=%d' % (self.position,
                                                       self.input_split.length))
    #--
    self.__read_block()
    self.__align_buffer()
    #--
    log('IA_RecordReader::init  end')
  #--
  def __del__(self):
    log('IA_RecordReader::del  begin')
    self.file.close()
    self.fs.close()
    log('IA_RecordReader::del  end')
  #--
  def __read_block(self):
    self.buffer = self.file.pread(self.position, self.min_record_size)
    self.bytes_read += len(self.buffer)
    log('IA_RecordReader:: read_block at %d; total bytes read = %d' %
        (self.position, self.bytes_read))
  #--
  def __align_buffer(self):
    if self.bytes_read < self.min_record_size:
      self.end_reached = True
      log('IA_RecordReader::__align_buffer  WARNING EOF reached.')
      return
    l = self.buffer.find(gap_image.MAGIC_NUMBER)
    if l < 0:
      log('IA_RecordReader::init  WARNING Cannot find MAGIC_NUMBER.')
      return
    if self.position == 0 and l != 0:
      log('IA_RecordReader::init  CRITICAL Misaligned MAGIC_NUMBER.')
      assert l == 0
    self.position += l
    if l > 0:
      self.__read_block()
      self.bytes_read += l - self.min_record_size
  #-
  def next(self):
    "@return tuple(bool have_a_record, str record_key, str record_value)"
    log('IA_RecordReader::next read, len = %d, %d' % \
        (self.bytes_read, self.input_split.length))
    if self.end_reached:
      return (False, '', '')
    #--
    # this is basically a consistency check. It will throw out on bad data
    img = gap_image(self.buffer)
    log('IA_RecordReader::next xsize,ysize = (%d, %d), (%d, %d)' % \
        (self.xsize, self.ysize, img.xsize, img.ysize))

    self.ctx.incrementCounter(self.input_images, 1)
    #--
    if self.bytes_read >= self.input_split.length:
      self.end_reached = True
    else:
      self.__read_block()
    #--
    log('IA_RecordReader::next outputting an image of %d bytes' % len(self.buffer))
    k = '%d;%d;%d' % (img.lane, img.tile, img.cycle)
    return (True, k, img.data)
  #--
  def getProgress(self):
    return float(self.bytes_read)/self.input_split.length


class mapper(Mapper):
  def __init__(self, task_ctx):
    Mapper.__init__(self)
    log('IA_Mapper::init  start')
    log('IA_Mapper::task_ctx = %s' % task_ctx)
    self.inputTiles = task_ctx.getCounter(IMAGE_ANALYSIS, ANALYZED_IMAGES)
    self.processedBytes = task_ctx.getCounter(IMAGE_ANALYSIS, PROCESSED_BYTES)
    log('IA_Mapper::init  end')
  #-
  def map(self, map_ctx):
    log('IA_Mapper::map self=%s, ctx=%s' % (self, map_ctx))
    jc = map_ctx.getJobConf()
    lane, tile, cycle = map_ctx.getInputKey().split(';')
    v = map_ctx.getInputValue()
    k = '%s;%s' % (lane, tile)
    map_ctx.emit(k, str(len(v)))
    map_ctx.incrementCounter(self.inputTiles, 1)
    map_ctx.incrementCounter(self.processedBytes, len(v))
    log('IA_Mapper::map end')

class reducer(Reducer):
  def __init__(self, task_ctx):
    Reducer.__init__(self)
    log('IA_Reducer::init  start')
    log('IA_Reducer::task_ctx = %s' % task_ctx)
    self.reduced_bytes = task_ctx.getCounter(IMAGE_ANALYSIS, REDUCED_BYTES)
    log('IA_Reducer::init  end')
  #-
  def reduce(self, red_ctx):
    log('IA_Reducer::reduce self=%s, ctx=%s' % (self, red_ctx))
    s = 0
    while red_ctx.nextValue():
      s += int(red_ctx.getInputValue())
    k = red_ctx.getInputKey()
    log('IA_Reducer::reduce k=%s, s=%d' % (k, s))
    red_ctx.emit(k, str(s))
    red_ctx.incrementCounter(self.reduced_bytes, s)

def main(argv):
  log('image_production started')
  runTask(Factory(mapper, reducer, record_reader))


if __name__ == "__main__":
  main(sys.argv)

# Local Variables: **
# mode: python **
# End: **
