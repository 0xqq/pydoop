#!/usr/bin/env python


# FIXME this is a local hack that should go away in the real dist.
import sys
import os
pydoop_path = os.path.join(os.environ['HOME'],
                           'svn/ac-dc/lib/pydoop/trunk/examples')
sys.path.append(pydoop_path)


from pydoop.pipes import Mapper, Reducer, Factory, runTask
from pydoop.hdfs  import hdfs
from pydoop.utils import split_hdfs_path


import gap_io

IMAGE_PRODUCTION = 'IMAGE_PRODUCTION'
INPUT_TILES      = 'INPUT_TILES'
OUTPUT_BYTES     = 'OUTPUT_BYTES'
OUTPUT_SLICES    = 'OUTPUT_SLICES'


def log(x):
  sys.stderr.write('%s\n' % x)

class IP_Mapper(Mapper):
  #--
  def _configure(self, jc, k, f, df):
    v = df
    if jc.hasKey(k):
      v = jc.get(k)
    setattr(self, f, v)
  #--
  def _configure_int(self, jc, k, f, df):
    v = df
    if jc.hasKey(k):
      v = jc.getInt(k)
    setattr(self, f, v)
  #--
  def __init__(self, task_ctx):
    Mapper.__init__(self)
    log('IP_Mapper::init  start')
    log('IP_Mapper::task_ctx = %s' % task_ctx)
    self.input_tiles  = task_ctx.getCounter(IMAGE_PRODUCTION, INPUT_TILES)
    self.output_bytes = task_ctx.getCounter(IMAGE_PRODUCTION, OUTPUT_BYTES)
    #--
    jc = task_ctx.getJobConf()
    self._configure(jc, 'image.production.repository', 'repository',
                    'hdfs://localhost:9000/user/zag/image_rep')
    self._configure_int(jc, 'image.production.cycles', 'n_cycles', 4)
    self._configure_int(jc, 'image.production.xsize',  'xsize', 32)
    self._configure_int(jc, 'image.production.ysize',  'ysize', 32)
    self.hdfs_host, self.hdfs_port, self.hdfs_root = \
                    split_hdfs_path(self.repository)

    self.img_factory = gap_io.fake_factory(self.xsize, self.ysize)

    log('IP_Mapper::init image_repository = %s' % self.repository)
    log('IP_Mapper::init hdfs_host = %s' % self.hdfs_host)
    log('IP_Mapper::init hdfs_port = %s' % self.hdfs_port)
    log('IP_Mapper::init cycles = %s' % self.n_cycles)
    log('IP_Mapper::init xsize  = %s' % self.xsize)
    log('IP_Mapper::init ysize  = %s' % self.ysize)
    log('IP_Mapper::init  end')
  #-
  def map(self, map_ctx):
    log('IP_Mapper::map self=%s, ctx=%s' % (self, map_ctx))
    iv = map_ctx.getInputValue().strip()
    if not iv:
      log('IP_Mapper::map empty record')
    else:
      lane, tile = map(int, map_ctx.getInputValue().split())
      log('IP_Mapper::map lane=%d, tile=%d' % (lane, tile))
      map_ctx.incrementCounter(self.input_tiles, 1)
      #--
      lane_tile_id = 'L%02dT%03d' % (lane, tile)
      path = os.path.join(self.hdfs_root, lane_tile_id)
      fs = hdfs(self.hdfs_host, self.hdfs_port)
      log('IP_Mapper::map opened server (%s:%s' % (self.hdfs_host,
                                                   self.hdfs_port))
      f = fs.open_file(path, os.O_WRONLY, 0, 0, 0)
      log('IP_Mapper::map opened hdfs file=%s' % path)
      for i in range(self.n_cycles):
        k = lane_tile_id
        img = self.img_factory.make(lane, tile, i)
        written = f.write(img.data)
        assert written == len(img.data)
        log('IP_Mapper::map written %d bytes' % written)
        log('IP_Mapper::map emit (%s, %s)' % (k, written))
        map_ctx.incrementCounter(self.output_bytes, written)
        map_ctx.emit(k, '%d' % 4)
      f.close()
      fs.close()

    log('IP_Mapper::map end')

class IP_Reducer(Reducer):
  def __init__(self, task_ctx):
    Reducer.__init__(self)
    log('IP_Reducer::init  start')
    log('IP_Reducer::task_ctx = %s' % task_ctx)
    self.outputSlices = task_ctx.getCounter(IMAGE_PRODUCTION, OUTPUT_SLICES)
  #-
  def reduce(self, red_ctx):
    log('IP_Reducer::reduce self=%s, ctx=%s' % (self, red_ctx))
    s = 0
    while red_ctx.nextValue():
      s += 1
    k = red_ctx.getInputKey()
    log('IP_Reducer::reduce k=%s, s=%d' % (k, s))
    red_ctx.emit(k, str(s))
    red_ctx.incrementCounter(self.outputSlices, s)

def main(argv):
  log('image_production started')
  runTask(Factory(IP_Mapper, IP_Reducer))


if __name__ == "__main__":
  main(sys.argv)

# Local Variables: **
# mode: python **
# End: **
