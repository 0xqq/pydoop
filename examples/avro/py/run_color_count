#!/usr/bin/env bash

set -euo pipefail
[ -n "${DEBUG:-}" ] && set -x
this="${BASH_SOURCE-$0}"
this_dir=$(cd -P -- "$(dirname -- "${this}")" && pwd -P)
. "${this_dir}/../../config.sh"

MODULE="color_count"
USER_SCHEMA_FILE="${this_dir}"/../schemas/user.avsc
STATS_SCHEMA_FILE="${this_dir}"/../schemas/stats.avsc
STATS_SCHEMA=$(cat "${STATS_SCHEMA_FILE}")
AVRO_FN=users.avro  # used also for KV
OUTPUT=results

# --- generate avro input ---
N=20
${PYTHON} generate_avro_users.py "${USER_SCHEMA_FILE}" ${N} "${AVRO_FN}"
${HADOOP} fs -mkdir -p /user/"${USER}"
${HADOOP} fs -rm "${AVRO_FN}" || :
${HADOOP} fs -put "${AVRO_FN}" "${AVRO_FN}"

# --- run cc ---
MPY="${MODULE}".py
JOBNAME="${MODULE}"-job
LOGLEVEL="DEBUG"
INPUT=${AVRO_FN}

${HADOOP} fs -rmr "/user/${USER}/${OUTPUT}" || :
${PYDOOP} submit \
    -D pydoop.mapreduce.avro.value.output.schema="${STATS_SCHEMA}" \
    --upload-file-to-cache "${MPY}" \
    --num-reducers 1 \
    --avro-input v \
    --avro-output v \
    --log-level "${LOGLEVEL}" \
    --job-name "${JOBNAME}" \
    "${MODULE}" "${INPUT}" "${OUTPUT}"

# --- dump & check results ---
DUMP_FN=stats.tsv
rm -rf "${OUTPUT}"
${HADOOP} fs -get "${OUTPUT}"
${PYTHON} avro_container_dump_results.py \
    "${OUTPUT}"/part-r-00000.avro "${DUMP_FN}"
${PYTHON} check_cc.py "${AVRO_FN}" "${DUMP_FN}"
