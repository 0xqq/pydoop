#!/usr/bin/env bash

set -euo pipefail
[ -n "${DEBUG:-}" ] && set -x
this="${BASH_SOURCE-$0}"
this_dir=$(cd -P -- "$(dirname -- "${this}")" && pwd -P)
. "${this_dir}/../../config.sh"

nargs=1
if [ $# -ne ${nargs} ]; then
    die "Usage: $0 k|v|kv"
fi
mode=$1
if [ "${mode}" == "k" ]; then
    MODULE=avro_key_in
elif [ "${mode}" == "v" ]; then
    MODULE=avro_value_in
elif [ "${mode}" == "kv" ]; then
    MODULE=avro_key_value_in
else
    die "invalid mode: ${mode}"
fi

USER_SCHEMA_FILE="${this_dir}"/../schemas/user.avsc
PET_SCHEMA_FILE="${this_dir}"/../schemas/pet.avsc
CSV_FN=/tmp/users.csv
INPUT=users.avro
AVRO_FN=/tmp/"${INPUT}"  # used also for KV
OUTPUT=results

# --- generate avro input ---
N=20
${PYTHON} create_input.py ${N} "${CSV_FN}"
if [ "${mode}" == "kv" ]; then
    pushd "${this_dir}"/../java >/dev/null
    ./write_avro_kv \
        "${USER_SCHEMA_FILE}" "${PET_SCHEMA_FILE}" "${CSV_FN}" "${AVRO_FN}"
    popd >/dev/null
else
    ${PYTHON} write_avro.py "${USER_SCHEMA_FILE}" "${CSV_FN}" "${AVRO_FN}"
fi
${HADOOP} fs -mkdir -p /user/"${USER}"
${HADOOP} fs -rm "${INPUT}" || :
${HADOOP} fs -put "${AVRO_FN}" "${INPUT}"

# --- run cc ---
MPY="${MODULE}".py
JOBNAME="${MODULE}"-job
LOGLEVEL="DEBUG"

${HADOOP} fs -rmr "/user/${USER}/${OUTPUT}" || :
${PYDOOP} submit \
    --upload-file-to-cache avro_base.py \
    --upload-file-to-cache "${MPY}" \
    --num-reducers 1 \
    --avro-input "${mode}" \
    --log-level "${LOGLEVEL}" \
    --job-name "${JOBNAME}" \
    "${MODULE}" "${INPUT}" "${OUTPUT}"

# --- check results ---
rm -rf "${OUTPUT}"
${HADOOP} fs -get "${OUTPUT}"
# this is intentionally hardwired.
${PYTHON} check_results.py "${CSV_FN}" "${OUTPUT}"/part-r-00000
