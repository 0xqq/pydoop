#!/usr/bin/env python

# BEGIN_COPYRIGHT
# END_COPYRIGHT

# You need a working Hadoop cluster to run this.

import sys, os, subprocess as sp
from pydoop.hadoop_utils import get_hadoop_version


HADOOP_HOME = os.environ.get("HADOOP_HOME", "/opt/hadoop")
HADOOP_VERSION = get_hadoop_version(HADOOP_HOME)
HADOOP = os.path.join(HADOOP_HOME, "bin/hadoop")
SCRIPT = "bin/ipcount_base.py"
OUTPUT = "output"

if HADOOP_VERSION < (0,21,0):
  MR_JOB_NAME = "mapred.job.name"
  PIPES_JAVA_RR = "hadoop.pipes.java.recordreader"
  PIPES_JAVA_RW = "hadoop.pipes.java.recordwriter"
else:
  MR_JOB_NAME = "mapreduce.job.name"
  PIPES_JAVA_RR = "mapreduce.pipes.isjavarecordreader"
  PIPES_JAVA_RW = "mapreduce.pipes.isjavarecordwriter"

MR_OPTIONS = {
  MR_JOB_NAME: "ipcount_base",
  PIPES_JAVA_RR: "true",
  PIPES_JAVA_RW: "true",
  }


def build_d_options(opt_dict):
  d_options = []
  for name, value in opt_dict.iteritems():
    d_options.append("-D %s=%s" % (name, value))
  return " ".join(d_options)


def run_cmd(cmd_string):
  return sp.call(cmd_string, shell=True)


def main(argv):
  try:
    local_input = argv[1]
  except IndexError:
    print "USAGE: %s LOCAL_INPUT" % argv[0]
    sys.exit(2)
  local_input = local_input.rstrip(os.path.sep)

  script_hdfs = os.path.basename(SCRIPT)
  input_ = os.path.basename(local_input)
  output = "output"
  dopts = build_d_options(MR_OPTIONS)
  run_cmd("%s fs -rmr %s %s" % (HADOOP, input_, output))
  run_cmd("%s fs -put %s %s" % (HADOOP, SCRIPT, script_hdfs))
  run_cmd("%s fs -put %s %s" % (HADOOP, local_input, input_))
  run_cmd("%s pipes %s -program %s -input %s -output %s" %
          (HADOOP, dopts, script_hdfs, input_, output))
  p1 = sp.Popen([HADOOP, "fs", "-cat", "%s/part*" % output], stdout=sp.PIPE)
  p2 = sp.Popen(["sort", "-k2,2nr"], stdin=p1.stdout, stdout=sp.PIPE)
  p3 = sp.Popen(["head", "-n", "5"], stdin=p2.stdout, stdout=sp.PIPE)
  print p3.communicate()[0]


if __name__ == "__main__":
  main(sys.argv)
