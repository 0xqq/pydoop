#!/usr/bin/env python

# BEGIN_COPYRIGHT
# END_COPYRIGHT

# You need a working Hadoop cluster to run this.

import sys, os, subprocess as sp
from pydoop.hadoop_utils import get_hadoop_version
import pydoop.hadut as hadut


HADOOP = hadut.hadoop
SCRIPT = "bin/ipcount_base.py"
OUTPUT = "output"

MR_JOB_NAME = "mapred.job.name"
MR_HOME_DIR = 'mapreduce.admin.user.home.dir'
PIPES_JAVA_RR = "hadoop.pipes.java.recordreader"
PIPES_JAVA_RW = "hadoop.pipes.java.recordwriter"

MR_OPTIONS = {
  MR_JOB_NAME: "ipcount_base",
  PIPES_JAVA_RR: "true",
  PIPES_JAVA_RW: "true",
	MR_HOME_DIR: os.path.expanduser("~"),
  }


def build_d_options(opt_dict):
  d_options = []
  for name, value in opt_dict.iteritems():
    d_options.append("-D %s=%s" % (name, value))
  return " ".join(d_options)


def run_cmd(cmd_string):
  return sp.call(cmd_string, shell=True)


def main(argv):
  try:
    local_input = argv[1]
  except IndexError:
    print "USAGE: %s LOCAL_INPUT" % argv[0]
    sys.exit(2)
  local_input = local_input.rstrip(os.path.sep)

  script_hdfs = os.path.basename(SCRIPT)
  input_ = os.path.basename(local_input)
  output = "output"
  dopts = build_d_options(MR_OPTIONS)
  run_cmd("%s fs -rmr %s %s" % (HADOOP, input_, output))
  run_cmd("%s fs -put %s %s" % (HADOOP, SCRIPT, script_hdfs))
  run_cmd("%s fs -put %s %s" % (HADOOP, local_input, input_))
  run_cmd("%s pipes %s -program %s -input %s -output %s" %
          (HADOOP, dopts, script_hdfs, input_, output))
  p1 = sp.Popen([HADOOP, "fs", "-cat", "%s/part*" % output],
                stdout=sp.PIPE, stderr=sp.PIPE)
  p2 = sp.Popen(["sort", "-k2,2nr"],
                stdin=p1.stdout, stdout=sp.PIPE,stderr=sp.PIPE)
  p3 = sp.Popen(["head", "-n", "5"],
                stdin=p2.stdout, stdout=sp.PIPE, stderr=sp.PIPE)
  out, err = p3.communicate()
  print out


if __name__ == "__main__":
  main(sys.argv)
