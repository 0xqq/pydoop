#!/usr/bin/env python

# BEGIN_COPYRIGHT
# END_COPYRIGHT

# You need a working Hadoop cluster to run this.

"""
Counts occurrences of IP addresses in Apache access log files,
optionally ignoring IP addresses listed in a given file. The only
required argument is the input directory containing the log files,
formatted as described in:

  http://httpd.apache.org/docs/1.3/logs.html#common

NOTE: the MapReduce application launched by this example wrapper is a
      Pydoop reimplementation of a Dumbo programming example from
      K. Bosteels, 'Fuzzy techniques in the usage and construction of
      comparison measures for music objects', PhD thesis, Ghent
      University, 2009, available at http://users.ugent.be/~klbostee.
"""

import sys, os, optparse, operator, subprocess as sp

import pydoop.hdfs as hdfs
import pydoop.hadoop_utils as hu


HADOOP = hu.get_hadoop_exec()
MR_SCRIPT = "bin/ipcount.py"
OUTPUT = "output"

MR_JOB_NAME = "mapred.job.name"
PIPES_JAVA_RR = "hadoop.pipes.java.recordreader"
PIPES_JAVA_RW = "hadoop.pipes.java.recordwriter"
MR_CACHE_FILES = "mapred.cache.files"
MR_CREATE_SYMLINK = "mapred.create.symlink"

BASE_MR_OPTIONS = {
  MR_JOB_NAME: "ipcount",
  PIPES_JAVA_RR: "true",
  PIPES_JAVA_RW: "true",
  }


def build_d_options(opt_dict):
  d_options = []
  for name, value in opt_dict.iteritems():
    d_options.append("-D %s=%s" % (name, value))
  return " ".join(d_options)


def hadoop_pipes(pipes_opts, hadoop=HADOOP):
  p = sp.Popen("%s pipes %s" % (hadoop, pipes_opts), shell=True)
  return os.waitpid(p.pid, 0)[1]


def print_first_n(fs, output_path, n, outf=sys.stdout):
  ip_list = []
  for entry in fs.list_directory(output_path):
    fn = entry["name"]
    if fn.rsplit("/",1)[-1].startswith("part"):
      f = fs.open_file(fn)
      for line in f:
        ip, count = line.strip().split()
        ip_list.append((ip, int(count)))
      f.close()
  ip_list.sort(key=operator.itemgetter(1), reverse=True)
  for ip, count in ip_list[:n]:
    outf.write("%s\t%d\n" % (ip, count))


class HelpFormatter(optparse.IndentedHelpFormatter):
  def format_description(self, description):
    return description + "\n" if description else ""


def make_parser():
  parser = optparse.OptionParser(
    usage="%prog [OPTIONS] INPUT",
    formatter=HelpFormatter(),
    )
  parser.set_description(__doc__.lstrip())
  parser.add_option("-s", type="str", dest="script", metavar="STRING",
                    help="MapReduce script ['%default']", default=MR_SCRIPT)
  parser.add_option("-e", type="str", dest="exclude_fn", metavar="STRING",
                    help="exclude IPs listed in this file [None]")
  parser.add_option("-o", type="str", dest="output", metavar="STRING",
                    help="HDFS output dir ['%default']", default=OUTPUT)
  parser.add_option("-n", type="int", dest="n_top", metavar="INT",
                    help="number of top IPs to list [%default]", default=5)
  parser.add_option("--local-output", type="str", metavar="STRING",
                    help="local output file [stdout]", default=sys.stdout)
  parser.add_option("--hadoop", type="str", metavar="STRING",
                    help="Hadoop executable ['%default']", default=HADOOP)
  return parser


def main():
  parser = make_parser()
  opt, args = parser.parse_args()
  try:
    input_ = args[0]
  except IndexError:
    parser.print_help()
    sys.exit(2)
  if opt.local_output is not sys.stdout:
    opt.local_output = open(opt.local_output, 'w')

  # output HDFS bust be the same as the input one
  opt.output = hdfs.path.split(opt.output)[-1]

  hostname, port, _ = hdfs.path.split(input_)
  fs = hdfs.hdfs(hostname, port)
  lfs = hdfs.hdfs("", 0)

  script_hdfs = os.path.basename(opt.script)
  input_hdfs = os.path.basename(input_)
  lfs.copy(opt.script, fs, script_hdfs)
  if not fs.exists(input_hdfs):
    try:
      lfs.copy(input_, fs, input_hdfs)
    except IOError:
      raise ValueError("could not find input dir in either HDFS or local fs")
  if fs.exists(opt.output):
    fs.delete(opt.output)

  mr_options = {}
  mr_options['mapreduce.admin.user.home.dir'] = os.path.expanduser("~")
  mr_options.update(BASE_MR_OPTIONS)
  if opt.exclude_fn:
    exclude_fn_hdfs = os.path.basename(opt.exclude_fn)
    lfs.copy(opt.exclude_fn, fs, exclude_fn_hdfs)
    mr_options[MR_CACHE_FILES] = "%(f)s#%(f)s" % {"f": exclude_fn_hdfs}
    mr_options[MR_CREATE_SYMLINK] = "yes"
    mr_options["ipcount.excludes"] = exclude_fn_hdfs
  d_options = build_d_options(mr_options)
  hadoop_pipes("%s -program %s -input %s -output %s" % (
    d_options, script_hdfs, input_hdfs, opt.output
    ))

  print_first_n(fs, opt.output, opt.n_top, outf=opt.local_output)

  lfs.close()
  fs.close()
  if opt.local_output is not sys.stdout:
    opt.local_output.close()


if __name__ == "__main__":
  main()
