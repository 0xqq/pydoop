#!/usr/bin/env python

# BEGIN_COPYRIGHT
# END_COPYRIGHT

# You need a working Hadoop cluster to run this.

"""
Counts occurrences of IP addresses in Apache access log files,
optionally ignoring IP addresses listed in a given file. The only
required argument is the input directory containing the log files,
formatted as described in:

  http://httpd.apache.org/docs/1.3/logs.html#common

NOTE: the MapReduce application launched by this example wrapper is a
      Pydoop reimplementation of a Dumbo programming example from
      K. Bosteels, 'Fuzzy techniques in the usage and construction of
      comparison measures for music objects', PhD thesis, Ghent
      University, 2009, available at http://users.ugent.be/~klbostee.
"""

import sys, os, optparse, operator, subprocess as sp
import pydoop.utils as pu
from pydoop.hdfs import hdfs


HADOOP_HOME = os.environ.get("HADOOP_HOME", "/opt/hadoop")
HADOOP = os.path.join(HADOOP_HOME, "bin/hadoop")
MR_SCRIPT = "bin/ipcount.py"
INPUT = "input"
OUTPUT = "output"
BASE_MR_OPTIONS = {
  "mapred.job.name": "ipcount",
  "hadoop.pipes.java.recordreader": "true",
  "hadoop.pipes.java.recordwriter": "true",
  "mapred.map.tasks": "2",
  "mapred.reduce.tasks": "2",
  }


def build_d_options(opt_dict):
  d_options = []
  for name, value in opt_dict.iteritems():
    d_options.append("-D %s=%s" % (name, value))
  return " ".join(d_options)


def hadoop_pipes(pipes_opts, hadoop=HADOOP):
  p = sp.Popen("%s pipes %s" % (hadoop, pipes_opts), shell=True)
  return os.waitpid(p.pid, 0)[1]


def overwrite_put(src, dst, hadoop=HADOOP):
  ret = hadoop_cmd("fs -test -e %s" % dst, hadoop)
  if ret == 0:
    hadoop_cmd("fs -rmr %s" % dst, hadoop)
  put = "fs -put %s %s" % (src, dst)
  ret = hadoop_cmd(put, hadoop)
  if ret:
    sys.exit("ERROR: '%s %s' failed" % (hadoop, put))


def print_first_n(fs, output_path, n):
  ip_list = []
  for entry in fs.list_directory(output_path):
    fn = entry["name"]
    if fn.rsplit("/",1)[-1].startswith("part"):
      f = fs.open_file(fn)
      for line in f:
        ip, count = line.strip().split()
        ip_list.append((ip, int(count)))
      f.close()
  ip_list.sort(key=operator.itemgetter(1), reverse=True)
  for ip, count in ip_list[:n]:
    print "%s\t%d" % (ip, count)


class HelpFormatter(optparse.IndentedHelpFormatter):
  def format_description(self, description):
    return description + "\n" if description else ""


def make_parser():
  parser = optparse.OptionParser(
    usage="ipcount [OPTIONS] HDFS_INPUT",
    formatter=HelpFormatter(),
    )
  parser.set_description(__doc__.lstrip())
  parser.add_option("-s", type="str", dest="script", metavar="STRING",
                    help="MapReduce script ['%default']", default=MR_SCRIPT)
  parser.add_option("-e", type="str", dest="exclude_fn", metavar="STRING",
                    help="exclude IPs listed in this file [None]")
  parser.add_option("-o", type="str", dest="output", metavar="STRING",
                    help="HDFS output dir ['%default']", default=OUTPUT)
  parser.add_option("-n", type="int", dest="n_top", metavar="INT",
                    help="number of top IPs to list [%default]", default=5)
  parser.add_option("--hadoop", type="str", metavar="STRING",
                    help="Hadoop executable ['%default']", default=HADOOP)
  return parser


def main(argv):
  parser = make_parser()
  opt, args = parser.parse_args()
  try:
    input_ = args[0]
  except IndexError:
    parser.print_help()
    sys.exit(2)

  # output HDFS bust be the same as the input one
  opt.output = pu.split_hdfs_path(opt.output)[-1]

  hostname, port, _ = pu.split_hdfs_path(input_)
  fs = hdfs(hostname, port)
  lfs = hdfs("", 0)

  script_hdfs = os.path.basename(opt.script)
  lfs.copy(opt.script, fs, script_hdfs)
  if not fs.exists(input_):
    try:
      lfs.copy(input_, fs, input_)
    except IOError:
      "could not find %r in either HDFS or local fs"
  if fs.exists(opt.output):
    fs.delete(opt.output)

  mr_options = {}
  mr_options.update(BASE_MR_OPTIONS)
  if opt.exclude_fn:
    exclude_fn_hdfs = os.path.basename(opt.exclude_fn)
    lfs.copy(opt.exclude_fn, fs, exclude_fn_hdfs)
    mr_options["mapred.cache.files"] = "%(f)s#%(f)s" % {"f": exclude_fn_hdfs}
    mr_options["mapred.create.symlink"] = "yes"
    mr_options["ipcount.excludes"] = exclude_fn_hdfs
  d_options = build_d_options(mr_options)
  hadoop_pipes("%s -program %s -input %s -output %s" % (
    d_options, script_hdfs, input_, opt.output
    ))

  print_first_n(fs, opt.output, opt.n_top)

  lfs.close()
  fs.close()


if __name__ == "__main__":
  main(sys.argv)
