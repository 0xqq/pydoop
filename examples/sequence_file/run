#!/usr/bin/env python

# BEGIN_COPYRIGHT
# END_COPYRIGHT

"""
Run a word count on the input, storing counts as 32-bit integers in
Hadoop SequenceFiles; subsequently, run a MapReduce application that
filters out those words whose count falls below a specified threshold.

The purpose of this example is to demonstrate the usage of
SequenceFileInputFormat and SequenceFileOutputFormat.
"""

import os, optparse, uuid, logging
logging.basicConfig(level=logging.INFO)

import pydoop.hadoop_utils as hu
import pydoop.hdfs as hdfs
import pydoop.test_support as pts
import pydoop.hadut as hadut


HADOOP = hu.get_hadoop_exec()
WD = "pydoop_test_sequence_file_%s" % uuid.uuid4().hex
OUTPUT = "output"
LOCAL_WC_SCRIPT = "bin/wordcount.py"
LOCAL_FILTER_SCRIPT = "bin/filter.py"

THIS_DIR = os.path.dirname(os.path.abspath(__file__))
DEFAULT_INPUT = os.path.normpath(os.path.join(THIS_DIR, "../input"))

MR_JOB_NAME = "mapred.job.name"
MR_HOME_DIR = 'mapreduce.admin.user.home.dir'
PIPES_JAVA_RR = "hadoop.pipes.java.recordreader"
PIPES_JAVA_RW = "hadoop.pipes.java.recordwriter"
MR_OUT_COMPRESS_TYPE = "mapred.output.compression.type"
MR_REDUCE_TASKS = "mapred.reduce.tasks"
MR_IN_CLASS = "mapred.input.format.class"
MR_OUT_CLASS = "mapred.output.format.class"
MRLIB = "org.apache.hadoop.mapred"

BASE_MR_OPTIONS = {
  PIPES_JAVA_RR: "true",
  PIPES_JAVA_RW: "true",
  MR_HOME_DIR: os.path.expanduser("~"),
  }


def make_parser():
  parser = optparse.OptionParser(usage="%prog [OPTIONS]")
  parser.add_option("-i", dest="input", metavar="STRING",
                    help="input dir/file ['%default']", default=DEFAULT_INPUT)
  parser.add_option("-t", type="int", dest="threshold", metavar="INT",
                    help="min word occurrence [%default]", default=10)
  return parser


def run_wc(opt):
  options = BASE_MR_OPTIONS.copy()
  options.update({
    MR_JOB_NAME: "wordcount",
    MR_OUT_CLASS: "%s.SequenceFileOutputFormat" % MRLIB,
    MR_OUT_COMPRESS_TYPE: "NONE",
    })
  script, input_, output = [hdfs.path.join(WD, uuid.uuid4().hex)
                           for _ in xrange(3)]
  with open(LOCAL_WC_SCRIPT) as f:
    pipes_code = pts.add_sys_path(f.read())
  hdfs.dump(pipes_code, script)
  hdfs.put(opt.input, input_)
  hadut.run_pipes(script, input_, output, properties=options)
  return output


def run_filter(opt, input_):
  options = BASE_MR_OPTIONS.copy()
  options.update({
    MR_JOB_NAME: "filter",
    MR_IN_CLASS: "%s.SequenceFileInputFormat" % MRLIB,
    MR_REDUCE_TASKS: "0",
    "filter.occurrence.threshold": opt.threshold,
    })
  script, output = [hdfs.path.join(WD, uuid.uuid4().hex) for _ in xrange(2)]
  with open(LOCAL_FILTER_SCRIPT) as f:
    pipes_code = pts.add_sys_path(f.read())
  hdfs.dump(pipes_code, script)
  hadut.run_pipes(script, input_, output, properties=options)
  return output


def main():
  parser = make_parser()
  opt, _ = parser.parse_args()
  hdfs.mkdir(WD)
  logging.info("running word count")
  wc_output = run_wc(opt)
  logging.info("running filter")
  filter_output = run_filter(opt, wc_output)
  logging.info("checking results")
  res = pts.collect_output(filter_output)
  hdfs.rmr(WD)
  local_wc = pts.LocalWordCount(opt.input, min_occurrence=opt.threshold)
  logging.info(local_wc.check(res))


if __name__ == "__main__":
  main()
