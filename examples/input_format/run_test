#!/usr/bin/env python

# BEGIN_COPYRIGHT
# END_COPYRIGHT

# You need a working Hadoop cluster to run this.

"""
Test overriding of RecordReader provided by InputFormat, with both the
deprecated 'mapred' and the new 'mapreduce' API.
"""

import sys, os, optparse, operator, subprocess as sp
import pydoop.utils as pu
from pydoop.hdfs import hdfs


HADOOP_HOME = os.environ.get("HADOOP_HOME", "/opt/hadoop")
HADOOP = os.path.join(HADOOP_HOME, "bin/hadoop")
MR_SCRIPT = "wordcount-rr.py"
OUTPUT = "output"
BASE_MR_OPTIONS = {
  "mapred.job.name": "test_rr_override",
  "hadoop.pipes.java.recordreader": "false",
  "hadoop.pipes.java.recordwriter": "true",
  "mapred.map.tasks": "2",
  "mapred.reduce.tasks": "2",
  }


def build_d_options(opt_dict):
  d_options = []
  for name, value in opt_dict.iteritems():
    d_options.append("-D %s=%s" % (name, value))
  return " ".join(d_options)


def hadoop_pipes(pipes_opts, hadoop=HADOOP):
  cmd = "%s pipes %s" % (hadoop, pipes_opts)
  sys.stderr.write("RUNNING PIPES CMD: %r\n" % cmd)
  p = sp.Popen(cmd, shell=True)
  return os.waitpid(p.pid, 0)[1]


def overwrite_put(src, dst, hadoop=HADOOP):
  ret = hadoop_cmd("fs -test -e %s" % dst, hadoop)
  if ret == 0:
    hadoop_cmd("fs -rmr %s" % dst, hadoop)
  put = "fs -put %s %s" % (src, dst)
  ret = hadoop_cmd(put, hadoop)
  if ret:
    sys.exit("ERROR: '%s %s' failed" % (hadoop, put))


class HelpFormatter(optparse.IndentedHelpFormatter):
  def format_description(self, description):
    return description + "\n" if description else ""


def make_parser():
  parser = optparse.OptionParser(
    usage="%prog [OPTIONS] HDFS_INPUT",
    formatter=HelpFormatter(),
    )
  parser.set_description(__doc__.lstrip())
  parser.add_option("-o", type="str", dest="output", metavar="STRING",
                    help="HDFS output dir ['%default']", default=OUTPUT)  
  parser.add_option("--java-rr", action="store_true",
                    help="Use Java RecordReader")
  parser.add_option("--deprecated", action="store_true",
                    help="Use InputFormat written with the deprecated API")
  parser.add_option("--splitable", action="store_true",
                    help="allow input format to split individual files")
  return parser


def main(argv):
  parser = make_parser()
  opt, args = parser.parse_args()
  try:
    input_ = args[0]
  except IndexError:
    parser.print_help()
    sys.exit(2)

  script=MR_SCRIPT
  mr = "mapred" if opt.deprecated else "mapreduce"

  # output HDFS bust be the same as the input one
  opt.output = pu.split_hdfs_path(opt.output)[-1]

  hostname, port, _ = pu.split_hdfs_path(input_)
  fs = hdfs(hostname, port)
  lfs = hdfs("", 0)

  script_hdfs = os.path.basename(script)
  lfs.copy(script, fs, script_hdfs)
  if not fs.exists(input_):
    try:
      lfs.copy(input_, fs, input_)
    except IOError:
      "could not find %r in either HDFS or local fs"
  if fs.exists(opt.output):
    fs.delete(opt.output)

  mr_options = {}
  mr_options.update(BASE_MR_OPTIONS)
  if opt.java_rr:
    mr_options["hadoop.pipes.java.recordreader"] = "true"
  mr_options["pydoop.input.issplitable"] = "true" if opt.splitable else "false"
  mr_options["mapred.input.format.class"] = "net.sourceforge.pydoop.%s.TextInputFormat" % mr
  jar = "pydoop-%s.jar" % mr
  d_options = build_d_options(mr_options)
  hadoop_pipes("%s -jar %s -program %s -input %s -output %s" % (
    d_options, jar, script_hdfs, input_, opt.output
    ))

  lfs.close()
  fs.close()


if __name__ == "__main__":
  main(sys.argv)
