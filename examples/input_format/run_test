#!/usr/bin/env python

# BEGIN_COPYRIGHT
# END_COPYRIGHT

# You need a working Hadoop cluster to run this.

"""
Test overriding of RecordReader provided by InputFormat.

You can use a custom Java InputFormat with a Python RecordReader: the
RecordReader supplied by the InputFormat will be overridden by the
Python one. Just remember to set 'hadoop.pipes.java.recordreader' to
'false' (try to run this with --java-rr and see how it crashes).

The example custom InputFormat is a simple modification of the
standard TextInputFormat: it adds a configurable boolean parameter
that, if set to true, makes input file non-splitable (i.e., you can't
get more InputSplits than the number of input files).
"""

import sys, os, optparse, subprocess as sp
import pydoop.utils as pu
from pydoop.hdfs import hdfs


HADOOP_HOME = os.environ.get("HADOOP_HOME", "/opt/hadoop")
HADOOP = os.path.join(HADOOP_HOME, "bin/hadoop")
MR_SCRIPT = "wordcount-rr.py"
OUTPUT = "output"
INPUT_FORMAT = "net.sourceforge.pydoop.mapred.TextInputFormat"
JAR = "pydoop-mapred.jar"
BASE_MR_OPTIONS = {
  "mapred.job.name": "test_rr_override",
  "hadoop.pipes.java.recordreader": "false",
  "hadoop.pipes.java.recordwriter": "true",
  "mapred.map.tasks": "2",
  "mapred.reduce.tasks": "2",
  "mapred.input.format.class": INPUT_FORMAT,
  }


def build_d_options(opt_dict):
  d_options = []
  for name, value in opt_dict.iteritems():
    d_options.append("-D %s=%s" % (name, value))
  return " ".join(d_options)


def hadoop_pipes(pipes_opts, hadoop=HADOOP):
  cmd = "%s pipes %s" % (hadoop, pipes_opts)
  sys.stderr.write("RUNNING PIPES CMD: %r\n" % cmd)
  p = sp.Popen(cmd, shell=True)
  return os.waitpid(p.pid, 0)[1]


def overwrite_put(src, dst, hadoop=HADOOP):
  ret = hadoop_cmd("fs -test -e %s" % dst, hadoop)
  if ret == 0:
    hadoop_cmd("fs -rmr %s" % dst, hadoop)
  put = "fs -put %s %s" % (src, dst)
  ret = hadoop_cmd(put, hadoop)
  if ret:
    sys.exit("ERROR: '%s %s' failed" % (hadoop, put))


class HelpFormatter(optparse.IndentedHelpFormatter):
  def format_description(self, description):
    return description + "\n" if description else ""


def make_parser():
  parser = optparse.OptionParser(
    usage="%prog [OPTIONS] INPUT",
    formatter=HelpFormatter(),
    )
  parser.set_description(__doc__.lstrip())
  parser.add_option("-o", type="str", dest="output", metavar="STRING",
                    help="HDFS output dir ['%default']", default=OUTPUT)  
  parser.add_option("--java-rr", action="store_true",
                    help="Java RecordReader (CRASHES THE APPLICATION)")
  parser.add_option("--splitable", action="store_true",
                    help="allow input format to split individual files")
  return parser


def main(argv):
  parser = make_parser()
  opt, args = parser.parse_args()
  try:
    input_ = args[0]
  except IndexError:
    parser.print_help()
    sys.exit(2)

  # output HDFS bust be the same as the input one
  opt.output = pu.split_hdfs_path(opt.output)[-1]

  hostname, port, _ = pu.split_hdfs_path(input_)
  fs = hdfs(hostname, port)
  lfs = hdfs("", 0)

  script_hdfs = os.path.basename(MR_SCRIPT)
  input_hdfs = os.path.basename(input_)
  lfs.copy(MR_SCRIPT, fs, script_hdfs)
  if not fs.exists(input_hdfs):
    try:
      lfs.copy(input_, fs, input_hdfs)
    except IOError:
      "could not find input dir in either HDFS or local fs"
  if fs.exists(opt.output):
    fs.delete(opt.output)

  mr_options = {}
  mr_options.update(BASE_MR_OPTIONS)
  if opt.java_rr:
    mr_options["hadoop.pipes.java.recordreader"] = "true"
  mr_options["pydoop.input.issplitable"] = "true" if opt.splitable else "false"
  d_options = build_d_options(mr_options)
  hadoop_pipes("%s -jar %s -program %s -input %s -output %s" % (
    d_options, JAR, script_hdfs, input_hdfs, opt.output
    ))

  lfs.close()
  fs.close()


if __name__ == "__main__":
  main(sys.argv)
