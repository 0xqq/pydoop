#!/usr/bin/env python

# BEGIN_COPYRIGHT
# END_COPYRIGHT

# You need a working Hadoop cluster to run this.

"""
Test overriding of RecordReader provided by InputFormat.

You can use a custom Java InputFormat with a Python RecordReader: the
RecordReader supplied by the InputFormat will be overridden by the
Python one. Just remember to set 'hadoop.pipes.java.recordreader' to
'false' (try to run this with --java-rr and see how it crashes).

The example custom InputFormat is a simple modification of the
standard TextInputFormat: it adds a configurable boolean parameter
that, if set to true, makes input file non-splitable (i.e., you can't
get more InputSplits than the number of input files).
"""

import sys, os, optparse, subprocess as sp
import pydoop.utils as pu
from pydoop.hdfs import hdfs
from pydoop.hadoop_utils import get_hadoop_version


HADOOP_HOME = os.getenv("HADOOP_HOME", "/opt/hadoop")
HADOOP_VERSION = get_hadoop_version(HADOOP_HOME)
HADOOP = os.path.join(HADOOP_HOME, "bin/hadoop")
MR_SCRIPT = "wordcount-rr.py"
OUTPUT = "output"
INPUT_FORMAT = "net.sourceforge.pydoop.mapred.TextInputFormat"
JAR = "pydoop-mapred.jar"

if HADOOP_VERSION < (0,21,0):
  MR_JOB_NAME = "mapred.job.name"
  PIPES_JAVA_RR = "hadoop.pipes.java.recordreader"
  PIPES_JAVA_RW = "hadoop.pipes.java.recordwriter"
else:
  MR_JOB_NAME = "mapreduce.job.name"
  PIPES_JAVA_RR = "mapreduce.pipes.isjavarecordreader"
  PIPES_JAVA_RW = "mapreduce.pipes.isjavarecordwriter"
INPUT_FORMAT_CLASS = "mapred.input.format.class"

BASE_MR_OPTIONS = {
  MR_JOB_NAME: "test_rr_override",
  PIPES_JAVA_RR: "false",
  PIPES_JAVA_RW: "true",
  INPUT_FORMAT_CLASS: INPUT_FORMAT,
  }


def build_d_options(opt_dict):
  d_options = []
  for name, value in opt_dict.iteritems():
    d_options.append("-D %s=%s" % (name, value))
  return " ".join(d_options)


def hadoop_pipes(pipes_opts, hadoop=HADOOP):
  cmd = "%s pipes %s" % (hadoop, pipes_opts)
  sys.stderr.write("RUNNING PIPES CMD: %r\n" % cmd)
  p = sp.Popen(cmd, shell=True)
  return os.waitpid(p.pid, 0)[1]


class HelpFormatter(optparse.IndentedHelpFormatter):
  def format_description(self, description):
    return description + "\n" if description else ""


def make_parser():
  parser = optparse.OptionParser(
    usage="%prog [OPTIONS] INPUT",
    formatter=HelpFormatter(),
    )
  parser.set_description(__doc__.lstrip())
  parser.add_option("-o", type="str", dest="output", metavar="STRING",
                    help="HDFS output dir ['%default']", default=OUTPUT)  
  parser.add_option("--java-rr", action="store_true",
                    help="Java RecordReader (CRASHES THE APPLICATION)")
  parser.add_option("--splitable", action="store_true",
                    help="allow input format to split individual files")
  return parser


def main(argv):
  parser = make_parser()
  opt, args = parser.parse_args()
  try:
    input_ = args[0]
  except IndexError:
    parser.print_help()
    sys.exit(2)

  # output HDFS bust be the same as the input one
  opt.output = pu.split_hdfs_path(opt.output)[-1]

  hostname, port, _ = pu.split_hdfs_path(input_)
  fs = hdfs(hostname, port)
  lfs = hdfs("", 0)

  script_hdfs = os.path.basename(MR_SCRIPT)
  input_hdfs = os.path.basename(input_)
  lfs.copy(MR_SCRIPT, fs, script_hdfs)
  if fs.exists(input_hdfs):
    fs.delete(input_hdfs)
  lfs.copy(input_, fs, input_hdfs)
  if fs.exists(opt.output):
    fs.delete(opt.output)

  mr_options = {}
  mr_options.update(BASE_MR_OPTIONS)
  if opt.java_rr:
    mr_options[PIPES_JAVA_RR] = "true"
  mr_options["pydoop.input.issplitable"] = "true" if opt.splitable else "false"
  d_options = build_d_options(mr_options)
  hadoop_pipes("%s -jar %s -program %s -input %s -output %s" % (
    d_options, JAR, script_hdfs, input_hdfs, opt.output
    ))

  lfs.close()
  fs.close()


if __name__ == "__main__":
  main(sys.argv)
