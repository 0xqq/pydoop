#!/usr/bin/env python
# BEGIN_COPYRIGHT
#
# Copyright 2012 CRS4.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may not
# use this file except in compliance with the License. You may obtain a copy
# of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
# END_COPYRIGHT

"""
FIXME
"""

import hashlib
import pydoop.hdfs as hdfs

FIELDS = 'name owner group size permissions last_mod md5'.split()
AVE_REC_SIZE = 200 # average
CHUNK_SIZE = 2**26 # 64MB

def format_info(info):
      w = map(str, map(info.get, FIELDS))
      return '\t'.join(w)

def compute_md5(fs, hdfs_path):
      block_size = CHUNK_SIZE
      m = hashlib.md5()
      path = hdfs.path.split(hdfs_path)[-1]
      with fs.open_file(path) as f:
            data = f.read(block_size)
            while data:
                  m.update(data)
                  data = f.read(block_size)
      return m.hexdigest()

def get_info(fname):
      host, port, path = hdfs.path.split(fname)
      fs = hdfs.hdfs(host, port)
      info = fs.get_path_info(path)
      info['md5'] = compute_md5(fs, path)
      return info

def isdir(fs, d):
  try:
    info = fs.get_path_info(d)
  except IOError:
    return False
  return info['kind'] == 'directory'

class Scanner(object):
      def __init__(self, fs, root, catalog):
            #-- <pre-requisites>
            root_info = fs.get_path_info(root)
            assert(root_info['kind'] == 'directory')
            #--</pre-requisites>
            self.fs = fs
            self.root = root
            self.catalog = catalog
            self.children = []
            self.depth = 0

      def leaves(self):
            if self.depth > 0:
                  return sum(map(lambda c: c.leaves(), self.children), [])
            else:
                  return [self.root]

      def can_go_deeper(self):
            if self.depth == 0:
                  return True
            for child in self.children:
                  if child.can_go_deeper():
                        return True
            return False

      def scan_level_zero(self):
            self.children = []
            for info in self.fs.list_directory(self.root):
                  if info['kind'] == 'directory':
                        child = Scanner(self.fs, info['name'], self.catalog)
                        self.children.append(child)
                  else:
                        self.catalog.append(info['name'])

      def go_deeper(self):
            if self.depth == 0:
                  self.scan_level_zero()
            else:
                  for child in self.children:
                        child.go_deeper()
            self.depth += 1


def build_catalog(fs, root, catalog, max_depth=None, max_size=None):
      scanner = Scanner(fs, root, catalog)
      while (scanner.can_go_deeper()
             and (max_size  is None or len(catalog) <= max_size)
             and (max_depth is None or scanner.depth < max_depth)):
            scanner.go_deeper()
      return scanner

#--------------------------------------------------------------------
# pydoop script mapper and driver

class Catalog(object):
      def __init__(self, writer):
            self.writer = writer
      def append(self, x):
            self.writer.emit('', x)

def catalog_mapper(_, root, writer):
      root = root.strip()
      catalog = Catalog(writer)
      host, port, path = hdfs.path.split(root)
      fs = hdfs.hdfs(host, port)
      build_catalog(fs, path, catalog)
      fs.close()

def hash_mapper(_, fname, writer):
      fname = fname.strip()
      writer.status('processing {}'.format(fname))
      info = get_info(fname)
      writer.emit('', format_info(info))


def run_catalog_hadoop_job(script, n_tasks, roots_fname, out_fname):
  logger = logging.getLogger("run_catalog_hadoop_job")
  logger.setLevel(logging.DEBUG)
  opts = [
        '--map-fn', 'catalog_mapper',
        '--num-reducers', '0',
        '--kv-separator', '',
        '-D', 'mapred.map.tasks={}'.format(n_tasks),
        ]
  runner = PydoopScriptRunner(logger=logger, prefix='pydoop_')
  runner.set_input(roots_fname, put=True)
  runner.run(script, more_args=opts)
  runner.collect_output(out_fname)
  runner.clean()

def run_hash_hadoop_job(script, n_tasks, cat_list_fname, out_fname):
  logger = logging.getLogger("run_hash_hadoop_job")
  logger.setLevel(logging.DEBUG)
  opts = [
        '--map-fn', 'hash_mapper',
        '--num-reducers', '0',
        '--kv-separator', '',
        '-D', 'mapred.map.tasks={}'.format(n_tasks),
        ]
  runner = PydoopScriptRunner(logger=logger, prefix='pydoop_')
  runner.set_input(cat_list_fname, put=True)
  runner.run(script, more_args=opts)
  runner.collect_output(out_fname)
  runner.clean()

#---------------------------------------------------------------------

def make_parser():
      parser = argparse.ArgumentParser(
            description="""
File system scanner

FIXME

""",
            formatter_class=argparse.ArgumentDefaultsHelpFormatter,
            )
      parser.add_argument('-d', '--directory', metavar='DIR', help='top dir')
      parser.add_argument('--max-depth', metavar='MAX_DEPTH',
                          type=int,
                          default=-1,
                          help='maximal local scan depth')
      parser.add_argument('--max-size', metavar='MAX_SIZE',
                          type=int,
                          default=-1,
                          help='maximal catalog size')
      parser.add_argument('--n-tasks', metavar='N_TASKS',
                          type=int,
                          default=-1,
                          help='number of tasks for hadoop job')
      parser.add_argument('--cat-file', metavar='CAT_FILE',
                          default='catalog.list',
                          help='filename')
      parser.add_argument("--hadoop", help="will fork off an hadoop if needed",
                          action="store_true")
      parser.add_argument("--hash", help="will compute hash",
                          action="store_true")
      return parser


def gather_file_list(args, root, cat_file_name, max_depth, max_size):
      logger = logging.getLogger("gather_file_list")
      host, port, path = hdfs.path.split(root)
      fs = hdfs.hdfs(host, port)
      if not isdir(fs, path):
            sys.exit("%r does not exist" % root)
      fs.set_working_directory(path)
      root = "{}".format(fs.working_directory())
      logger.info('root: {}'.format(root))
      catalog = []
      scanner = build_catalog(fs, root, catalog, max_depth, max_size)
      fs.close()
      #
      with open(cat_file_name, 'w') as o:
            if len(catalog) > 0:
                  logger.info('dumping {} catalog entries'.format(len(catalog)))
                  for x in catalog:
                        o.write(x + '\n')
      leaves = scanner.leaves()
      logger.info('done with scalar part. {} leaves left'.format(len(leaves)))
      _, tmp_fname = tempfile.mkstemp()
      if len(leaves) > 0:
            with open(tmp_fname, 'w') as o:
                  for x in leaves:
                        o.write('{}\n'.format(x))
            if args.hadoop:
                  run_catalog_hadoop_job(__file__, len(leaves),
                                         tmp_fname,
                                         cat_file_name)
      os.unlink(tmp_fname)

def compute_hash(args, file_name_list, files_catalog_file):
      logger = logging.getLogger("compute_hash")
      if not args.hadoop:
            logger.info('reading list from {}'.format(file_name_list))
            logger.info('writing to {}'.format(files_catalog_file))
            with open(files_catalog_file, 'w') as o:
                  o.write('\t'.join(FIELDS) + '\n')
                  with open(file_name_list) as f:
                        for fname in f:
                              info = get_info(fname)
                              o.write(format_info(info) + '\n')
      else:
            with open(files_catalog_file, 'w') as o:
                  o.write('\t'.join(FIELDS) + '\n')
            default_n_tasks = 1 + (os.stat(file_name_list).st_size/AVE_REC_SIZE)
            n_tasks = args.n_tasks if args.n_tasks > 0 else default_n_tasks
            run_hash_hadoop_job( __file__, n_tasks, file_name_list,
                                 files_catalog_file)

def main(argv=None):
      logger = logging.getLogger("main")
      parser = make_parser()
      args = parser.parse_args(argv)
      root = args.directory
      max_depth = args.max_depth if args.max_depth >= 0 else None
      max_size  = args.max_size  if args.max_size >= 0  else None
      cat_file_name = args.cat_file

      if args.hash:
            _, tmp_file = tempfile.mkstemp()
            gather_file_list(args, root, tmp_file, max_depth, max_size)
            compute_hash(args, tmp_file, cat_file_name)
            os.unlink(tmp_file)
      else:
            gather_file_list(args, root, cat_file_name, max_depth, max_size)



if __name__ == "__main__":
      import sys, os, logging, tempfile, argparse
      logging.basicConfig(level=logging.INFO)
      from pydoop.hadut import PydoopScriptRunner
      main()

