#!/usr/bin/env python

# BEGIN_COPYRIGHT
# END_COPYRIGHT

# Do not run this directly: run 'make run' instead.

import sys, os, subprocess as sp
from pydoop.hadoop_utils import get_hadoop_version


HADOOP_HOME = os.environ.get("HADOOP_HOME", "/opt/hadoop")
HADOOP_VERSION = get_hadoop_version(HADOOP_HOME)
HADOOP = os.path.join(HADOOP_HOME, "bin/hadoop")
try:
  HDFS_WD = os.environ["HDFS_WORK_DIR"]
except KeyError:
  sys.exit("ERROR: HDFS_WORK_DIR not set")
MR_SCRIPT = "%s/bin/cv" % HDFS_WD


if HADOOP_VERSION < (0,21,0):
  MR_JOB_NAME = "mapred.job.name"
  PIPES_JAVA_RR = "hadoop.pipes.java.recordreader"
  PIPES_JAVA_RW = "hadoop.pipes.java.recordwriter"
  MR_CACHE_ARCHIVES = "mapred.cache.archives"
  MR_CREATE_SYMLINK = "mapred.create.symlink"
else:
  MR_JOB_NAME = "mapreduce.job.name"
  PIPES_JAVA_RR = "mapreduce.pipes.isjavarecordreader"
  PIPES_JAVA_RW = "mapreduce.pipes.isjavarecordwriter"
  MR_CACHE_ARCHIVES = "mapreduce.job.cache.archives"
  MR_CREATE_SYMLINK = "mapreduce.job.cache.symlink.create"

MR_OPTIONS = {
  MR_JOB_NAME: "cv",
  PIPES_JAVA_RR: "true",
  PIPES_JAVA_RW: "true",
  MR_CACHE_ARCHIVES: "%s/pydoop.tgz#pydoop,%s/cv.tgz#cv" % (HDFS_WD, HDFS_WD),
  MR_CREATE_SYMLINK: "yes",
  }


def build_d_options(opt_dict):
  d_options = []
  for name, value in opt_dict.iteritems():
    d_options.append("-D %s=%s" % (name, value))
  return " ".join(d_options)


def hadoop_pipes(pipes_opts, hadoop=HADOOP):
  p = sp.Popen("%s pipes %s" % (hadoop, pipes_opts), shell=True)
  return os.waitpid(p.pid, 0)[1]


def main(argv):
  d_options = build_d_options(MR_OPTIONS)
  input_ = "%s/input" % HDFS_WD
  output = "%s/output" % HDFS_WD
  hadoop_pipes("%s -program %s -input %s -output %s" % (
    d_options, MR_SCRIPT, input_, output
    ))


if __name__ == "__main__":
  main(sys.argv)
