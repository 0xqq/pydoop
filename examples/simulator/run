#!/usr/bin/env python

# BEGIN_COPYRIGHT
#
# Copyright 2009-2015 CRS4.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may not
# use this file except in compliance with the License. You may obtain a copy
# of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
# END_COPYRIGHT

from pydoop.mapreduce.simulator import HadoopSimulatorNetwork
from pydoop.mapreduce.simulator import HadoopSimulatorLocal
from pydoop.mapreduce.pipes import InputSplit
from pydoop.avrolib import AvroContext
import pydoop.mapreduce.pipes as pp

import logging
import os
import shutil
import tempfile
import subprocess
import pydoop.test_support as pts

import sys
md_folder = os.path.realpath(os.path.abspath('../wordcount/new_api'))
if md_folder not in sys.path:
    sys.path.insert(0, md_folder)
avro_folder = os.path.realpath(os.path.abspath('../avro/py'))
if avro_folder not in sys.path:
    sys.path.insert(0, avro_folder)


from wordcount_minimal import FACTORY as factory_minimal
from wordcount_full import FACTORY as factory_full
import avro_base
import check_results as avro_check_results
import create_input
import write_avro
import avro_container_dump_results
DATA_OUT = '/tmp/pydoop-test-avro_out'
STATS_SCHEMA = os.path.join(avro_folder, '../schemas/stats.avsc')

"""
Use simulators to run wordcount
===============================

This example shows how to use the two different version of the
HadoopSimulator to run wordcount with different configurations.

"""


def create_configuration():
    data_in = '../input/alice.txt'
    data_out = 'results.txt'
    data_in_path = os.path.realpath(data_in)
    data_in_uri = 'file://' + data_in_path
    data_in_size = os.stat(data_in_path).st_size
    output_dir = tempfile.mkdtemp(prefix="pydoop_")
    output_dir_uri = 'file://' + os.path.realpath(output_dir)
    conf = {
        "mapred.map.tasks": "2",
        "mapred.reduce.tasks": "1",
        "mapred.job.name": "wordcount",
        "mapred.work.output.dir": output_dir_uri,
        "mapred.task.partition": "0",
        }
    input_split = InputSplit.to_string(data_in_uri, 0, data_in_size)
    return data_in, data_out, conf, input_split, output_dir


def dump_counters(hs, logger):
    counters = hs.get_counters()
    for phase in ['mapping', 'reducing']:
        logger.info("%s counters:", phase.capitalize())
        for group in counters[phase]:
            logger.info("  Group %s", group)
            for c, v in counters[phase][group].iteritems():
                logger.info("   %s: %s", c, v)


def clean_up(data_out, output_dir):
    os.unlink(data_out)
    shutil.rmtree(output_dir)


def check_results(data_in, data_out, logger):
    local_wc = pts.LocalWordCount(data_in)
    logger.info("checking results")
    logger.info(local_wc.check(open(data_out).read()))


def run_network_minimal(logger):
    program_name = '../wordcount/new_api/wordcount_minimal.py'
    data_in, data_out, conf, input_split, output_dir = create_configuration()
    hs = HadoopSimulatorNetwork(program=program_name, logger=logger,
                                loglevel=logging.INFO)
    hs.run(open(data_in), open(data_out, 'w'), conf)
    dump_counters(hs, logger)
    check_results(data_in, data_out, logger)
    clean_up(data_out, output_dir)


def run_local_minimal(logger):
    hs = HadoopSimulatorLocal(factory=factory_minimal, logger=logger,
                              loglevel=logging.INFO)
    data_in, data_out, conf, input_split, output_dir = create_configuration()
    hs.run(open(data_in), open(data_out, 'w'), conf)
    dump_counters(hs, logger)
    check_results(data_in, data_out, logger)
    clean_up(data_out, output_dir)


def run_local_full(logger):
    data_in, data_out, conf, input_split, output_dir = create_configuration()
    hsl = HadoopSimulatorLocal(factory=factory_full, logger=logger,
                               loglevel=logging.INFO)
    hsl.run(None, None, conf, input_split=input_split, num_reducers=1)
    data_out = os.path.join(output_dir,
                            'part-%05d' % int(conf["mapred.task.partition"]))
    dump_counters(hsl, logger)
    check_results(data_in, data_out, logger)
    clean_up(data_out, output_dir)


def run_network_full(logger):
    program_name = '../wordcount/new_api/wordcount_full.py'
    data_in, data_out, conf, input_split, output_dir = create_configuration()
    hs = HadoopSimulatorNetwork(program=program_name, logger=logger,
                                loglevel=logging.INFO)
    hs.run(None, None, conf, input_split=input_split)
    data_out = os.path.join(output_dir,
                            'part-%05d' % int(conf["mapred.task.partition"]))
    dump_counters(hs, logger)
    check_results(data_in, data_out, logger)
    clean_up(data_out, output_dir)


def prepare_avro_data(csv_fn, avro_fn, schema_fn):
    create_input.main(20, csv_fn)
    write_avro.main(schema_fn, csv_fn, avro_fn)


def run_local_avro_value_in(logger, file_in, data_in):
    factory = pp.Factory(mapper_class=avro_base.AvroValueColorPick, reducer_class=avro_base.NoAvroColorCount)
    hsl = HadoopSimulatorLocal(factory=factory, logger=logger,
                               loglevel=logging.INFO, context_cls=AvroContext, avro_input='v')
    hsl.run(open(file_in), open(DATA_OUT, 'w'), {}, num_reducers=1)
    dump_counters(hsl, logger)
    avro_check_results.main(data_in, DATA_OUT)

def run_local_avro_value_in_out(logger, file_in, data_in):
    factory = pp.Factory(mapper_class=avro_base.AvroValueColorPick, reducer_class=avro_base.AvroValueColorCount)
    with open(STATS_SCHEMA) as schema_f:
        hsl = HadoopSimulatorLocal(
            factory, logger, logging.INFO, AvroContext, 'v', 'v', None,  schema_f.read())
    with open(file_in) as f_in:
        with open(DATA_OUT, 'wb') as f_out:
            hsl.run(f_in, f_out, {}, num_reducers=1)

    dump_counters(hsl, logger)
    data_out_des = DATA_OUT + '-des'
    avro_container_dump_results.main(DATA_OUT, data_out_des, 'v')
    avro_check_results.main(data_in, data_out_des)

def run_local_avro_key_in(logger, file_in, data_in):
    factory = pp.Factory(mapper_class=avro_base.AvroKeyColorPick, reducer_class=avro_base.NoAvroColorCount)
    hsl = HadoopSimulatorLocal(factory=factory, logger=logger,
                               loglevel=logging.DEBUG, context_cls=AvroContext, avro_input='k')
    hsl.run(open(file_in), open(DATA_OUT, 'w'), {}, num_reducers=1)
    dump_counters(hsl, logger)
    avro_check_results.main(data_in, DATA_OUT)

def run_local_avro_key_in_out(logger, file_in, data_in):
    factory = pp.Factory(mapper_class=avro_base.AvroKeyColorPick, reducer_class=avro_base.AvroKeyColorCount)
    with open(STATS_SCHEMA) as schema_f:
        hsl = HadoopSimulatorLocal(factory, logger, logging.DEBUG, AvroContext, 'k', 'k', schema_f.read())
    hsl.run(open(file_in), open(DATA_OUT, 'w'), {}, num_reducers=1)

    dump_counters(hsl, logger)
    data_out_des = DATA_OUT + '-des'
    avro_container_dump_results.main(DATA_OUT, data_out_des, 'k')
    avro_check_results.main(data_in, data_out_des)

def run_local_avro_key_value_in(logger,file_in, data_in):
    factory = pp.Factory(mapper_class=avro_base.AvroKeyValueColorPick, reducer_class=avro_base.NoAvroColorCount)
    hsl = HadoopSimulatorLocal(factory=factory, logger=logger,
                               loglevel=logging.INFO, context_cls=AvroContext, avro_input='kv')

    hsl.run(open(file_in), open(DATA_OUT, 'w'), {}, num_reducers=1)
    dump_counters(hsl, logger)
    avro_check_results.main(data_in, DATA_OUT)

def run_local_avro_key_value_in_out(logger, file_in, data_in):
    factory = pp.Factory(mapper_class=avro_base.AvroKeyValueColorPick, reducer_class=avro_base.AvroKeyValueColorCount)
    with open(STATS_SCHEMA) as schema_f:
        schema_str = schema_f.read()
        hsl = HadoopSimulatorLocal(factory, logger, logging.INFO, AvroContext, 'kv', 'kv', schema_str, schema_str)
    hsl.run(open(file_in), open(DATA_OUT, 'w'), {}, num_reducers=1)

    dump_counters(hsl, logger)
    data_out_des = DATA_OUT + '-des'
    avro_container_dump_results.main(DATA_OUT, data_out_des, 'kv')
    avro_check_results.main(data_in, data_out_des)


def main():
    logger = logging.getLogger("main")
    logger.setLevel(logging.INFO)
    logger.info("*** word count full using HadoopSimulatorNetwork ***")
    run_network_full(logger)
    logger.info("*** word count minimal using HadoopSimulatorNetwork ***")
    run_network_minimal(logger)
    logger.info("*** word count minimal using HadoopSimulatorLocal ***")
    run_local_minimal(logger)
    logger.info("*** word count full using HadoopSimulatorLocal ***")
    run_local_full(logger)

    csv_fn = '/tmp/users.csv'
    avro_fn = '/tmp/users.avro'
    users_schema = os.path.join(avro_folder, '../schemas/user.avsc')
    pets_schema = os.path.join(avro_folder, '../schemas/pet.avsc')
    prepare_avro_data(csv_fn, avro_fn, users_schema)

    run_local_avro_value_in(logger, avro_fn, csv_fn)
    run_local_avro_value_in_out(logger, avro_fn, csv_fn)
    run_local_avro_key_in(logger, avro_fn, csv_fn)
    run_local_avro_key_in_out(logger, avro_fn, csv_fn)

    users_pets_fn = '/tmp/users_pets.avro'
    subprocess.check_call(['./write_avro_kv', users_schema,  pets_schema, csv_fn, users_pets_fn],
                          cwd=os.path.realpath(os.path.abspath('../avro/java')))
    run_local_avro_key_value_in(logger, users_pets_fn, csv_fn)
    run_local_avro_key_value_in_out(logger, users_pets_fn, csv_fn)


if __name__ == "__main__":
    main()
