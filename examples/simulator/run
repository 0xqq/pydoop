#!/usr/bin/env python

# BEGIN_COPYRIGHT
#
# Copyright 2009-2014 CRS4.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may not
# use this file except in compliance with the License. You may obtain a copy
# of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
# END_COPYRIGHT

from pydoop.mapreduce.simulator import HadoopSimulatorNetwork
from pydoop.mapreduce.pipes import InputSplit
import logging
import os
import shutil
import pydoop.test_support as pts

"""
Use the simulator to run the workcount examples.

"""


def run_full(logger):
    program_name = '../wordcount/new_api/wordcount-full.py'
    data_in = '../input/alice.txt'
    output_dir = './output'
    data_in_path = os.path.realpath(data_in)
    data_in_uri = 'file://' + data_in_path
    data_in_size = os.stat(data_in_path).st_size
    # FIXME this is intentionally breaking if output_dir already exists
    os.makedirs(output_dir)
    output_dir_uri = 'file://' + os.path.realpath(output_dir)

    conf = {
        "mapred.job.name": "wordcount",
        "mapred.work.output.dir": output_dir_uri,
        "mapred.task.partition": "0",
        }

    input_split = InputSplit.to_string(data_in_uri, 0, data_in_size)
    hsn = HadoopSimulatorNetwork(program=program_name, logger=logger,
                                 loglevel=logging.INFO)
    hsn.run(None, None, conf, input_split=input_split)
    counters = hsn.get_counters()
    for phase in ['mapping', 'reducing']:
        logger.info("%s counters:", phase.capitalize())
        for group in counters[phase]:
            logger.info("  Group %s", group)
            for c, v in counters[phase][group].iteritems():
                logger.info("   %s: %s", c, v)
    data_out = os.path.join(output_dir,
                            'part-%05d' % int(conf["mapred.task.partition"]))
    local_wc = pts.LocalWordCount(data_in)
    logger.info("checking results")
    logger.info(local_wc.check(open(data_out).read()))
    logger.info("clean-up")
    shutil.rmtree(output_dir)


def run_minimal(logger):
    program_name = '../wordcount/new_api/wordcount-minimal.py'
    data_in = '../input/alice.txt'
    data_out = 'results.txt'
    conf = {
        "mapred.map.tasks": "2",
        "mapred.reduce.tasks": "1",
        "mapred.job.name": "wordcount",
        }
    hsn = HadoopSimulatorNetwork(program=program_name, logger=logger,
                                 loglevel=logging.INFO)
    hsn.run(open(data_in), open(data_out, 'w'), conf)
    local_wc = pts.LocalWordCount(data_in)
    logger.info("checking results")
    logger.info(local_wc.check(open(data_out).read()))
    os.unlink(data_out)

def main():
    logger = logging.getLogger("main")
    logger.setLevel(logging.INFO)
    logger.info("running word count full")
    run_full(logger)
    logger.info("running word count minimal")
    run_minimal(logger)


if __name__ == "__main__":
    main()
