# BEGIN_COPYRIGHT
# 
# Copyright 2012 CRS4.
# 
# Licensed under the Apache License, Version 2.0 (the "License"); you may not
# use this file except in compliance with the License. You may obtain a copy
# of the License at
# 
#   http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
# 
# END_COPYRIGHT

"""
Important environment variables
-------------------------------

The Pydoop setup looks in a number of default paths for what it
needs.  If necessary, you can override its behaviour or provide an
alternative path by exporting the environment variables below::

  JAVA_HOME, e.g., /opt/sun-jdk
  HADOOP_HOME, e.g., /opt/hadoop-1.0.2
  HADOOP_CPP_SRC, e.g., /usr/src/hadoop-0.20/c++
  MAPRED_INCLUDE, HDFS_INCLUDE, HDFS_LINK: colon-separated directories
    contaning, respectively, MapReduce header files, HDFS header files
    and the HDFS C library.

Other relevant environment variables include::

  BOOST_PYTHON: name of the Boost.Python library, with the leading 'lib'
    and the trailing extension stripped. Defaults to 'boost_python'.

  HADOOP_VERSION, e.g., 0.20.2-cdh3u4 (override Hadoop's version string).
"""

import os, platform, re, glob, shutil
from distutils.core import setup
from distutils.extension import Extension
from distutils.command.build_ext import build_ext as distutils_build_ext
from distutils.command.clean import clean as distutils_clean
from distutils.command.build import build as distutils_build
from distutils.command.build_py import build_py as distutils_build_py
from distutils.errors import DistutilsSetupError
from distutils import log

import pydoop
import pydoop.hadoop_utils as hu


HADOOP_HOME = pydoop.hadoop_home(fallback=None)
HADOOP_VERSION_INFO = pydoop.hadoop_version_info()
BOOST_PYTHON = os.getenv("BOOST_PYTHON", "boost_python")

PIPES_SRC = ["pipes", "pipes_context", "pipes_test_support",
             "pipes_serial_utils", "exceptions", "pipes_input_split"]
HDFS_SRC = ["hdfs_fs", "hdfs_file", "hdfs_common"]
PIPES_EXT_NAME = "_pipes"
HDFS_EXT_NAME = "_hdfs"


# -------
# PATCHES
# -------

# https://issues.apache.org/jira/browse/MAPREDUCE-1125
OLD_DESERIALIZE_FLOAT = """void deserializeFloat(float& t, InStream& stream)
  {
    char buf[sizeof(float)];
    stream.read(buf, sizeof(float));
    XDR xdrs;
    xdrmem_create(&xdrs, buf, sizeof(float), XDR_DECODE);
    xdr_float(&xdrs, &t);
  }"""
NEW_DESERIALIZE_FLOAT = """float deserializeFloat(InStream& stream)
  {
    float t;
    char buf[sizeof(float)];
    stream.read(buf, sizeof(float));
    XDR xdrs;
    xdrmem_create(&xdrs, buf, sizeof(float), XDR_DECODE);
    xdr_float(&xdrs, &t);
    return t;
  }"""

# Ticket #250
OLD_WRITE_BUFFER = r"""void writeBuffer(const string& buffer) {
      fprintf(stream, quoteString(buffer, "\t\n").c_str());
    }"""
NEW_WRITE_BUFFER = r"""void writeBuffer(const string& buffer) {
      fprintf(stream, "%s", quoteString(buffer, "\t\n").c_str());
    }"""

# Pipes.hh and SerialUtils.hh don't include stdint.h.  Let's include it
# in HadoopPipes.cc before it includes the other headers
OLD_PIPES_INCLUDE = """#include "hadoop/Pipes.hh"\n"""
NEW_PIPES_INCLUDE = """#include <stdint.h>\n#include "hadoop/Pipes.hh"\n"""

OLD_SERIAL_UTILS_INCLUDE = """#include "hadoop/SerialUtils.hh"\n"""
NEW_SERIAL_UTILS_INCLUDE = """#include <stdint.h>
#include "hadoop/SerialUtils.hh"
"""


# ---------
# UTILITIES
# ---------

def get_arch():
  bits, _ = platform.architecture()
  if bits == "64bit":
    return "amd64", "64"
  return "i386", "32"

HADOOP_ARCH_STR = "Linux-%s-%s" % get_arch()

def get_java_include_dirs(java_home):
  p = platform.system().lower()  # Linux-specific
  java_inc = os.path.join(java_home, "include")
  java_platform_inc = "%s/%s" % (java_inc, p)
  return [java_inc, java_platform_inc]


def get_java_library_dirs(java_home):
  a = get_arch()[0]
  return [os.path.join(java_home, "jre/lib/%s/server" % a)]


def mtime(fn):
  return os.stat(fn).st_mtime


def must_generate(target, prerequisites):
  try:
    return max(mtime(p) for p in prerequisites) > mtime(target)
  except OSError:
    return True


def get_version_string(filename="VERSION"):
  try:
    with open(filename) as f:
      return f.read().strip()
  except IOError:
    raise DistutilsSetupError("failed to read version info")


def write_config(filename="pydoop/config.py"):
  prereq = "DEFAULT_HADOOP_HOME"
  if not os.path.exists(prereq):
    with open(prereq, "w") as f:
      f.write("%s\n" % HADOOP_HOME)
  if must_generate(filename, [prereq]):
    with open(filename, "w") as f:
      f.write("# GENERATED BY setup.py\n")
      f.write("DEFAULT_HADOOP_HOME='%s'\n" % HADOOP_HOME)


def write_version(filename="pydoop/version.py"):
  prereq = "VERSION"
  if must_generate(filename, [prereq]):
    version = get_version_string(filename=prereq)
    with open(filename, "w") as f:
      f.write("# GENERATED BY setup.py\n")
      f.write("version='%s'\n" % version)


def get_hdfs_macros(hdfs_hdr):
  """
  Search libhdfs headers for specific features.
  """
  hdfs_macros = []
  with open(hdfs_hdr) as f:
    t = f.read()
  delete_args = re.search(r"hdfsDelete\((.+)\)", t).groups()[0].split(",")
  cas_args = re.search(r"hdfsConnectAsUser\((.+)\)", t).groups()[0].split(",")
  if len(delete_args) > 2:
    hdfs_macros.append(("RECURSIVE_DELETE", None))
  if len(cas_args) > 3:
    hdfs_macros.append(("CONNECT_GROUP_INFO", None))
  return hdfs_macros


class PathFinder():

  def __init__(self):
    self.__java_home = None
    self.__hadoop_cpp_src = None
    self.__mapred_inc = []
    self.__hdfs_inc = []
    self.__hdfs_link = []

  def __error(self, what, env_var):
    raise RuntimeError("%s not found, try setting %s" % (what, env_var))

  @property
  def java_home(self):
    if not self.__java_home:
      try:
        self.__java_home = os.environ["JAVA_HOME"]
      except KeyError:
        self.__error("Java home", "JAVA_HOME")
    return self.__java_home

  @property
  def hadoop_cpp_src(self):
    if not self.__hadoop_cpp_src:
      try:
        self.__hadoop_cpp_src = os.environ["HADOOP_CPP_SRC"]
      except KeyError:
        src = os.path.join(HADOOP_HOME, "src", "c++")
        if os.path.isdir(src):
          self.__hadoop_cpp_src = src
        else:
          cloudera_src = hu.first_dir_in_glob("/usr/src/hadoop*/c++")
          if cloudera_src:
            self.__hadoop_cpp_src = cloudera_src
          else:
            self.__error("Hadoop source code", "HADOOP_CPP_SRC")
    return self.__hadoop_cpp_src

  @property
  def mapred_inc(self):
    if not self.__mapred_inc:
      try:
        self.__mapred_inc = os.environ["MAPRED_INCLUDE"].split(os.pathsep)
      except KeyError:
        paths = []
        for lib in "pipes", "utils":
          p = os.path.join(self.hadoop_cpp_src, lib, "api")
          if os.path.isdir(p):
            paths.append(p)
        if len(paths) == 2:
          self.__mapred_inc = paths
        else:
          p = os.path.join(HADOOP_HOME, "c++", HADOOP_ARCH_STR, "include")
          if os.path.isdir(p):
            self.__mapred_inc = [p]
          else:
            self.__error("MapReduce include paths", "MAPRED_INCLUDE")
    return self.__mapred_inc

  @property
  def hdfs_inc(self):
    if not self.__hdfs_inc:
      try:
        self.__hdfs_inc = os.environ["HDFS_INCLUDE"].split(os.pathsep)
      except KeyError:
        p = os.path.join(self.hadoop_cpp_src, "libhdfs")
        if os.path.isdir(p):
          self.__hdfs_inc = [p]
        else:
          self.__error("HDFS include paths", "HDFS_INCLUDE")
    return self.__hdfs_inc

  @property
  def hdfs_link(self):
    if not self.__hdfs_link:
      try:
        self.__hdfs_link = os.environ["HDFS_LINK"].split(os.pathsep)
      except KeyError:
        p = os.path.join(HADOOP_HOME, "c++", HADOOP_ARCH_STR, "lib")
        if os.path.isdir(p):
          self.__hdfs_link = [p]
        elif not os.path.exists("/usr/lib/libhdfs.so"):
          self.__error("HDFS link paths", "HDFS_LINK")
    return self.__hdfs_link


PATH_FINDER = PathFinder()

# ------------------------------------------------------------------------------
# Create extension objects.
#
# We first create some basic Extension objects to pass to the distutils setup
# function.  They act as little more than placeholders, simply telling distutils
# the name of the extension and what source files it depends on.
#   functions:  create_basic_(pipes|hdfs)_ext
#
# When our build_pydoop_ext command is invoked, we build a complete extension
# object that includes all the information required for the build process.  In
# particular, it includes all the relevant paths.
#
# The reason for the two-stage process is to delay verifying paths to when
# they're needed (build) and avoiding those checks for other commands (such
# as clean).
# ------------------------------------------------------------------------------

def create_basic_pipes_ext():
  return BoostExtension(PIPES_EXT_NAME, ["src/%s.cpp" % n for n in PIPES_SRC],
                        [])


def create_basic_hdfs_ext():
  return BoostExtension(HDFS_EXT_NAME, ["src/%s.cpp" % n for n in HDFS_SRC], [])


def create_full_pipes_ext():
  basedir = PATH_FINDER.hadoop_cpp_src
  serial_utils = os.path.join(basedir, "utils/impl/SerialUtils.cc")
  string_utils = os.path.join(basedir, "utils/impl/StringUtils.cc")
  pipes = os.path.join(basedir, "pipes/impl/HadoopPipes.cc")
  patches = {
    serial_utils: {
      OLD_DESERIALIZE_FLOAT: NEW_DESERIALIZE_FLOAT,
      OLD_SERIAL_UTILS_INCLUDE: NEW_SERIAL_UTILS_INCLUDE,
      },
    string_utils: {},
    pipes: {
      OLD_WRITE_BUFFER: NEW_WRITE_BUFFER,
      OLD_PIPES_INCLUDE: NEW_PIPES_INCLUDE,
      },
    }
  include_dirs = PATH_FINDER.mapred_inc
  libraries = ["pthread", BOOST_PYTHON, "ssl"]  # FIXME: not version-dep
  return BoostExtension(
    pydoop.complete_mod_name(PIPES_EXT_NAME, HADOOP_VERSION_INFO),
    ["src/%s.cpp" % n for n in PIPES_SRC],
    [],  # aux
    patches=patches,
    include_dirs=include_dirs,
    libraries=libraries
    )


def create_full_hdfs_ext():
  include_dirs = get_java_include_dirs(PATH_FINDER.java_home)
  include_dirs.extend(PATH_FINDER.hdfs_inc)
  library_dirs = get_java_library_dirs(PATH_FINDER.java_home)
  library_dirs.extend(PATH_FINDER.hdfs_link)
  return BoostExtension(
    pydoop.complete_mod_name(HDFS_EXT_NAME, HADOOP_VERSION_INFO),
    ["src/%s.cpp" % n for n in HDFS_SRC],
    [],  # aux
    include_dirs=include_dirs,
    library_dirs=library_dirs,
    runtime_library_dirs=library_dirs,
    libraries=["pthread", BOOST_PYTHON, "hdfs", "jvm"],
    define_macros=get_hdfs_macros(
      os.path.join(PATH_FINDER.hdfs_inc[0], "hdfs.h")
      ),
    )


# ---------------------------------------
# Custom distutils extension and commands
# ---------------------------------------

class BoostExtension(Extension):
  """
  Customized Extension class that generates the necessary Boost.Python
  export code.
  """
  export_pattern = re.compile(r"void\s+export_(\w+)")

  def __init__(self, name, wrap_sources, aux_sources, patches=None, **kw):
    Extension.__init__(self, name, wrap_sources+aux_sources, **kw)
    self.module_name = self.name.rsplit(".", 1)[-1]
    self.wrap_sources = wrap_sources
    self.patches = patches

  def generate_main(self):
    destdir = os.path.split(self.wrap_sources[0])[0]  # should be ok
    outfn = os.path.join(destdir, "%s_main.cpp" % self.module_name)
    if must_generate(outfn, self.wrap_sources):
      log.debug("generating main for %s\n" % self.name)
      first_half = ["#include <boost/python.hpp>"]
      second_half = ["BOOST_PYTHON_MODULE(%s){" % self.module_name]
      for fn in self.wrap_sources:
        with open(fn) as f:
          code = f.read()
        m = self.export_pattern.search(code)
        if m is not None:
          fun_name = "export_%s" % m.groups()[0]
          first_half.append("void %s();" % fun_name)
          second_half.append("%s();" % fun_name)
      second_half.append("}")
      with open(outfn, "w") as outf:
        for line in first_half:
          outf.write("%s%s" % (line, os.linesep))
        for line in second_half:
          outf.write("%s%s" % (line, os.linesep))
    return outfn

  def generate_patched_aux(self):
    aux = []
    if not self.patches:
      return aux
    for fn, p in self.patches.iteritems():
      patched_fn = "src/%s" % os.path.basename(fn)
      # FIXME: the patch should also be listed as a prerequisite.
      if must_generate(patched_fn, [fn]):
        log.debug("copying and patching %s" % fn)
        with open(fn) as f:
          contents = f.read()
        for old, new in p.iteritems():
          contents = contents.replace(old, new)
        with open(patched_fn, "w") as f:
          f.write(contents)
      aux.append(patched_fn)
    return aux


class build_pydoop_ext(distutils_build_ext):

  def finalize_options(self):
    distutils_build_ext.finalize_options(self)
    self.extensions = [
      create_full_pipes_ext(),
      create_full_hdfs_ext(),
      ]
    for e in self.extensions:
      e.sources.append(e.generate_main())
      e.sources.extend(e.generate_patched_aux())

  def build_extension(self, ext):
    try:
      self.compiler.compiler_so.remove("-Wstrict-prototypes")
    except ValueError:
      pass
    distutils_build_ext.build_extension(self, ext)


def create_ext_modules():
  ext_modules = []
  ext_modules.append(create_basic_pipes_ext())
  ext_modules.append(create_basic_hdfs_ext())
  return ext_modules


class pydoop_clean(distutils_clean):
  """
  Custom clean action that removes files generated by the build
  process.  In particular, the build process generates _*_main.cpp
  files for the boost extensions, and some patched Hadoop source code
  files, all inside the src directory.  These are removed when this
  clean action is executed.
  """
  def run(self):
    distutils_clean.run(self)
    this_dir = os.path.dirname(os.path.realpath(__file__))
    shutil.rmtree(os.path.join(this_dir, 'dist'), ignore_errors=True)
    pydoop_src_path = os.path.join(this_dir, 'src')
    r = re.compile('(%s|%s)_.*_main.cpp$' % (HDFS_EXT_NAME, PIPES_EXT_NAME))
    paths = filter(r.search, os.listdir(pydoop_src_path)) + \
            ['SerialUtils.cc', 'StringUtils.cc', 'HadoopPipes.cc']
    absolute_paths = [os.path.join(pydoop_src_path, f) for f in paths]
    for f in absolute_paths:
      if not self.dry_run:
        try:
          if os.path.exists(f):
            os.remove(f)
        except OSError as e:
          log.warn("Error removing file: %s" % e)


class pydoop_build(distutils_build):

  def run(self):
    log.info("hadoop_home: %r" % (HADOOP_HOME,))
    log.info("hadoop_version: %r" % (HADOOP_VERSION_INFO,))
    for a in (
      "java_home", "hadoop_cpp_src", "mapred_inc", "hdfs_inc", "hdfs_link"
      ):
      log.info("%s: %r" % (a, getattr(PATH_FINDER, a)))
    distutils_build.run(self)
    if HADOOP_VERSION_INFO >= (1, 0, 0):
      self.__build_java_component()

  def __build_java_component(self):
    compile_cmd = "javac"
    classpath = ':'.join(
        glob.glob(os.path.join(HADOOP_HOME, 'hadoop-*.jar')) +
        glob.glob(os.path.join(HADOOP_HOME, 'lib', '*.jar'))
      )
    if classpath:
      compile_cmd += " -classpath %s" % classpath
    else:
      log.warn("could not set classpath, java code may not compile")
    class_dir = os.path.join(self.build_temp, 'pydoop_java')
    package_path = os.path.join(self.build_lib, 'pydoop', pydoop.__jar_name__)
    if not os.path.exists(class_dir):
      os.mkdir(class_dir)
    compile_cmd += " -d '%s'" % class_dir
    java_files = ["src/it/crs4/pydoop/pipes/*"]
    log.info("Compiling Java classes")
    for f in java_files:
      compile_cmd += " %s" % f
      log.debug("Command: %s", compile_cmd)
      ret = os.system(compile_cmd)
      if ret:
        raise DistutilsSetupError(
          "Error compiling java component.  Command: %s" % compile_cmd
          )
    package_cmd = "jar -cf %(package_path)s -C %(class_dir)s ./it" % {
      'package_path': package_path, 'class_dir': class_dir
      }
    log.info("Packaging Java classes")
    log.debug("Command: %s", package_cmd)
    ret = os.system(package_cmd)
    if ret:
      raise DistutilsSetupError(
        "Error packaging java component.  Command: %s" % package_cmd
        )


class pydoop_build_py(distutils_build_py):

  def run(self):
    write_config()
    write_version()
    distutils_build_py.run(self)


setup(
  name="pydoop",
  version=get_version_string(),
  description=pydoop.__doc__.strip().splitlines()[0],
  long_description=pydoop.__doc__.lstrip(),
  author=pydoop.__author__,
  author_email=pydoop.__author_email__,
  url=pydoop.__url__,
  download_url="https://sourceforge.net/projects/pydoop/files/",
  packages=[
    "pydoop",
    "pydoop.hdfs",
    "pydoop.app",
    ],
  cmdclass={
    "build": pydoop_build,
    "build_py": pydoop_build_py,
    "build_ext": build_pydoop_ext,
    "clean": pydoop_clean
    },
  ext_modules=create_ext_modules(),
  scripts=["scripts/pydoop"],
  platforms=["Linux"],
  license="Apache-2.0",
  keywords=["hadoop", "mapreduce"],
  classifiers=[
    "Programming Language :: Python",
    "License :: OSI Approved :: Apache Software License",
    "Operating System :: POSIX :: Linux",
    "Topic :: Software Development :: Libraries :: Application Frameworks",
    "Intended Audience :: Developers",
    ],
  )

# vim: set sw=2 ts=2 et
