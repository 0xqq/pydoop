diff -urN src/hadoop-1.0.4/it/crs4/pydoop/pipes/Application.java src/hadoop-1.0.4.patched/it/crs4/pydoop/pipes/Application.java
--- src/hadoop-1.0.4/it/crs4/pydoop/pipes/Application.java	1970-01-01 01:00:00.000000000 +0100
+++ src/hadoop-1.0.4.patched/it/crs4/pydoop/pipes/Application.java	2013-02-13 23:15:48.687084967 +0100
@@ -0,0 +1,274 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package  it.crs4.pydoop.pipes;
+
+import java.io.File;
+import java.io.IOException;
+import java.net.ServerSocket;
+import java.net.Socket;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Random;
+
+import javax.crypto.SecretKey;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.filecache.DistributedCache;
+import org.apache.hadoop.fs.FSDataOutputStream;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.FileUtil;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.io.FloatWritable;
+import org.apache.hadoop.io.NullWritable;
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.io.WritableComparable;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.hadoop.mapred.RecordReader;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.TaskAttemptID;
+import org.apache.hadoop.mapred.TaskLog;
+import org.apache.hadoop.mapred.TaskTracker;
+import org.apache.hadoop.mapreduce.security.SecureShuffleUtils;
+import org.apache.hadoop.mapreduce.security.TokenCache;
+import org.apache.hadoop.mapreduce.security.token.JobTokenIdentifier;
+import org.apache.hadoop.mapreduce.security.token.JobTokenSecretManager;
+import org.apache.hadoop.security.token.Token;
+import org.apache.hadoop.util.ReflectionUtils;
+import org.apache.hadoop.util.StringUtils;
+
+/**
+ * This class is responsible for launching and communicating with the child 
+ * process.
+ */
+class Application<K1 extends WritableComparable, V1 extends Writable,
+                  K2 extends WritableComparable, V2 extends Writable> {
+  private static final Log LOG = LogFactory.getLog(Application.class.getName());
+  private ServerSocket serverSocket;
+  private Process process;
+  private Socket clientSocket;
+  private OutputHandler<K2, V2> handler;
+  private DownwardProtocol<K1, V1> downlink;
+  static final boolean WINDOWS
+  = System.getProperty("os.name").startsWith("Windows");
+
+  /**
+   * Start the child process to handle the task for us.
+   * @param conf the task's configuration
+   * @param recordReader the fake record reader to update progress with
+   * @param output the collector to send output to
+   * @param reporter the reporter for the task
+   * @param outputKeyClass the class of the output keys
+   * @param outputValueClass the class of the output values
+   * @throws IOException
+   * @throws InterruptedException
+   */
+  Application(JobConf conf, 
+              RecordReader<FloatWritable, NullWritable> recordReader, 
+              OutputCollector<K2,V2> output, Reporter reporter,
+              Class<? extends K2> outputKeyClass,
+              Class<? extends V2> outputValueClass
+              ) throws IOException, InterruptedException {
+    serverSocket = new ServerSocket(0);
+    Map<String, String> env = new HashMap<String,String>();
+    // add TMPDIR environment variable with the value of java.io.tmpdir
+    env.put("TMPDIR", System.getProperty("java.io.tmpdir"));
+    env.put("hadoop.pipes.command.port", 
+            Integer.toString(serverSocket.getLocalPort()));
+    
+    TaskAttemptID taskid = TaskAttemptID.forName(conf.get("mapred.task.id"));
+
+    // get the task's working directory
+    String workDir = TaskTracker.getLocalTaskDir(conf.getUser(),
+            taskid.getJobID().toString(),
+            taskid.getTaskID().toString());
+
+    //Add token to the environment if security is enabled
+    Token<JobTokenIdentifier> jobToken = TokenCache.getJobToken(conf
+        .getCredentials());
+    // This password is used as shared secret key between this application and
+    // child pipes process
+    byte[]  password = jobToken.getPassword();
+
+    String localPasswordFile = new File(workDir, "jobTokenPassword").getAbsolutePath();
+    writePasswordToLocalFile(localPasswordFile, password, conf);
+    env.put("hadoop.pipes.shared.secret.location", localPasswordFile);
+ 
+    List<String> cmd = new ArrayList<String>();
+    String interpretor = conf.get("hadoop.pipes.executable.interpretor");
+    if (interpretor != null) {
+      cmd.add(interpretor);
+    }
+
+    String executable = DistributedCache.getLocalCacheFiles(conf)[0].toString();
+    if (!new File(executable).canExecute()) {
+      // LinuxTaskController sets +x permissions on all distcache files already.
+      // In case of DefaultTaskController, set permissions here.
+      FileUtil.chmod(executable, "u+x");
+    }
+    cmd.add(executable);
+    // wrap the command in a stdout/stderr capture
+    // we are starting map/reduce task of the pipes job. this is not a cleanup
+    // attempt. 
+    File stdout = TaskLog.getTaskLogFile(taskid, false, TaskLog.LogName.STDOUT);
+    File stderr = TaskLog.getTaskLogFile(taskid, false, TaskLog.LogName.STDERR);
+    long logLength = TaskLog.getTaskLogLength(conf);
+    cmd = TaskLog.captureOutAndError(null, cmd, stdout, stderr, logLength,
+        false);
+
+    process = runClient(cmd, env);
+    clientSocket = serverSocket.accept();
+    
+    String challenge = getSecurityChallenge();
+    String digestToSend = createDigest(password, challenge);
+    String digestExpected = createDigest(password, digestToSend);
+    
+    handler = new OutputHandler<K2, V2>(output, reporter, recordReader, 
+        digestExpected);
+    K2 outputKey = (K2)
+      ReflectionUtils.newInstance(outputKeyClass, conf);
+    V2 outputValue = (V2) 
+      ReflectionUtils.newInstance(outputValueClass, conf);
+    downlink = new BinaryProtocol<K1, V1, K2, V2>(clientSocket, handler, 
+                                  outputKey, outputValue, conf);
+    
+    downlink.authenticate(digestToSend, challenge);
+    waitForAuthentication();
+    LOG.debug("Authentication succeeded");
+    downlink.start();
+    downlink.setJobConf(conf);
+  }
+
+  private String getSecurityChallenge() {
+    Random rand = new Random(System.currentTimeMillis());
+    //Use 4 random integers so as to have 16 random bytes.
+    StringBuilder strBuilder = new StringBuilder();
+    strBuilder.append(rand.nextInt(0x7fffffff));
+    strBuilder.append(rand.nextInt(0x7fffffff));
+    strBuilder.append(rand.nextInt(0x7fffffff));
+    strBuilder.append(rand.nextInt(0x7fffffff));
+    return strBuilder.toString();
+  }
+
+  private void writePasswordToLocalFile(String localPasswordFile,
+      byte[] password, JobConf conf) throws IOException {
+    FileSystem localFs = FileSystem.getLocal(conf);
+    Path localPath = new Path(localPasswordFile);
+    FSDataOutputStream out = FileSystem.create(localFs, localPath,
+        new FsPermission("400"));
+    out.write(password);
+    out.close();
+  }
+
+  /**
+   * Get the downward protocol object that can send commands down to the
+   * application.
+   * @return the downlink proxy
+   */
+  DownwardProtocol<K1, V1> getDownlink() {
+    return downlink;
+  }
+  
+  /**
+   * Wait for authentication response.
+   * @throws IOException
+   * @throws InterruptedException
+   */
+  void waitForAuthentication() throws IOException,
+      InterruptedException {
+    downlink.flush();
+    LOG.debug("Waiting for authentication response");
+    handler.waitForAuthentication();
+  }
+  
+  /**
+   * Wait for the application to finish
+   * @return did the application finish correctly?
+   * @throws Throwable
+   */
+  boolean waitForFinish() throws Throwable {
+    downlink.flush();
+    return handler.waitForFinish();
+  }
+
+  /**
+   * Abort the application and wait for it to finish.
+   * @param t the exception that signalled the problem
+   * @throws IOException A wrapper around the exception that was passed in
+   */
+  void abort(Throwable t) throws IOException {
+    LOG.info("Aborting because of " + StringUtils.stringifyException(t));
+    try {
+      downlink.abort();
+      downlink.flush();
+    } catch (IOException e) {
+      // IGNORE cleanup problems
+    }
+    try {
+      handler.waitForFinish();
+    } catch (Throwable ignored) {
+      process.destroy();
+    }
+    IOException wrapper = new IOException("pipe child exception");
+    wrapper.initCause(t);
+    throw wrapper;      
+  }
+  
+  /**
+   * Clean up the child procress and socket.
+   * @throws IOException
+   */
+  void cleanup() throws IOException {
+    serverSocket.close();
+    try {
+      downlink.close();
+    } catch (InterruptedException ie) {
+      Thread.currentThread().interrupt();
+    }      
+  }
+
+  /**
+   * Run a given command in a subprocess, including threads to copy its stdout
+   * and stderr to our stdout and stderr.
+   * @param command the command and its arguments
+   * @param env the environment to run the process in
+   * @return a handle on the process
+   * @throws IOException
+   */
+  static Process runClient(List<String> command, 
+                           Map<String, String> env) throws IOException {
+    ProcessBuilder builder = new ProcessBuilder(command);
+    if (env != null) {
+      builder.environment().putAll(env);
+    }
+    Process result = builder.start();
+    return result;
+  }
+  
+  public static String createDigest(byte[] password, String data)
+      throws IOException {
+    SecretKey key = JobTokenSecretManager.createSecretKey(password);
+    return SecureShuffleUtils.hashFromString(data, key);
+  }
+
+}
diff -urN src/hadoop-1.0.4/it/crs4/pydoop/pipes/BinaryProtocol.java src/hadoop-1.0.4.patched/it/crs4/pydoop/pipes/BinaryProtocol.java
--- src/hadoop-1.0.4/it/crs4/pydoop/pipes/BinaryProtocol.java	1970-01-01 01:00:00.000000000 +0100
+++ src/hadoop-1.0.4.patched/it/crs4/pydoop/pipes/BinaryProtocol.java	2013-02-13 23:15:48.687084967 +0100
@@ -0,0 +1,370 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package  it.crs4.pydoop.pipes;
+
+import java.io.*;
+import java.net.Socket;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+
+import javax.crypto.SecretKey;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.io.BytesWritable;
+import org.apache.hadoop.io.DataOutputBuffer;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.io.WritableComparable;
+import org.apache.hadoop.io.WritableUtils;
+import org.apache.hadoop.mapred.InputSplit;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapreduce.security.SecureShuffleUtils;
+import org.apache.hadoop.mapreduce.security.token.JobTokenSecretManager;
+import org.apache.hadoop.util.StringUtils;
+
+/**
+ * This protocol is a binary implementation of the Pipes protocol.
+ */
+class BinaryProtocol<K1 extends WritableComparable, V1 extends Writable,
+                     K2 extends WritableComparable, V2 extends Writable>
+  implements DownwardProtocol<K1, V1> {
+  
+  public static final int CURRENT_PROTOCOL_VERSION = 0;
+  /**
+   * The buffer size for the command socket
+   */
+  private static final int BUFFER_SIZE = 128*1024;
+
+  private DataOutputStream stream;
+  private DataOutputBuffer buffer = new DataOutputBuffer();
+  private static final Log LOG = 
+    LogFactory.getLog(BinaryProtocol.class.getName());
+  private UplinkReaderThread uplink;
+
+  /**
+   * The integer codes to represent the different messages. These must match
+   * the C++ codes or massive confusion will result.
+   */
+  private static enum MessageType { START(0),
+                                    SET_JOB_CONF(1),
+                                    SET_INPUT_TYPES(2),
+                                    RUN_MAP(3),
+                                    MAP_ITEM(4),
+                                    RUN_REDUCE(5),
+                                    REDUCE_KEY(6),
+                                    REDUCE_VALUE(7),
+                                    CLOSE(8),
+                                    ABORT(9),
+                                    AUTHENTICATION_REQ(10),
+                                    OUTPUT(50),
+                                    PARTITIONED_OUTPUT(51),
+                                    STATUS(52),
+                                    PROGRESS(53),
+                                    DONE(54),
+                                    REGISTER_COUNTER(55),
+                                    INCREMENT_COUNTER(56),
+                                    AUTHENTICATION_RESP(57);
+    final int code;
+    MessageType(int code) {
+      this.code = code;
+    }
+  }
+
+  private static class UplinkReaderThread<K2 extends WritableComparable,
+                                          V2 extends Writable>  
+    extends Thread {
+    
+    private DataInputStream inStream;
+    private UpwardProtocol<K2, V2> handler;
+    private K2 key;
+    private V2 value;
+    private boolean authPending = true;
+    
+    public UplinkReaderThread(InputStream stream,
+                              UpwardProtocol<K2, V2> handler, 
+                              K2 key, V2 value) throws IOException{
+      inStream = new DataInputStream(new BufferedInputStream(stream, 
+                                                             BUFFER_SIZE));
+      this.handler = handler;
+      this.key = key;
+      this.value = value;
+    }
+
+    public void closeConnection() throws IOException {
+      inStream.close();
+    }
+
+    public void run() {
+      while (true) {
+        try {
+          if (Thread.currentThread().isInterrupted()) {
+            throw new InterruptedException();
+          }
+          int cmd = WritableUtils.readVInt(inStream);
+          LOG.debug("Handling uplink command " + cmd);
+          if (cmd == MessageType.AUTHENTICATION_RESP.code) {
+            String digest = Text.readString(inStream);
+            authPending = !handler.authenticate(digest);
+          } else if (authPending) {
+            LOG.warn("Message " + cmd + " received before authentication is "
+                + "complete. Ignoring");
+            continue;
+          } else if (cmd == MessageType.OUTPUT.code) {
+            readObject(key);
+            readObject(value);
+            handler.output(key, value);
+          } else if (cmd == MessageType.PARTITIONED_OUTPUT.code) {
+            int part = WritableUtils.readVInt(inStream);
+            readObject(key);
+            readObject(value);
+            handler.partitionedOutput(part, key, value);
+          } else if (cmd == MessageType.STATUS.code) {
+            handler.status(Text.readString(inStream));
+          } else if (cmd == MessageType.PROGRESS.code) {
+            handler.progress(inStream.readFloat());
+          } else if (cmd == MessageType.REGISTER_COUNTER.code) {
+            int id = WritableUtils.readVInt(inStream);
+            String group = Text.readString(inStream);
+            String name = Text.readString(inStream);
+            handler.registerCounter(id, group, name);
+          } else if (cmd == MessageType.INCREMENT_COUNTER.code) {
+            int id = WritableUtils.readVInt(inStream);
+            long amount = WritableUtils.readVLong(inStream);
+            handler.incrementCounter(id, amount);
+          } else if (cmd == MessageType.DONE.code) {
+            LOG.debug("Pipe child done");
+            handler.done();
+            return;
+          } else {
+            throw new IOException("Bad command code: " + cmd);
+          }
+        } catch (InterruptedException e) {
+          return;
+        } catch (Throwable e) {
+          LOG.error(StringUtils.stringifyException(e));
+          handler.failed(e);
+          return;
+        }
+      }
+    }
+    
+    private void readObject(Writable obj) throws IOException {
+      int numBytes = WritableUtils.readVInt(inStream);
+      byte[] buffer;
+      // For BytesWritable and Text, use the specified length to set the length
+      // this causes the "obvious" translations to work. So that if you emit
+      // a string "abc" from C++, it shows up as "abc".
+      if (obj instanceof BytesWritable) {
+        buffer = new byte[numBytes];
+        inStream.readFully(buffer);
+        ((BytesWritable) obj).set(buffer, 0, numBytes);
+      } else if (obj instanceof Text) {
+        buffer = new byte[numBytes];
+        inStream.readFully(buffer);
+        ((Text) obj).set(buffer);
+      } else {
+        obj.readFields(inStream);
+      }
+    }
+  }
+
+  /**
+   * An output stream that will save a copy of the data into a file.
+   */
+  private static class TeeOutputStream extends FilterOutputStream {
+    private OutputStream file;
+    TeeOutputStream(String filename, OutputStream base) throws IOException {
+      super(base);
+      file = new FileOutputStream(filename);
+    }
+    public void write(byte b[], int off, int len) throws IOException {
+      file.write(b,off,len);
+      out.write(b,off,len);
+    }
+
+    public void write(int b) throws IOException {
+      file.write(b);
+      out.write(b);
+    }
+
+    public void flush() throws IOException {
+      file.flush();
+      out.flush();
+    }
+
+    public void close() throws IOException {
+      flush();
+      file.close();
+      out.close();
+    }
+  }
+
+  /**
+   * Create a proxy object that will speak the binary protocol on a socket.
+   * Upward messages are passed on the specified handler and downward
+   * downward messages are public methods on this object.
+   * @param sock The socket to communicate on.
+   * @param handler The handler for the received messages.
+   * @param key The object to read keys into.
+   * @param value The object to read values into.
+   * @param config The job's configuration
+   * @throws IOException
+   */
+  public BinaryProtocol(Socket sock, 
+                        UpwardProtocol<K2, V2> handler,
+                        K2 key,
+                        V2 value,
+                        JobConf config) throws IOException {
+    OutputStream raw = sock.getOutputStream();
+    // If we are debugging, save a copy of the downlink commands to a file
+    if (Submitter.getKeepCommandFile(config)) {
+      raw = new TeeOutputStream("downlink.data", raw);
+    }
+    stream = new DataOutputStream(new BufferedOutputStream(raw, 
+                                                           BUFFER_SIZE)) ;
+    uplink = new UplinkReaderThread<K2, V2>(sock.getInputStream(),
+                                            handler, key, value);
+    uplink.setName("pipe-uplink-handler");
+    uplink.start();
+  }
+
+  /**
+   * Close the connection and shutdown the handler thread.
+   * @throws IOException
+   * @throws InterruptedException
+   */
+  public void close() throws IOException, InterruptedException {
+    LOG.debug("closing connection");
+    stream.close();
+    uplink.closeConnection();
+    uplink.interrupt();
+    uplink.join();
+  }
+  
+  public void authenticate(String digest, String challenge)
+      throws IOException {
+    LOG.debug("Sending AUTHENTICATION_REQ, digest=" + digest + ", challenge="
+        + challenge);
+    WritableUtils.writeVInt(stream, MessageType.AUTHENTICATION_REQ.code);
+    Text.writeString(stream, digest);
+    Text.writeString(stream, challenge);
+  }
+
+  public void start() throws IOException {
+    LOG.debug("starting downlink");
+    WritableUtils.writeVInt(stream, MessageType.START.code);
+    WritableUtils.writeVInt(stream, CURRENT_PROTOCOL_VERSION);
+  }
+
+  public void setJobConf(JobConf job) throws IOException {
+    WritableUtils.writeVInt(stream, MessageType.SET_JOB_CONF.code);
+    List<String> list = new ArrayList<String>();
+    for(Map.Entry<String, String> itm: job) {
+      list.add(itm.getKey());
+      list.add(itm.getValue());
+    }
+    WritableUtils.writeVInt(stream, list.size());
+    for(String entry: list){
+      Text.writeString(stream, entry);
+    }
+  }
+
+  public void setInputTypes(String keyType, 
+                            String valueType) throws IOException {
+    WritableUtils.writeVInt(stream, MessageType.SET_INPUT_TYPES.code);
+    Text.writeString(stream, keyType);
+    Text.writeString(stream, valueType);
+  }
+
+  public void runMap(InputSplit split, int numReduces, 
+                     boolean pipedInput) throws IOException {
+    WritableUtils.writeVInt(stream, MessageType.RUN_MAP.code);
+    writeObject(split);
+    WritableUtils.writeVInt(stream, numReduces);
+    WritableUtils.writeVInt(stream, pipedInput ? 1 : 0);
+  }
+
+  public void mapItem(WritableComparable key, 
+                      Writable value) throws IOException {
+    WritableUtils.writeVInt(stream, MessageType.MAP_ITEM.code);
+    writeObject(key);
+    writeObject(value);
+  }
+
+  public void runReduce(int reduce, boolean pipedOutput) throws IOException {
+    WritableUtils.writeVInt(stream, MessageType.RUN_REDUCE.code);
+    WritableUtils.writeVInt(stream, reduce);
+    WritableUtils.writeVInt(stream, pipedOutput ? 1 : 0);
+  }
+
+  public void reduceKey(WritableComparable key) throws IOException {
+    WritableUtils.writeVInt(stream, MessageType.REDUCE_KEY.code);
+    writeObject(key);
+  }
+
+  public void reduceValue(Writable value) throws IOException {
+    WritableUtils.writeVInt(stream, MessageType.REDUCE_VALUE.code);
+    writeObject(value);
+  }
+
+  public void endOfInput() throws IOException {
+    WritableUtils.writeVInt(stream, MessageType.CLOSE.code);
+    LOG.debug("Sent close command");
+  }
+  
+  public void abort() throws IOException {
+    WritableUtils.writeVInt(stream, MessageType.ABORT.code);
+    LOG.debug("Sent abort command");
+  }
+
+  public void flush() throws IOException {
+    stream.flush();
+  }
+
+  /**
+   * Write the given object to the stream. If it is a Text or BytesWritable,
+   * write it directly. Otherwise, write it to a buffer and then write the
+   * length and data to the stream.
+   * @param obj the object to write
+   * @throws IOException
+   */
+  private void writeObject(Writable obj) throws IOException {
+    // For Text and BytesWritable, encode them directly, so that they end up
+    // in C++ as the natural translations.
+    if (obj instanceof Text) {
+      Text t = (Text) obj;
+      int len = t.getLength();
+      WritableUtils.writeVInt(stream, len);
+      stream.write(t.getBytes(), 0, len);
+    } else if (obj instanceof BytesWritable) {
+      BytesWritable b = (BytesWritable) obj;
+      int len = b.getLength();
+      WritableUtils.writeVInt(stream, len);
+      stream.write(b.getBytes(), 0, len);
+    } else {
+      buffer.reset();
+      obj.write(buffer);
+      int length = buffer.getLength();
+      WritableUtils.writeVInt(stream, length);
+      stream.write(buffer.getData(), 0, length);
+    }
+  }
+}
diff -urN src/hadoop-1.0.4/it/crs4/pydoop/pipes/DownwardProtocol.java src/hadoop-1.0.4.patched/it/crs4/pydoop/pipes/DownwardProtocol.java
--- src/hadoop-1.0.4/it/crs4/pydoop/pipes/DownwardProtocol.java	1970-01-01 01:00:00.000000000 +0100
+++ src/hadoop-1.0.4.patched/it/crs4/pydoop/pipes/DownwardProtocol.java	2013-02-13 23:15:48.687084967 +0100
@@ -0,0 +1,123 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package  it.crs4.pydoop.pipes;
+
+import java.io.IOException;
+
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.io.WritableComparable;
+import org.apache.hadoop.mapred.InputSplit;
+import org.apache.hadoop.mapred.JobConf;
+
+/**
+ * The abstract description of the downward (from Java to C++) Pipes protocol.
+ * All of these calls are asynchronous and return before the message has been 
+ * processed.
+ */
+interface DownwardProtocol<K extends WritableComparable, V extends Writable> {
+  /**
+   * request authentication
+   * @throws IOException
+   */
+  void authenticate(String digest, String challenge) throws IOException;
+  
+  /**
+   * Start communication
+   * @throws IOException
+   */
+  void start() throws IOException;
+  
+  /**
+   * Set the JobConf for the task.
+   * @param conf
+   * @throws IOException
+   */
+  void setJobConf(JobConf conf) throws IOException;
+  
+  /**
+   * Set the input types for Maps.
+   * @param keyType the name of the key's type
+   * @param valueType the name of the value's type
+   * @throws IOException
+   */
+  void setInputTypes(String keyType, String valueType) throws IOException;
+  
+  /**
+   * Run a map task in the child.
+   * @param split The input split for this map.
+   * @param numReduces The number of reduces for this job.
+   * @param pipedInput Is the input coming from Java?
+   * @throws IOException
+   */
+  void runMap(InputSplit split, int numReduces, 
+              boolean pipedInput) throws IOException;
+  
+  /**
+   * For maps with pipedInput, the key/value pairs are sent via this messaage.
+   * @param key The record's key
+   * @param value The record's value
+   * @throws IOException
+   */
+  void mapItem(K key, V value) throws IOException;
+  
+  /**
+   * Run a reduce task in the child
+   * @param reduce the index of the reduce (0 .. numReduces - 1)
+   * @param pipedOutput is the output being sent to Java?
+   * @throws IOException
+   */
+  void runReduce(int reduce, boolean pipedOutput) throws IOException;
+  
+  /**
+   * The reduce should be given a new key
+   * @param key the new key
+   * @throws IOException
+   */
+  void reduceKey(K key) throws IOException;
+  
+  /**
+   * The reduce should be given a new value
+   * @param value the new value
+   * @throws IOException
+   */
+  void reduceValue(V value) throws IOException;
+  
+  /**
+   * The task has no more input coming, but it should finish processing it's 
+   * input.
+   * @throws IOException
+   */
+  void endOfInput() throws IOException;
+  
+  /**
+   * The task should stop as soon as possible, because something has gone wrong.
+   * @throws IOException
+   */
+  void abort() throws IOException;
+  
+  /**
+   * Flush the data through any buffers.
+   */
+  void flush() throws IOException;
+  
+  /**
+   * Close the connection.
+   */
+  void close() throws IOException, InterruptedException;
+}
diff -urN src/hadoop-1.0.4/it/crs4/pydoop/pipes/OutputHandler.java src/hadoop-1.0.4.patched/it/crs4/pydoop/pipes/OutputHandler.java
--- src/hadoop-1.0.4/it/crs4/pydoop/pipes/OutputHandler.java	1970-01-01 01:00:00.000000000 +0100
+++ src/hadoop-1.0.4.patched/it/crs4/pydoop/pipes/OutputHandler.java	2013-02-13 23:15:48.687084967 +0100
@@ -0,0 +1,191 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package  it.crs4.pydoop.pipes;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.hadoop.io.FloatWritable;
+import org.apache.hadoop.io.NullWritable;
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.io.WritableComparable;
+import org.apache.hadoop.mapred.Counters;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.hadoop.mapred.RecordReader;
+import org.apache.hadoop.mapred.Reporter;
+
+/**
+ * Handles the upward (C++ to Java) messages from the application.
+ */
+class OutputHandler<K extends WritableComparable,
+                    V extends Writable>
+  implements UpwardProtocol<K, V> {
+  
+  private Reporter reporter;
+  private OutputCollector<K, V> collector;
+  private float progressValue = 0.0f;
+  private boolean done = false;
+  
+  private Throwable exception = null;
+  RecordReader<FloatWritable,NullWritable> recordReader = null;
+  private Map<Integer, Counters.Counter> registeredCounters = 
+    new HashMap<Integer, Counters.Counter>();
+
+  private String expectedDigest = null;
+  private boolean digestReceived = false;
+  /**
+   * Create a handler that will handle any records output from the application.
+   * @param collector the "real" collector that takes the output
+   * @param reporter the reporter for reporting progress
+   */
+  public OutputHandler(OutputCollector<K, V> collector, Reporter reporter, 
+                       RecordReader<FloatWritable,NullWritable> recordReader,
+                       String expectedDigest) {
+    this.reporter = reporter;
+    this.collector = collector;
+    this.recordReader = recordReader;
+    this.expectedDigest = expectedDigest;
+  }
+
+  /**
+   * The task output a normal record.
+   */
+  public void output(K key, V value) throws IOException {
+    collector.collect(key, value);
+  }
+
+  /**
+   * The task output a record with a partition number attached.
+   */
+  public void partitionedOutput(int reduce, K key, 
+                                V value) throws IOException {
+    PipesPartitioner.setNextPartition(reduce);
+    collector.collect(key, value);
+  }
+
+  /**
+   * Update the status message for the task.
+   */
+  public void status(String msg) {
+    reporter.setStatus(msg);
+  }
+
+  private FloatWritable progressKey = new FloatWritable(0.0f);
+  private NullWritable nullValue = NullWritable.get();
+  /**
+   * Update the amount done and call progress on the reporter.
+   */
+  public void progress(float progress) throws IOException {
+    progressValue = progress;
+    reporter.progress();
+    
+    if (recordReader != null) {
+      progressKey.set(progress);
+      recordReader.next(progressKey, nullValue);
+    }
+  }
+
+  /**
+   * The task finished successfully.
+   */
+  public void done() throws IOException {
+    synchronized (this) {
+      done = true;
+      notify();
+    }
+  }
+
+  /**
+   * Get the current amount done.
+   * @return a float between 0.0 and 1.0
+   */
+  public float getProgress() {
+    return progressValue;
+  }
+
+  /**
+   * The task failed with an exception.
+   */
+  public void failed(Throwable e) {
+    synchronized (this) {
+      exception = e;
+      notify();
+    }
+  }
+
+  /**
+   * Wait for the task to finish or abort.
+   * @return did the task finish correctly?
+   * @throws Throwable
+   */
+  public synchronized boolean waitForFinish() throws Throwable {
+    while (!done && exception == null) {
+      wait();
+    }
+    if (exception != null) {
+      throw exception;
+    }
+    return done;
+  }
+
+  public void registerCounter(int id, String group, String name) throws IOException {
+    Counters.Counter counter = reporter.getCounter(group, name);
+    registeredCounters.put(id, counter);
+  }
+
+  public void incrementCounter(int id, long amount) throws IOException {
+    if (id < registeredCounters.size()) {
+      Counters.Counter counter = registeredCounters.get(id);
+      counter.increment(amount);
+    } else {
+      throw new IOException("Invalid counter with id: " + id);
+    }
+  }
+  
+  public synchronized boolean authenticate(String digest) throws IOException {
+    boolean success = true;
+    if (!expectedDigest.equals(digest)) {
+      exception = new IOException("Authentication Failed: Expected digest="
+          + expectedDigest + ", received=" + digestReceived);
+      success = false;
+    }
+    digestReceived = true;
+    notify();
+    return success;
+  }
+
+  /**
+   * This is called by Application and blocks the thread until
+   * authentication response is received.
+   * @throws IOException
+   * @throws InterruptedException
+   */
+  synchronized void waitForAuthentication()
+      throws IOException, InterruptedException {
+    while (digestReceived == false && exception == null) {
+      wait();
+    }
+    if (exception != null) {
+      throw new IOException(exception.getMessage());
+    }
+  }
+}
diff -urN src/hadoop-1.0.4/it/crs4/pydoop/pipes/PipesMapRunner.java src/hadoop-1.0.4.patched/it/crs4/pydoop/pipes/PipesMapRunner.java
--- src/hadoop-1.0.4/it/crs4/pydoop/pipes/PipesMapRunner.java	1970-01-01 01:00:00.000000000 +0100
+++ src/hadoop-1.0.4.patched/it/crs4/pydoop/pipes/PipesMapRunner.java	2013-02-13 23:15:48.687084967 +0100
@@ -0,0 +1,107 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package  it.crs4.pydoop.pipes;
+
+import java.io.IOException;
+
+import org.apache.hadoop.io.FloatWritable;
+import org.apache.hadoop.io.NullWritable;
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.io.WritableComparable;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.MapRunner;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.hadoop.mapred.RecordReader;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.SkipBadRecords;
+
+/**
+ * An adaptor to run a C++ mapper.
+ */
+class PipesMapRunner<K1 extends WritableComparable, V1 extends Writable,
+    K2 extends WritableComparable, V2 extends Writable>
+    extends MapRunner<K1, V1, K2, V2> {
+  private JobConf job;
+
+  /**
+   * Get the new configuration.
+   * @param job the job's configuration
+   */
+  public void configure(JobConf job) {
+    this.job = job;
+    //disable the auto increment of the counter. For pipes, no of processed 
+    //records could be different(equal or less) than the no of records input.
+    SkipBadRecords.setAutoIncrMapperProcCount(job, false);
+  }
+
+  /**
+   * Run the map task.
+   * @param input the set of inputs
+   * @param output the object to collect the outputs of the map
+   * @param reporter the object to update with status
+   */
+  @SuppressWarnings("unchecked")
+  public void run(RecordReader<K1, V1> input, OutputCollector<K2, V2> output,
+                  Reporter reporter) throws IOException {
+    Application<K1, V1, K2, V2> application = null;
+    try {
+      RecordReader<FloatWritable, NullWritable> fakeInput = 
+        (!Submitter.getIsJavaRecordReader(job) && 
+         !Submitter.getIsJavaMapper(job)) ? 
+	  (RecordReader<FloatWritable, NullWritable>) input : null;
+      application = new Application<K1, V1, K2, V2>(job, fakeInput, output, 
+                                                    reporter,
+          (Class<? extends K2>) job.getOutputKeyClass(), 
+          (Class<? extends V2>) job.getOutputValueClass());
+    } catch (InterruptedException ie) {
+      throw new RuntimeException("interrupted", ie);
+    }
+    DownwardProtocol<K1, V1> downlink = application.getDownlink();
+    boolean isJavaInput = Submitter.getIsJavaRecordReader(job);
+    downlink.runMap(reporter.getInputSplit(), 
+                    job.getNumReduceTasks(), isJavaInput);
+    boolean skipping = job.getBoolean("mapred.skip.on", false);
+    try {
+      if (isJavaInput) {
+        // allocate key & value instances that are re-used for all entries
+        K1 key = input.createKey();
+        V1 value = input.createValue();
+        downlink.setInputTypes(key.getClass().getName(),
+                               value.getClass().getName());
+        
+        while (input.next(key, value)) {
+          // map pair to output
+          downlink.mapItem(key, value);
+          if(skipping) {
+            //flush the streams on every record input if running in skip mode
+            //so that we don't buffer other records surrounding a bad record.
+            downlink.flush();
+          }
+        }
+        downlink.endOfInput();
+      }
+      application.waitForFinish();
+    } catch (Throwable t) {
+      application.abort(t);
+    } finally {
+      application.cleanup();
+    }
+  }
+  
+}
diff -urN src/hadoop-1.0.4/it/crs4/pydoop/pipes/PipesNonJavaInputFormat.java src/hadoop-1.0.4.patched/it/crs4/pydoop/pipes/PipesNonJavaInputFormat.java
--- src/hadoop-1.0.4/it/crs4/pydoop/pipes/PipesNonJavaInputFormat.java	1970-01-01 01:00:00.000000000 +0100
+++ src/hadoop-1.0.4.patched/it/crs4/pydoop/pipes/PipesNonJavaInputFormat.java	2013-02-13 23:15:48.687084967 +0100
@@ -0,0 +1,101 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package  it.crs4.pydoop.pipes;
+
+import java.io.IOException;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.io.FloatWritable;
+import org.apache.hadoop.io.NullWritable;
+import org.apache.hadoop.mapred.InputFormat;
+import org.apache.hadoop.mapred.InputSplit;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.RecordReader;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.TextInputFormat;
+import org.apache.hadoop.util.ReflectionUtils;
+
+/**
+ * Dummy input format used when non-Java a {@link RecordReader} is used by
+ * the Pipes' application.
+ *
+ * The only useful thing this does is set up the Map-Reduce job to get the
+ * {@link PipesDummyRecordReader}, everything else left for the 'actual'
+ * InputFormat specified by the user which is given by 
+ * <i>mapred.pipes.user.inputformat</i>.
+ */
+class PipesNonJavaInputFormat 
+implements InputFormat<FloatWritable, NullWritable> {
+
+  public RecordReader<FloatWritable, NullWritable> getRecordReader(
+      InputSplit genericSplit, JobConf job, Reporter reporter)
+      throws IOException {
+    return new PipesDummyRecordReader(job, genericSplit);
+  }
+  
+  public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {
+    // Delegate the generation of input splits to the 'original' InputFormat
+    return ReflectionUtils.newInstance(
+        job.getClass("mapred.pipes.user.inputformat", 
+                     TextInputFormat.class, 
+                     InputFormat.class), job).getSplits(job, numSplits);
+  }
+
+  /**
+   * A dummy {@link org.apache.hadoop.mapred.RecordReader} to help track the
+   * progress of Hadoop Pipes' applications when they are using a non-Java
+   * <code>RecordReader</code>.
+   *
+   * The <code>PipesDummyRecordReader</code> is informed of the 'progress' of
+   * the task by the {@link OutputHandler#progress(float)} which calls the
+   * {@link #next(FloatWritable, NullWritable)} with the progress as the
+   * <code>key</code>.
+   */
+  static class PipesDummyRecordReader implements RecordReader<FloatWritable, NullWritable> {
+    float progress = 0.0f;
+    
+    public PipesDummyRecordReader(Configuration job, InputSplit split)
+    throws IOException{
+    }
+
+    
+    public FloatWritable createKey() {
+      return null;
+    }
+
+    public NullWritable createValue() {
+      return null;
+    }
+
+    public synchronized void close() throws IOException {}
+
+    public synchronized long getPos() throws IOException {
+      return 0;
+    }
+
+    public float getProgress() {
+      return progress;
+    }
+
+    public synchronized boolean next(FloatWritable key, NullWritable value)
+        throws IOException {
+      progress = key.get();
+      return true;
+    }
+  }
+}
diff -urN src/hadoop-1.0.4/it/crs4/pydoop/pipes/PipesPartitioner.java src/hadoop-1.0.4.patched/it/crs4/pydoop/pipes/PipesPartitioner.java
--- src/hadoop-1.0.4/it/crs4/pydoop/pipes/PipesPartitioner.java	1970-01-01 01:00:00.000000000 +0100
+++ src/hadoop-1.0.4.patched/it/crs4/pydoop/pipes/PipesPartitioner.java	2013-02-13 23:15:48.687084967 +0100
@@ -0,0 +1,69 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package  it.crs4.pydoop.pipes;
+
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.io.WritableComparable;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.Partitioner;
+import org.apache.hadoop.util.ReflectionUtils;
+
+/**
+ * This partitioner is one that can either be set manually per a record or it
+ * can fall back onto a Java partitioner that was set by the user.
+ */
+class PipesPartitioner<K extends WritableComparable,
+                       V extends Writable>
+  implements Partitioner<K, V> {
+  
+  private static ThreadLocal<Integer> cache = new ThreadLocal<Integer>();
+  private Partitioner<K, V> part = null;
+  
+  @SuppressWarnings("unchecked")
+  public void configure(JobConf conf) {
+    part =
+      ReflectionUtils.newInstance(Submitter.getJavaPartitioner(conf), conf);
+  }
+
+  /**
+   * Set the next key to have the given partition.
+   * @param newValue the next partition value
+   */
+  static void setNextPartition(int newValue) {
+    cache.set(newValue);
+  }
+
+  /**
+   * If a partition result was set manually, return it. Otherwise, we call
+   * the Java partitioner.
+   * @param key the key to partition
+   * @param value the value to partition
+   * @param numPartitions the number of reduces
+   */
+  public int getPartition(K key, V value, 
+                          int numPartitions) {
+    Integer result = cache.get();
+    if (result == null) {
+      return part.getPartition(key, value, numPartitions);
+    } else {
+      return result;
+    }
+  }
+
+}
diff -urN src/hadoop-1.0.4/it/crs4/pydoop/pipes/PipesReducer.java src/hadoop-1.0.4.patched/it/crs4/pydoop/pipes/PipesReducer.java
--- src/hadoop-1.0.4/it/crs4/pydoop/pipes/PipesReducer.java	1970-01-01 01:00:00.000000000 +0100
+++ src/hadoop-1.0.4.patched/it/crs4/pydoop/pipes/PipesReducer.java	2013-02-13 23:15:48.687084967 +0100
@@ -0,0 +1,125 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package  it.crs4.pydoop.pipes;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.io.WritableComparable;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.hadoop.mapred.Reducer;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.SkipBadRecords;
+
+import java.io.IOException;
+import java.util.Iterator;
+
+/**
+ * This class is used to talk to a C++ reduce task.
+ */
+class PipesReducer<K2 extends WritableComparable, V2 extends Writable,
+    K3 extends WritableComparable, V3 extends Writable>
+    implements Reducer<K2, V2, K3, V3> {
+  private static final Log LOG= LogFactory.getLog(PipesReducer.class.getName());
+  private JobConf job;
+  private Application<K2, V2, K3, V3> application = null;
+  private DownwardProtocol<K2, V2> downlink = null;
+  private boolean isOk = true;
+  private boolean skipping = false;
+
+  public void configure(JobConf job) {
+    this.job = job;
+    //disable the auto increment of the counter. For pipes, no of processed 
+    //records could be different(equal or less) than the no of records input.
+    SkipBadRecords.setAutoIncrReducerProcCount(job, false);
+    skipping = job.getBoolean("mapred.skip.on", false);
+  }
+
+  /**
+   * Process all of the keys and values. Start up the application if we haven't
+   * started it yet.
+   */
+  public void reduce(K2 key, Iterator<V2> values, 
+                     OutputCollector<K3, V3> output, Reporter reporter
+                     ) throws IOException {
+    isOk = false;
+    startApplication(output, reporter);
+    downlink.reduceKey(key);
+    while (values.hasNext()) {
+      downlink.reduceValue(values.next());
+    }
+    if(skipping) {
+      //flush the streams on every record input if running in skip mode
+      //so that we don't buffer other records surrounding a bad record.
+      downlink.flush();
+    }
+    isOk = true;
+  }
+
+  @SuppressWarnings("unchecked")
+  private void startApplication(OutputCollector<K3, V3> output, Reporter reporter) throws IOException {
+    if (application == null) {
+      try {
+        LOG.info("starting application");
+        application = 
+          new Application<K2, V2, K3, V3>(
+              job, null, output, reporter, 
+              (Class<? extends K3>) job.getOutputKeyClass(), 
+              (Class<? extends V3>) job.getOutputValueClass());
+        downlink = application.getDownlink();
+      } catch (InterruptedException ie) {
+        throw new RuntimeException("interrupted", ie);
+      }
+      int reduce=0;
+      downlink.runReduce(reduce, Submitter.getIsJavaRecordWriter(job));
+    }
+  }
+
+  /**
+   * Handle the end of the input by closing down the application.
+   */
+  public void close() throws IOException {
+    // if we haven't started the application, we have nothing to do
+    if (isOk) {
+      OutputCollector<K3, V3> nullCollector = new OutputCollector<K3, V3>() {
+        public void collect(K3 key, 
+                            V3 value) throws IOException {
+          // NULL
+        }
+      };
+      startApplication(nullCollector, Reporter.NULL);
+    }
+    try {
+      if (isOk) {
+        application.getDownlink().endOfInput();
+      } else {
+        // send the abort to the application and let it clean up
+        application.getDownlink().abort();
+      }
+      LOG.info("waiting for finish");
+      application.waitForFinish();
+      LOG.info("got done");
+    } catch (Throwable t) {
+      application.abort(t);
+    } finally {
+      application.cleanup();
+    }
+  }
+}
diff -urN src/hadoop-1.0.4/it/crs4/pydoop/pipes/Submitter.java src/hadoop-1.0.4.patched/it/crs4/pydoop/pipes/Submitter.java
--- src/hadoop-1.0.4/it/crs4/pydoop/pipes/Submitter.java	1970-01-01 01:00:00.000000000 +0100
+++ src/hadoop-1.0.4.patched/it/crs4/pydoop/pipes/Submitter.java	2013-02-13 23:15:48.687084967 +0100
@@ -0,0 +1,498 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package  it.crs4.pydoop.pipes;
+
+import java.io.IOException;
+import java.net.URI;
+import java.net.URISyntaxException;
+import java.net.URL;
+import java.net.URLClassLoader;
+import java.security.AccessController;
+import java.security.PrivilegedAction;
+import java.util.Iterator;
+import java.util.StringTokenizer;
+
+import org.apache.commons.cli.BasicParser;
+import org.apache.commons.cli.CommandLine;
+import org.apache.commons.cli.Option;
+import org.apache.commons.cli.OptionBuilder;
+import org.apache.commons.cli.OptionGroup;
+import org.apache.commons.cli.CommandLineParser;
+import org.apache.commons.cli.Options;
+import org.apache.commons.cli.ParseException;
+import org.apache.commons.cli.Parser;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.conf.Configured;
+import org.apache.hadoop.filecache.DistributedCache;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapred.FileInputFormat;
+import org.apache.hadoop.mapred.FileOutputFormat;
+import org.apache.hadoop.mapred.InputFormat;
+import org.apache.hadoop.mapred.JobClient;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.Mapper;
+import org.apache.hadoop.mapred.OutputFormat;
+import org.apache.hadoop.mapred.Partitioner;
+import org.apache.hadoop.mapred.Reducer;
+import org.apache.hadoop.mapred.RunningJob;
+import org.apache.hadoop.mapred.lib.HashPartitioner;
+import org.apache.hadoop.mapred.lib.NullOutputFormat;
+import org.apache.hadoop.util.GenericOptionsParser;
+import org.apache.hadoop.util.Tool;
+
+/**
+ * The main entry point and job submitter. It may either be used as a command
+ * line-based or API-based method to launch Pipes jobs.
+ */
+public class Submitter extends Configured implements Tool {
+
+  protected static final Log LOG = LogFactory.getLog(Submitter.class);
+  
+  public Submitter() {
+    this(new Configuration());
+  }
+  
+  public Submitter(Configuration conf) {
+    setConf(conf);
+  }
+  
+  /**
+   * Get the URI of the application's executable.
+   * @param conf
+   * @return the URI where the application's executable is located
+   */
+  public static String getExecutable(JobConf conf) {
+    return conf.get("hadoop.pipes.executable");
+  }
+  
+  /**
+   * Set the URI for the application's executable. Normally this is a hdfs: 
+   * location.
+   * @param conf
+   * @param executable The URI of the application's executable.
+   */
+  public static void setExecutable(JobConf conf, String executable) {
+    conf.set("hadoop.pipes.executable", executable);
+  }
+
+  /**
+   * Set whether the job is using a Java RecordReader.
+   * @param conf the configuration to modify
+   * @param value the new value
+   */
+  public static void setIsJavaRecordReader(JobConf conf, boolean value) {
+    conf.setBoolean("hadoop.pipes.java.recordreader", value);
+  }
+
+  /**
+   * Check whether the job is using a Java RecordReader
+   * @param conf the configuration to check
+   * @return is it a Java RecordReader?
+   */
+  public static boolean getIsJavaRecordReader(JobConf conf) {
+    return conf.getBoolean("hadoop.pipes.java.recordreader", false);
+  }
+
+  /**
+   * Set whether the Mapper is written in Java.
+   * @param conf the configuration to modify
+   * @param value the new value
+   */
+  public static void setIsJavaMapper(JobConf conf, boolean value) {
+    conf.setBoolean("hadoop.pipes.java.mapper", value);
+  }
+
+  /**
+   * Check whether the job is using a Java Mapper.
+   * @param conf the configuration to check
+   * @return is it a Java Mapper?
+   */
+  public static boolean getIsJavaMapper(JobConf conf) {
+    return conf.getBoolean("hadoop.pipes.java.mapper", false);
+  }
+
+  /**
+   * Set whether the Reducer is written in Java.
+   * @param conf the configuration to modify
+   * @param value the new value
+   */
+  public static void setIsJavaReducer(JobConf conf, boolean value) {
+    conf.setBoolean("hadoop.pipes.java.reducer", value);
+  }
+
+  /**
+   * Check whether the job is using a Java Reducer.
+   * @param conf the configuration to check
+   * @return is it a Java Reducer?
+   */
+  public static boolean getIsJavaReducer(JobConf conf) {
+    return conf.getBoolean("hadoop.pipes.java.reducer", false);
+  }
+
+  /**
+   * Set whether the job will use a Java RecordWriter.
+   * @param conf the configuration to modify
+   * @param value the new value to set
+   */
+  public static void setIsJavaRecordWriter(JobConf conf, boolean value) {
+    conf.setBoolean("hadoop.pipes.java.recordwriter", value);
+  }
+
+  /**
+   * Will the reduce use a Java RecordWriter?
+   * @param conf the configuration to check
+   * @return true, if the output of the job will be written by Java
+   */
+  public static boolean getIsJavaRecordWriter(JobConf conf) {
+    return conf.getBoolean("hadoop.pipes.java.recordwriter", false);
+  }
+
+  /**
+   * Set the configuration, if it doesn't already have a value for the given
+   * key.
+   * @param conf the configuration to modify
+   * @param key the key to set
+   * @param value the new "default" value to set
+   */
+  private static void setIfUnset(JobConf conf, String key, String value) {
+    if (conf.get(key) == null) {
+      conf.set(key, value);
+    }
+  }
+
+  /**
+   * Save away the user's original partitioner before we override it.
+   * @param conf the configuration to modify
+   * @param cls the user's partitioner class
+   */
+  static void setJavaPartitioner(JobConf conf, Class cls) {
+    conf.set("hadoop.pipes.partitioner", cls.getName());
+  }
+  
+  /**
+   * Get the user's original partitioner.
+   * @param conf the configuration to look in
+   * @return the class that the user submitted
+   */
+  static Class<? extends Partitioner> getJavaPartitioner(JobConf conf) {
+    return conf.getClass("hadoop.pipes.partitioner", 
+                         HashPartitioner.class,
+                         Partitioner.class);
+  }
+
+  /**
+   * Does the user want to keep the command file for debugging? If this is
+   * true, pipes will write a copy of the command data to a file in the
+   * task directory named "downlink.data", which may be used to run the C++
+   * program under the debugger. You probably also want to set 
+   * JobConf.setKeepFailedTaskFiles(true) to keep the entire directory from
+   * being deleted.
+   * To run using the data file, set the environment variable 
+   * "hadoop.pipes.command.file" to point to the file.
+   * @param conf the configuration to check
+   * @return will the framework save the command file?
+   */
+  public static boolean getKeepCommandFile(JobConf conf) {
+    return conf.getBoolean("hadoop.pipes.command-file.keep", false);
+  }
+
+  /**
+   * Set whether to keep the command file for debugging
+   * @param conf the configuration to modify
+   * @param keep the new value
+   */
+  public static void setKeepCommandFile(JobConf conf, boolean keep) {
+    conf.setBoolean("hadoop.pipes.command-file.keep", keep);
+  }
+
+  /**
+   * Submit a job to the map/reduce cluster. All of the necessary modifications
+   * to the job to run under pipes are made to the configuration.
+   * @param conf the job to submit to the cluster (MODIFIED)
+   * @throws IOException
+   * @deprecated Use {@link Submitter#runJob(JobConf)}
+   */
+  @Deprecated
+  public static RunningJob submitJob(JobConf conf) throws IOException {
+    return runJob(conf);
+  }
+
+  /**
+   * Submit a job to the map/reduce cluster. All of the necessary modifications
+   * to the job to run under pipes are made to the configuration.
+   * @param conf the job to submit to the cluster (MODIFIED)
+   * @throws IOException
+   */
+  public static RunningJob runJob(JobConf conf) throws IOException {
+    setupPipesJob(conf);
+    return JobClient.runJob(conf);
+  }
+
+  /**
+   * Submit a job to the Map-Reduce framework.
+   * This returns a handle to the {@link RunningJob} which can be used to track
+   * the running-job.
+   * 
+   * @param conf the job configuration.
+   * @return a handle to the {@link RunningJob} which can be used to track the
+   *         running-job.
+   * @throws IOException
+   */
+  public static RunningJob jobSubmit(JobConf conf) throws IOException {
+    setupPipesJob(conf);
+    return new JobClient(conf).submitJob(conf);
+  }
+  
+  private static void setupPipesJob(JobConf conf) throws IOException {
+    // default map output types to Text
+    if (!getIsJavaMapper(conf)) {
+      conf.setMapRunnerClass(PipesMapRunner.class);
+      // Save the user's partitioner and hook in our's.
+      setJavaPartitioner(conf, conf.getPartitionerClass());
+      conf.setPartitionerClass(PipesPartitioner.class);
+    }
+    if (!getIsJavaReducer(conf)) {
+      conf.setReducerClass(PipesReducer.class);
+      if (!getIsJavaRecordWriter(conf)) {
+        conf.setOutputFormat(NullOutputFormat.class);
+      }
+    }
+    String textClassname = Text.class.getName();
+    setIfUnset(conf, "mapred.mapoutput.key.class", textClassname);
+    setIfUnset(conf, "mapred.mapoutput.value.class", textClassname);
+    setIfUnset(conf, "mapred.output.key.class", textClassname);
+    setIfUnset(conf, "mapred.output.value.class", textClassname);
+    
+    // Use PipesNonJavaInputFormat if necessary to handle progress reporting
+    // from C++ RecordReaders ...
+    if (!getIsJavaRecordReader(conf) && !getIsJavaMapper(conf)) {
+      conf.setClass("mapred.pipes.user.inputformat", 
+                    conf.getInputFormat().getClass(), InputFormat.class);
+      conf.setInputFormat(PipesNonJavaInputFormat.class);
+    }
+    
+    String exec = getExecutable(conf);
+    if (exec == null) {
+      throw new IllegalArgumentException("No application program defined.");
+    }
+    // add default debug script only when executable is expressed as
+    // <path>#<executable>
+    if (exec.contains("#")) {
+      DistributedCache.createSymlink(conf);
+      // set default gdb commands for map and reduce task 
+      String defScript = "$HADOOP_HOME/src/c++/pipes/debug/pipes-default-script";
+      setIfUnset(conf,"mapred.map.task.debug.script",defScript);
+      setIfUnset(conf,"mapred.reduce.task.debug.script",defScript);
+    }
+    URI[] fileCache = DistributedCache.getCacheFiles(conf);
+    if (fileCache == null) {
+      fileCache = new URI[1];
+    } else {
+      URI[] tmp = new URI[fileCache.length+1];
+      System.arraycopy(fileCache, 0, tmp, 1, fileCache.length);
+      fileCache = tmp;
+    }
+    try {
+      fileCache[0] = new URI(exec);
+    } catch (URISyntaxException e) {
+      IOException ie = new IOException("Problem parsing execable URI " + exec);
+      ie.initCause(e);
+      throw ie;
+    }
+    DistributedCache.setCacheFiles(fileCache, conf);
+  }
+
+  /**
+   * A command line parser for the CLI-based Pipes job submitter.
+   */
+  static class CommandLineParser {
+    private Options options = new Options();
+    
+    void addOption(String longName, boolean required, String description, 
+                   String paramName) {
+      Option option = OptionBuilder.withArgName(paramName).hasArgs(1).withDescription(description).isRequired(required).create(longName);
+      options.addOption(option);
+    }
+    
+    void addArgument(String name, boolean required, String description) {
+      Option option = OptionBuilder.withArgName(name).hasArgs(1).withDescription(description).isRequired(required).create();
+      options.addOption(option);
+
+    }
+
+    Parser createParser() {
+      Parser result = new BasicParser();
+      return result;
+    }
+    
+    void printUsage() {
+      // The CLI package should do this for us, but I can't figure out how
+      // to make it print something reasonable.
+      System.out.println("bin/hadoop pipes");
+      System.out.println("  [-input <path>] // Input directory");
+      System.out.println("  [-output <path>] // Output directory");
+      System.out.println("  [-jar <jar file> // jar filename");
+      System.out.println("  [-inputformat <class>] // InputFormat class");
+      System.out.println("  [-map <class>] // Java Map class");
+      System.out.println("  [-partitioner <class>] // Java Partitioner");
+      System.out.println("  [-reduce <class>] // Java Reduce class");
+      System.out.println("  [-writer <class>] // Java RecordWriter");
+      System.out.println("  [-program <executable>] // executable URI");
+      System.out.println("  [-reduces <num>] // number of reduces");
+      System.out.println();
+      GenericOptionsParser.printGenericCommandUsage(System.out);
+    }
+  }
+  
+  private static <InterfaceType> 
+  Class<? extends InterfaceType> getClass(CommandLine cl, String key, 
+                                          JobConf conf, 
+                                          Class<InterfaceType> cls
+                                         ) throws ClassNotFoundException {
+    return conf.getClassByName((String) cl.getOptionValue(key)).asSubclass(cls);
+  }
+
+  @Override
+  public int run(String[] args) throws Exception {
+    CommandLineParser cli = new CommandLineParser();
+    if (args.length == 0) {
+      cli.printUsage();
+      return 1;
+    }
+    cli.addOption("input", false, "input path to the maps", "path");
+    cli.addOption("output", false, "output path from the reduces", "path");
+    
+    cli.addOption("jar", false, "job jar file", "path");
+    cli.addOption("inputformat", false, "java classname of InputFormat", 
+                  "class");
+    //cli.addArgument("javareader", false, "is the RecordReader in Java");
+    cli.addOption("map", false, "java classname of Mapper", "class");
+    cli.addOption("partitioner", false, "java classname of Partitioner", 
+                  "class");
+    cli.addOption("reduce", false, "java classname of Reducer", "class");
+    cli.addOption("writer", false, "java classname of OutputFormat", "class");
+    cli.addOption("program", false, "URI to application executable", "class");
+    cli.addOption("reduces", false, "number of reduces", "num");
+    cli.addOption("jobconf", false, 
+        "\"n1=v1,n2=v2,..\" (Deprecated) Optional. Add or override a JobConf property.",
+        "key=val");
+    Parser parser = cli.createParser();
+    try {
+      
+      GenericOptionsParser genericParser = new GenericOptionsParser(getConf(), args);
+      CommandLine results = 
+        parser.parse(cli.options, genericParser.getRemainingArgs());
+      
+      JobConf job = new JobConf(getConf());
+      
+      if (results.hasOption("input")) {
+        FileInputFormat.setInputPaths(job, 
+                          (String) results.getOptionValue("input"));
+      }
+      if (results.hasOption("output")) {
+        FileOutputFormat.setOutputPath(job, 
+          new Path((String) results.getOptionValue("output")));
+      }
+      if (results.hasOption("jar")) {
+        job.setJar((String) results.getOptionValue("jar"));
+      }
+      if (results.hasOption("inputformat")) {
+        setIsJavaRecordReader(job, true);
+        job.setInputFormat(getClass(results, "inputformat", job,
+                                     InputFormat.class));
+      }
+      if (results.hasOption("javareader")) {
+        setIsJavaRecordReader(job, true);
+      }
+      if (results.hasOption("map")) {
+        setIsJavaMapper(job, true);
+        job.setMapperClass(getClass(results, "map", job, Mapper.class));
+      }
+      if (results.hasOption("partitioner")) {
+        job.setPartitionerClass(getClass(results, "partitioner", job,
+                                          Partitioner.class));
+      }
+      if (results.hasOption("reduce")) {
+        setIsJavaReducer(job, true);
+        job.setReducerClass(getClass(results, "reduce", job, Reducer.class));
+      }
+      if (results.hasOption("reduces")) {
+        job.setNumReduceTasks(Integer.parseInt((String) 
+                                            results.getOptionValue("reduces")));
+      }
+      if (results.hasOption("writer")) {
+        setIsJavaRecordWriter(job, true);
+        job.setOutputFormat(getClass(results, "writer", job, 
+                                      OutputFormat.class));
+      }
+      if (results.hasOption("program")) {
+        setExecutable(job, (String) results.getOptionValue("program"));
+      }
+      if (results.hasOption("jobconf")) {
+        LOG.warn("-jobconf option is deprecated, please use -D instead.");
+        String options = (String)results.getOptionValue("jobconf");
+        StringTokenizer tokenizer = new StringTokenizer(options, ",");
+        while (tokenizer.hasMoreTokens()) {
+          String keyVal = tokenizer.nextToken().trim();
+          String[] keyValSplit = keyVal.split("=", 2);
+          job.set(keyValSplit[0], keyValSplit[1]);
+        }
+      }
+      // if they gave us a jar file, include it into the class path
+      String jarFile = job.getJar();
+      if (jarFile != null) {
+        final URL[] urls = new URL[]{ FileSystem.getLocal(job).
+            pathToFile(new Path(jarFile)).toURL()};
+        //FindBugs complains that creating a URLClassLoader should be
+        //in a doPrivileged() block. 
+        ClassLoader loader =
+          AccessController.doPrivileged(
+              new PrivilegedAction<ClassLoader>() {
+                public ClassLoader run() {
+                  return new URLClassLoader(urls);
+                }
+              }
+            );
+        job.setClassLoader(loader);
+      }
+      
+      runJob(job);
+      return 0;
+    } catch (ParseException pe) {
+      LOG.info("Error : " + pe);
+      cli.printUsage();
+      return 1;
+    }
+    
+  }
+  
+  /**
+   * Submit a pipes job based on the command line arguments.
+   * @param args
+   */
+  public static void main(String[] args) throws Exception {
+    int exitCode =  new Submitter().run(args);
+    System.exit(exitCode);
+  }
+
+}
diff -urN src/hadoop-1.0.4/it/crs4/pydoop/pipes/UpwardProtocol.java src/hadoop-1.0.4.patched/it/crs4/pydoop/pipes/UpwardProtocol.java
--- src/hadoop-1.0.4/it/crs4/pydoop/pipes/UpwardProtocol.java	1970-01-01 01:00:00.000000000 +0100
+++ src/hadoop-1.0.4.patched/it/crs4/pydoop/pipes/UpwardProtocol.java	2013-02-13 23:15:48.687084967 +0100
@@ -0,0 +1,101 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package  it.crs4.pydoop.pipes;
+
+import java.io.IOException;
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.io.WritableComparable;
+
+/**
+ * The interface for the messages that can come up from the child. All of these
+ * calls are asynchronous and return before the message has been processed.
+ */
+interface UpwardProtocol<K extends WritableComparable, V extends Writable> {
+  /**
+   * Output a record from the child.
+   * @param key the record's key
+   * @param value the record's value
+   * @throws IOException
+   */
+  void output(K key, V value) throws IOException;
+  
+  /**
+   * Map functions where the application has defined a partition function
+   * output records along with their partition.
+   * @param reduce the reduce to send this record to
+   * @param key the record's key
+   * @param value the record's value
+   * @throws IOException
+   */
+  void partitionedOutput(int reduce, K key, 
+                         V value) throws IOException;
+  
+  /**
+   * Update the task's status message
+   * @param msg the string to display to the user
+   * @throws IOException
+   */
+  void status(String msg) throws IOException;
+  
+  /**
+   * Report making progress (and the current progress)
+   * @param progress the current progress (0.0 to 1.0)
+   * @throws IOException
+   */
+  void progress(float progress) throws IOException;
+  
+  /**
+   * Report that the application has finished processing all inputs 
+   * successfully.
+   * @throws IOException
+   */
+  void done() throws IOException;
+  
+  /**
+   * Report that the application or more likely communication failed.
+   * @param e
+   */
+  void failed(Throwable e);
+  
+  /**
+   * Register a counter with the given id and group/name.
+   * @param group counter group
+   * @param name counter name
+   * @throws IOException
+   */
+  void registerCounter(int id, String group, String name) throws IOException;
+  
+  /**
+   * Increment the value of a registered counter.
+   * @param id counter id of the registered counter
+   * @param amount increment for the counter value
+   * @throws IOException
+   */
+  void incrementCounter(int id, long amount) throws IOException;
+
+  /**
+   * Handles authentication response from client.
+   * It must notify the threads waiting for authentication response.
+   * @param digest
+   * @return true if authentication is successful
+   * @throws IOException
+   */
+  boolean authenticate(String digest) throws IOException;
+
+}
diff -urN src/hadoop-1.0.4/libhdfs/config.h src/hadoop-1.0.4.patched/libhdfs/config.h
--- src/hadoop-1.0.4/libhdfs/config.h	1970-01-01 01:00:00.000000000 +0100
+++ src/hadoop-1.0.4.patched/libhdfs/config.h	2013-02-13 23:15:48.687084967 +0100
@@ -0,0 +1,3 @@
+#ifndef CONFIG_H
+#define CONFIG_H
+#endif
diff -urN src/hadoop-1.0.4/libhdfs/hdfsJniHelper.c src/hadoop-1.0.4.patched/libhdfs/hdfsJniHelper.c
--- src/hadoop-1.0.4/libhdfs/hdfsJniHelper.c	2012-12-03 19:17:41.471432541 +0100
+++ src/hadoop-1.0.4.patched/libhdfs/hdfsJniHelper.c	2013-02-13 23:15:48.683751644 +0100
@@ -15,7 +15,6 @@
  */
 
 #include <string.h> 
-#include <error.h>
 #include "hdfsJniHelper.h"
 
 static pthread_mutex_t hdfsHashMutex = PTHREAD_MUTEX_INITIALIZER;
diff -urN src/hadoop-1.0.4/org/apache/hadoop/mapred/pipes/Application.java src/hadoop-1.0.4.patched/org/apache/hadoop/mapred/pipes/Application.java
--- src/hadoop-1.0.4/org/apache/hadoop/mapred/pipes/Application.java	2012-12-03 19:17:41.471432541 +0100
+++ src/hadoop-1.0.4.patched/org/apache/hadoop/mapred/pipes/Application.java	2013-02-13 23:15:48.683751644 +0100
@@ -48,6 +48,7 @@
 import org.apache.hadoop.mapred.Reporter;
 import org.apache.hadoop.mapred.TaskAttemptID;
 import org.apache.hadoop.mapred.TaskLog;
+import org.apache.hadoop.mapred.TaskTracker;
 import org.apache.hadoop.mapreduce.security.SecureShuffleUtils;
 import org.apache.hadoop.mapreduce.security.TokenCache;
 import org.apache.hadoop.mapreduce.security.token.JobTokenIdentifier;
@@ -95,14 +96,21 @@
     env.put("hadoop.pipes.command.port", 
             Integer.toString(serverSocket.getLocalPort()));
     
+    TaskAttemptID taskid = TaskAttemptID.forName(conf.get("mapred.task.id"));
+
+    // get the task's working directory
+    String workDir = TaskTracker.getLocalTaskDir(conf.getUser(),
+            taskid.getJobID().toString(),
+            taskid.getTaskID().toString());
+
     //Add token to the environment if security is enabled
     Token<JobTokenIdentifier> jobToken = TokenCache.getJobToken(conf
         .getCredentials());
     // This password is used as shared secret key between this application and
     // child pipes process
     byte[]  password = jobToken.getPassword();
-    String localPasswordFile = new File(".") + Path.SEPARATOR
-        + "jobTokenPassword";
+
+    String localPasswordFile = new File(workDir, "jobTokenPassword").getAbsolutePath();
     writePasswordToLocalFile(localPasswordFile, password, conf);
     env.put("hadoop.pipes.shared.secret.location", localPasswordFile);
  
@@ -120,7 +128,6 @@
     }
     cmd.add(executable);
     // wrap the command in a stdout/stderr capture
-    TaskAttemptID taskid = TaskAttemptID.forName(conf.get("mapred.task.id"));
     // we are starting map/reduce task of the pipes job. this is not a cleanup
     // attempt. 
     File stdout = TaskLog.getTaskLogFile(taskid, false, TaskLog.LogName.STDOUT);
diff -urN src/hadoop-1.0.4/pipes/impl/HadoopPipes.cc src/hadoop-1.0.4.patched/pipes/impl/HadoopPipes.cc
--- src/hadoop-1.0.4/pipes/impl/HadoopPipes.cc	2012-12-03 19:17:41.471432541 +0100
+++ src/hadoop-1.0.4.patched/pipes/impl/HadoopPipes.cc	2013-02-14 00:00:15.745377986 +0100
@@ -30,6 +30,7 @@
 #include <stdlib.h>
 #include <string.h>
 #include <strings.h>
+#include <unistd.h>
 #include <sys/socket.h>
 #include <pthread.h>
 #include <iostream>
@@ -126,7 +127,7 @@
     static const char lineSeparator = '\n';
 
     void writeBuffer(const string& buffer) {
-      fprintf(stream, quoteString(buffer, "\t\n").c_str());
+      fprintf(stream, "%s", quoteString(buffer, "\t\n").c_str());
     }
 
   public:
@@ -799,6 +800,7 @@
       }
       mapper = factory->createMapper(*this);
       numReduces = _numReduces;
+#if 0 // Original version
       if (numReduces != 0) { 
         reducer = factory->createCombiner(*this);
         partitioner = factory->createPartitioner(*this);
@@ -811,6 +813,23 @@
         writer = new CombineRunner(spillSize * 1024 * 1024, this, reducer, 
                                    uplink, partitioner, numReduces);
       }
+#else
+      if (numReduces != 0) { 
+        reducer = factory->createCombiner(*this);
+        partitioner = factory->createPartitioner(*this);
+
+	if (reducer != NULL) {
+	  int64_t spillSize = 100;
+	  if (jobConf->hasKey("io.sort.mb")) {
+	    spillSize = jobConf->getInt("io.sort.mb");
+	  }
+	  writer = new CombineRunner(spillSize * 1024 * 1024, this, reducer, 
+				     uplink, partitioner, numReduces);
+	}
+      } else {
+	writer = factory->createRecordWriter(*this);
+      }
+#endif
       hasTask = true;
     }
 
diff -urN src/hadoop-1.0.4/pipes/impl/HadoopPipes.cc~ src/hadoop-1.0.4.patched/pipes/impl/HadoopPipes.cc~
--- src/hadoop-1.0.4/pipes/impl/HadoopPipes.cc~	1970-01-01 01:00:00.000000000 +0100
+++ src/hadoop-1.0.4.patched/pipes/impl/HadoopPipes.cc~	2013-02-13 23:15:48.683751644 +0100
@@ -0,0 +1,1182 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "hadoop/Pipes.hh"
+#include "hadoop/SerialUtils.hh"
+#include "hadoop/StringUtils.hh"
+
+#include <map>
+#include <vector>
+
+#include <errno.h>
+#include <netinet/in.h>
+#include <stdint.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#include <strings.h>
+#include <unistd.h>
+#include <sys/socket.h>
+#include <pthread.h>
+#include <iostream>
+#include <fstream>
+
+#include <openssl/hmac.h>
+#include <openssl/buffer.h>
+
+using std::map;
+using std::string;
+using std::vector;
+
+using namespace HadoopUtils;
+
+namespace HadoopPipes {
+
+  class JobConfImpl: public JobConf {
+  private:
+    map<string, string> values;
+  public:
+    void set(const string& key, const string& value) {
+      values[key] = value;
+    }
+
+    virtual bool hasKey(const string& key) const {
+      return values.find(key) != values.end();
+    }
+
+    virtual const string& get(const string& key) const {
+      map<string,string>::const_iterator itr = values.find(key);
+      if (itr == values.end()) {
+        throw Error("Key " + key + " not found in JobConf");
+      }
+      return itr->second;
+    }
+
+    virtual int getInt(const string& key) const {
+      const string& val = get(key);
+      return toInt(val);
+    }
+
+    virtual float getFloat(const string& key) const {
+      const string& val = get(key);
+      return toFloat(val);
+    }
+
+    virtual bool getBoolean(const string&key) const {
+      const string& val = get(key);
+      return toBool(val);
+    }
+  };
+
+  class DownwardProtocol {
+  public:
+    virtual void start(int protocol) = 0;
+    virtual void setJobConf(vector<string> values) = 0;
+    virtual void setInputTypes(string keyType, string valueType) = 0;
+    virtual void runMap(string inputSplit, int numReduces, bool pipedInput)= 0;
+    virtual void mapItem(const string& key, const string& value) = 0;
+    virtual void runReduce(int reduce, bool pipedOutput) = 0;
+    virtual void reduceKey(const string& key) = 0;
+    virtual void reduceValue(const string& value) = 0;
+    virtual void close() = 0;
+    virtual void abort() = 0;
+    virtual ~DownwardProtocol() {}
+  };
+
+  class UpwardProtocol {
+  public:
+    virtual void output(const string& key, const string& value) = 0;
+    virtual void partitionedOutput(int reduce, const string& key,
+                                   const string& value) = 0;
+    virtual void status(const string& message) = 0;
+    virtual void progress(float progress) = 0;
+    virtual void done() = 0;
+    virtual void registerCounter(int id, const string& group, 
+                                 const string& name) = 0;
+    virtual void 
+      incrementCounter(const TaskContext::Counter* counter, uint64_t amount) = 0;
+    virtual ~UpwardProtocol() {}
+  };
+
+  class Protocol {
+  public:
+    virtual void nextEvent() = 0;
+    virtual UpwardProtocol* getUplink() = 0;
+    virtual ~Protocol() {}
+  };
+
+  class TextUpwardProtocol: public UpwardProtocol {
+  private:
+    FILE* stream;
+    static const char fieldSeparator = '\t';
+    static const char lineSeparator = '\n';
+
+    void writeBuffer(const string& buffer) {
+      fprintf(stream, "%s", quoteString(buffer, "\t\n").c_str());
+    }
+
+  public:
+    TextUpwardProtocol(FILE* _stream): stream(_stream) {}
+    
+    virtual void output(const string& key, const string& value) {
+      fprintf(stream, "output%c", fieldSeparator);
+      writeBuffer(key);
+      fprintf(stream, "%c", fieldSeparator);
+      writeBuffer(value);
+      fprintf(stream, "%c", lineSeparator);
+    }
+
+    virtual void partitionedOutput(int reduce, const string& key,
+                                   const string& value) {
+      fprintf(stream, "parititionedOutput%c%d%c", fieldSeparator, reduce, 
+              fieldSeparator);
+      writeBuffer(key);
+      fprintf(stream, "%c", fieldSeparator);
+      writeBuffer(value);
+      fprintf(stream, "%c", lineSeparator);
+    }
+
+    virtual void status(const string& message) {
+      fprintf(stream, "status%c%s%c", fieldSeparator, message.c_str(), 
+              lineSeparator);
+    }
+
+    virtual void progress(float progress) {
+      fprintf(stream, "progress%c%f%c", fieldSeparator, progress, 
+              lineSeparator);
+    }
+
+    virtual void registerCounter(int id, const string& group, 
+                                 const string& name) {
+      fprintf(stream, "registerCounter%c%d%c%s%c%s%c", fieldSeparator, id,
+              fieldSeparator, group.c_str(), fieldSeparator, name.c_str(), 
+              lineSeparator);
+    }
+
+    virtual void incrementCounter(const TaskContext::Counter* counter, 
+                                  uint64_t amount) {
+      fprintf(stream, "incrCounter%c%d%c%ld%c", fieldSeparator, counter->getId(), 
+              fieldSeparator, (long)amount, lineSeparator);
+    }
+    
+    virtual void done() {
+      fprintf(stream, "done%c", lineSeparator);
+    }
+  };
+
+  class TextProtocol: public Protocol {
+  private:
+    FILE* downStream;
+    DownwardProtocol* handler;
+    UpwardProtocol* uplink;
+    string key;
+    string value;
+
+    int readUpto(string& buffer, const char* limit) {
+      int ch;
+      buffer.clear();
+      while ((ch = getc(downStream)) != -1) {
+        if (strchr(limit, ch) != NULL) {
+          return ch;
+        }
+        buffer += ch;
+      }
+      return -1;
+    }
+
+    static const char* delim;
+  public:
+
+    TextProtocol(FILE* down, DownwardProtocol* _handler, FILE* up) {
+      downStream = down;
+      uplink = new TextUpwardProtocol(up);
+      handler = _handler;
+    }
+
+    UpwardProtocol* getUplink() {
+      return uplink;
+    }
+
+    virtual void nextEvent() {
+      string command;
+      string arg;
+      int sep;
+      sep = readUpto(command, delim);
+      if (command == "mapItem") {
+        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
+        sep = readUpto(key, delim);
+        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
+        sep = readUpto(value, delim);
+        HADOOP_ASSERT(sep == '\n', "Long text protocol command " + command);
+        handler->mapItem(key, value);
+      } else if (command == "reduceValue") {
+        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
+        sep = readUpto(value, delim);
+        HADOOP_ASSERT(sep == '\n', "Long text protocol command " + command);
+        handler->reduceValue(value);
+      } else if (command == "reduceKey") {
+        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
+        sep = readUpto(key, delim);
+        HADOOP_ASSERT(sep == '\n', "Long text protocol command " + command);
+        handler->reduceKey(key);
+      } else if (command == "start") {
+        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
+        sep = readUpto(arg, delim);
+        HADOOP_ASSERT(sep == '\n', "Long text protocol command " + command);
+        handler->start(toInt(arg));
+      } else if (command == "setJobConf") {
+        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
+        sep = readUpto(arg, delim);
+        int len = toInt(arg);
+        vector<string> values(len);
+        for(int i=0; i < len; ++i) {
+          HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
+          sep = readUpto(arg, delim);
+          values.push_back(arg);
+        }
+        HADOOP_ASSERT(sep == '\n', "Long text protocol command " + command);
+        handler->setJobConf(values);
+      } else if (command == "setInputTypes") {
+        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
+        sep = readUpto(key, delim);
+        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
+        sep = readUpto(value, delim);
+        HADOOP_ASSERT(sep == '\n', "Long text protocol command " + command);
+        handler->setInputTypes(key, value);
+      } else if (command == "runMap") {
+        string split;
+        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
+        sep = readUpto(split, delim);
+        string reduces;
+        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
+        sep = readUpto(reduces, delim);
+        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
+        sep = readUpto(arg, delim);
+        HADOOP_ASSERT(sep == '\n', "Long text protocol command " + command);
+        handler->runMap(split, toInt(reduces), toBool(arg));
+      } else if (command == "runReduce") {
+        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
+        sep = readUpto(arg, delim);
+        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
+        string piped;
+        sep = readUpto(piped, delim);
+        HADOOP_ASSERT(sep == '\n', "Long text protocol command " + command);
+        handler->runReduce(toInt(arg), toBool(piped));
+      } else if (command == "abort") { 
+        HADOOP_ASSERT(sep == '\n', "Long text protocol command " + command);
+        handler->abort();
+      } else if (command == "close") {
+        HADOOP_ASSERT(sep == '\n', "Long text protocol command " + command);
+        handler->close();
+      } else {
+        throw Error("Illegal text protocol command " + command);
+      }
+    }
+
+    ~TextProtocol() {
+      delete uplink;
+    }
+  };
+  const char* TextProtocol::delim = "\t\n";
+
+  enum MESSAGE_TYPE {START_MESSAGE, SET_JOB_CONF, SET_INPUT_TYPES, RUN_MAP, 
+                     MAP_ITEM, RUN_REDUCE, REDUCE_KEY, REDUCE_VALUE, 
+                     CLOSE, ABORT, AUTHENTICATION_REQ,
+                     OUTPUT=50, PARTITIONED_OUTPUT, STATUS, PROGRESS, DONE,
+                     REGISTER_COUNTER, INCREMENT_COUNTER, AUTHENTICATION_RESP};
+
+  class BinaryUpwardProtocol: public UpwardProtocol {
+  private:
+    FileOutStream* stream;
+  public:
+    BinaryUpwardProtocol(FILE* _stream) {
+      stream = new FileOutStream();
+      HADOOP_ASSERT(stream->open(_stream), "problem opening stream");
+    }
+
+    virtual void authenticate(const string &responseDigest) {
+      serializeInt(AUTHENTICATION_RESP, *stream);
+      serializeString(responseDigest, *stream);
+      stream->flush();
+    }
+
+    virtual void output(const string& key, const string& value) {
+      serializeInt(OUTPUT, *stream);
+      serializeString(key, *stream);
+      serializeString(value, *stream);
+    }
+
+    virtual void partitionedOutput(int reduce, const string& key,
+                                   const string& value) {
+      serializeInt(PARTITIONED_OUTPUT, *stream);
+      serializeInt(reduce, *stream);
+      serializeString(key, *stream);
+      serializeString(value, *stream);
+    }
+
+    virtual void status(const string& message) {
+      serializeInt(STATUS, *stream);
+      serializeString(message, *stream);
+    }
+
+    virtual void progress(float progress) {
+      serializeInt(PROGRESS, *stream);
+      serializeFloat(progress, *stream);
+      stream->flush();
+    }
+
+    virtual void done() {
+      serializeInt(DONE, *stream);
+    }
+
+    virtual void registerCounter(int id, const string& group, 
+                                 const string& name) {
+      serializeInt(REGISTER_COUNTER, *stream);
+      serializeInt(id, *stream);
+      serializeString(group, *stream);
+      serializeString(name, *stream);
+    }
+
+    virtual void incrementCounter(const TaskContext::Counter* counter, 
+                                  uint64_t amount) {
+      serializeInt(INCREMENT_COUNTER, *stream);
+      serializeInt(counter->getId(), *stream);
+      serializeLong(amount, *stream);
+    }
+    
+    ~BinaryUpwardProtocol() {
+      delete stream;
+    }
+  };
+
+  class BinaryProtocol: public Protocol {
+  private:
+    FileInStream* downStream;
+    DownwardProtocol* handler;
+    BinaryUpwardProtocol * uplink;
+    string key;
+    string value;
+    string password;
+    bool authDone;
+    void getPassword(string &password) {
+      const char *passwordFile = getenv("hadoop.pipes.shared.secret.location");
+      if (passwordFile == NULL) {
+        return;
+      }
+      std::ifstream fstr(passwordFile, std::fstream::binary);
+      if (fstr.fail()) {
+        std::cerr << "Could not open the password file" << std::endl;
+        return;
+      } 
+      unsigned char * passBuff = new unsigned char [512];
+      fstr.read((char *)passBuff, 512);
+      int passwordLength = fstr.gcount();
+      fstr.close();
+      passBuff[passwordLength] = 0;
+      password.replace(0, passwordLength, (const char *) passBuff, passwordLength);
+      delete [] passBuff;
+      return; 
+    }
+
+    void verifyDigestAndRespond(string& digest, string& challenge) {
+      if (password.empty()) {
+        //password can be empty if process is running in debug mode from
+        //command file.
+        authDone = true;
+        return;
+      }
+
+      if (!verifyDigest(password, digest, challenge)) {
+        std::cerr << "Server failed to authenticate. Exiting" << std::endl;
+        exit(-1);
+      }
+      authDone = true;
+      string responseDigest = createDigest(password, digest);
+      uplink->authenticate(responseDigest);
+    }
+
+    bool verifyDigest(string &password, string& digest, string& challenge) {
+      string expectedDigest = createDigest(password, challenge);
+      if (digest == expectedDigest) {
+        return true;
+      } else {
+        return false;
+      }
+    }
+
+    string createDigest(string &password, string& msg) {
+      HMAC_CTX ctx;
+      unsigned char digest[EVP_MAX_MD_SIZE];
+      HMAC_Init(&ctx, (const unsigned char *)password.c_str(), 
+          password.length(), EVP_sha1());
+      HMAC_Update(&ctx, (const unsigned char *)msg.c_str(), msg.length());
+      unsigned int digestLen;
+      HMAC_Final(&ctx, digest, &digestLen);
+      HMAC_cleanup(&ctx);
+
+      //now apply base64 encoding
+      BIO *bmem, *b64;
+      BUF_MEM *bptr;
+
+      b64 = BIO_new(BIO_f_base64());
+      bmem = BIO_new(BIO_s_mem());
+      b64 = BIO_push(b64, bmem);
+      BIO_write(b64, digest, digestLen);
+      BIO_flush(b64);
+      BIO_get_mem_ptr(b64, &bptr);
+
+      char digestBuffer[bptr->length];
+      memcpy(digestBuffer, bptr->data, bptr->length-1);
+      digestBuffer[bptr->length-1] = 0;
+      BIO_free_all(b64);
+
+      return string(digestBuffer);
+    }
+
+  public:
+    BinaryProtocol(FILE* down, DownwardProtocol* _handler, FILE* up) {
+      downStream = new FileInStream();
+      downStream->open(down);
+      uplink = new BinaryUpwardProtocol(up);
+      handler = _handler;
+      authDone = false;
+      getPassword(password);
+    }
+
+    UpwardProtocol* getUplink() {
+      return uplink;
+    }
+
+    virtual void nextEvent() {
+      int32_t cmd;
+      cmd = deserializeInt(*downStream);
+      if (!authDone && cmd != AUTHENTICATION_REQ) {
+        //Authentication request must be the first message if
+        //authentication is not complete
+        std::cerr << "Command:" << cmd << "received before authentication. " 
+            << "Exiting.." << std::endl;
+        exit(-1);
+      }
+      switch (cmd) {
+      case AUTHENTICATION_REQ: {
+        string digest;
+        string challenge;
+        deserializeString(digest, *downStream);
+        deserializeString(challenge, *downStream);
+        verifyDigestAndRespond(digest, challenge);
+        break;
+      }
+      case START_MESSAGE: {
+        int32_t prot;
+        prot = deserializeInt(*downStream);
+        handler->start(prot);
+        break;
+      }
+      case SET_JOB_CONF: {
+        int32_t entries;
+        entries = deserializeInt(*downStream);
+        vector<string> result(entries);
+        for(int i=0; i < entries; ++i) {
+          string item;
+          deserializeString(item, *downStream);
+          result.push_back(item);
+        }
+        handler->setJobConf(result);
+        break;
+      }
+      case SET_INPUT_TYPES: {
+        string keyType;
+        string valueType;
+        deserializeString(keyType, *downStream);
+        deserializeString(valueType, *downStream);
+        handler->setInputTypes(keyType, valueType);
+        break;
+      }
+      case RUN_MAP: {
+        string split;
+        int32_t numReduces;
+        int32_t piped;
+        deserializeString(split, *downStream);
+        numReduces = deserializeInt(*downStream);
+        piped = deserializeInt(*downStream);
+        handler->runMap(split, numReduces, piped);
+        break;
+      }
+      case MAP_ITEM: {
+        deserializeString(key, *downStream);
+        deserializeString(value, *downStream);
+        handler->mapItem(key, value);
+        break;
+      }
+      case RUN_REDUCE: {
+        int32_t reduce;
+        int32_t piped;
+        reduce = deserializeInt(*downStream);
+        piped = deserializeInt(*downStream);
+        handler->runReduce(reduce, piped);
+        break;
+      }
+      case REDUCE_KEY: {
+        deserializeString(key, *downStream);
+        handler->reduceKey(key);
+        break;
+      }
+      case REDUCE_VALUE: {
+        deserializeString(value, *downStream);
+        handler->reduceValue(value);
+        break;
+      }
+      case CLOSE:
+        handler->close();
+        break;
+      case ABORT:
+        handler->abort();
+        break;
+      default:
+        HADOOP_ASSERT(false, "Unknown binary command " + toString(cmd));
+      }
+    }
+
+    virtual ~BinaryProtocol() {
+      delete downStream;
+      delete uplink;
+    }
+  };
+
+  /**
+   * Define a context object to give to combiners that will let them
+   * go through the values and emit their results correctly.
+   */
+  class CombineContext: public ReduceContext {
+  private:
+    ReduceContext* baseContext;
+    Partitioner* partitioner;
+    int numReduces;
+    UpwardProtocol* uplink;
+    bool firstKey;
+    bool firstValue;
+    map<string, vector<string> >::iterator keyItr;
+    map<string, vector<string> >::iterator endKeyItr;
+    vector<string>::iterator valueItr;
+    vector<string>::iterator endValueItr;
+
+  public:
+    CombineContext(ReduceContext* _baseContext,
+                   Partitioner* _partitioner,
+                   int _numReduces,
+                   UpwardProtocol* _uplink,
+                   map<string, vector<string> >& data) {
+      baseContext = _baseContext;
+      partitioner = _partitioner;
+      numReduces = _numReduces;
+      uplink = _uplink;
+      keyItr = data.begin();
+      endKeyItr = data.end();
+      firstKey = true;
+      firstValue = true;
+    }
+
+    virtual const JobConf* getJobConf() {
+      return baseContext->getJobConf();
+    }
+
+    virtual const std::string& getInputKey() {
+      return keyItr->first;
+    }
+
+    virtual const std::string& getInputValue() {
+      return *valueItr;
+    }
+
+    virtual void emit(const std::string& key, const std::string& value) {
+      if (partitioner != NULL) {
+        uplink->partitionedOutput(partitioner->partition(key, numReduces),
+                                  key, value);
+      } else {
+        uplink->output(key, value);
+      }
+    }
+
+    virtual void progress() {
+      baseContext->progress();
+    }
+
+    virtual void setStatus(const std::string& status) {
+      baseContext->setStatus(status);
+    }
+
+    bool nextKey() {
+      if (firstKey) {
+        firstKey = false;
+      } else {
+        ++keyItr;
+      }
+      if (keyItr != endKeyItr) {
+        valueItr = keyItr->second.begin();
+        endValueItr = keyItr->second.end();
+        firstValue = true;
+        return true;
+      }
+      return false;
+    }
+
+    virtual bool nextValue() {
+      if (firstValue) {
+        firstValue = false;
+      } else {
+        ++valueItr;
+      }
+      return valueItr != endValueItr;
+    }
+    
+    virtual Counter* getCounter(const std::string& group, 
+                               const std::string& name) {
+      return baseContext->getCounter(group, name);
+    }
+
+    virtual void incrementCounter(const Counter* counter, uint64_t amount) {
+      baseContext->incrementCounter(counter, amount);
+    }
+  };
+
+  /**
+   * A RecordWriter that will take the map outputs, buffer them up and then
+   * combine then when the buffer is full.
+   */
+  class CombineRunner: public RecordWriter {
+  private:
+    map<string, vector<string> > data;
+    int64_t spillSize;
+    int64_t numBytes;
+    ReduceContext* baseContext;
+    Partitioner* partitioner;
+    int numReduces;
+    UpwardProtocol* uplink;
+    Reducer* combiner;
+  public:
+    CombineRunner(int64_t _spillSize, ReduceContext* _baseContext, 
+                  Reducer* _combiner, UpwardProtocol* _uplink, 
+                  Partitioner* _partitioner, int _numReduces) {
+      numBytes = 0;
+      spillSize = _spillSize;
+      baseContext = _baseContext;
+      partitioner = _partitioner;
+      numReduces = _numReduces;
+      uplink = _uplink;
+      combiner = _combiner;
+    }
+
+    virtual void emit(const std::string& key,
+                      const std::string& value) {
+      numBytes += key.length() + value.length();
+      data[key].push_back(value);
+      if (numBytes >= spillSize) {
+        spillAll();
+      }
+    }
+
+    virtual void close() {
+      spillAll();
+    }
+
+  private:
+    void spillAll() {
+      CombineContext context(baseContext, partitioner, numReduces, 
+                             uplink, data);
+      while (context.nextKey()) {
+        combiner->reduce(context);
+      }
+      data.clear();
+      numBytes = 0;
+    }
+  };
+
+  class TaskContextImpl: public MapContext, public ReduceContext, 
+                         public DownwardProtocol {
+  private:
+    bool done;
+    JobConf* jobConf;
+    string key;
+    const string* newKey;
+    const string* value;
+    bool hasTask;
+    bool isNewKey;
+    bool isNewValue;
+    string* inputKeyClass;
+    string* inputValueClass;
+    string status;
+    float progressFloat;
+    uint64_t lastProgress;
+    bool statusSet;
+    Protocol* protocol;
+    UpwardProtocol *uplink;
+    string* inputSplit;
+    RecordReader* reader;
+    Mapper* mapper;
+    Reducer* reducer;
+    RecordWriter* writer;
+    Partitioner* partitioner;
+    int numReduces;
+    const Factory* factory;
+    pthread_mutex_t mutexDone;
+    std::vector<int> registeredCounterIds;
+
+  public:
+
+    TaskContextImpl(const Factory& _factory) {
+      statusSet = false;
+      done = false;
+      newKey = NULL;
+      factory = &_factory;
+      jobConf = NULL;
+      inputKeyClass = NULL;
+      inputValueClass = NULL;
+      inputSplit = NULL;
+      mapper = NULL;
+      reducer = NULL;
+      reader = NULL;
+      writer = NULL;
+      partitioner = NULL;
+      protocol = NULL;
+      isNewKey = false;
+      isNewValue = false;
+      lastProgress = 0;
+      progressFloat = 0.0f;
+      hasTask = false;
+      pthread_mutex_init(&mutexDone, NULL);
+    }
+
+    void setProtocol(Protocol* _protocol, UpwardProtocol* _uplink) {
+
+      protocol = _protocol;
+      uplink = _uplink;
+    }
+
+    virtual void start(int protocol) {
+      if (protocol != 0) {
+        throw Error("Protocol version " + toString(protocol) + 
+                    " not supported");
+      }
+    }
+
+    virtual void setJobConf(vector<string> values) {
+      int len = values.size();
+      JobConfImpl* result = new JobConfImpl();
+      HADOOP_ASSERT(len % 2 == 0, "Odd length of job conf values");
+      for(int i=0; i < len; i += 2) {
+        result->set(values[i], values[i+1]);
+      }
+      jobConf = result;
+    }
+
+    virtual void setInputTypes(string keyType, string valueType) {
+      inputKeyClass = new string(keyType);
+      inputValueClass = new string(valueType);
+    }
+
+    virtual void runMap(string _inputSplit, int _numReduces, bool pipedInput) {
+      inputSplit = new string(_inputSplit);
+      reader = factory->createRecordReader(*this);
+      HADOOP_ASSERT((reader == NULL) == pipedInput,
+                    pipedInput ? "RecordReader defined when not needed.":
+                    "RecordReader not defined");
+      if (reader != NULL) {
+        value = new string();
+      }
+      mapper = factory->createMapper(*this);
+      numReduces = _numReduces;
+      if (numReduces != 0) { 
+        reducer = factory->createCombiner(*this);
+        partitioner = factory->createPartitioner(*this);
+      }
+      if (reducer != NULL) {
+        int64_t spillSize = 100;
+        if (jobConf->hasKey("io.sort.mb")) {
+          spillSize = jobConf->getInt("io.sort.mb");
+        }
+        writer = new CombineRunner(spillSize * 1024 * 1024, this, reducer, 
+                                   uplink, partitioner, numReduces);
+      }
+      hasTask = true;
+    }
+
+    virtual void mapItem(const string& _key, const string& _value) {
+      newKey = &_key;
+      value = &_value;
+      isNewKey = true;
+    }
+
+    virtual void runReduce(int reduce, bool pipedOutput) {
+      reducer = factory->createReducer(*this);
+      writer = factory->createRecordWriter(*this);
+      HADOOP_ASSERT((writer == NULL) == pipedOutput,
+                    pipedOutput ? "RecordWriter defined when not needed.":
+                    "RecordWriter not defined");
+      hasTask = true;
+    }
+
+    virtual void reduceKey(const string& _key) {
+      isNewKey = true;
+      newKey = &_key;
+    }
+
+    virtual void reduceValue(const string& _value) {
+      isNewValue = true;
+      value = &_value;
+    }
+    
+    virtual bool isDone() {
+      pthread_mutex_lock(&mutexDone);
+      bool doneCopy = done;
+      pthread_mutex_unlock(&mutexDone);
+      return doneCopy;
+    }
+
+    virtual void close() {
+      pthread_mutex_lock(&mutexDone);
+      done = true;
+      pthread_mutex_unlock(&mutexDone);
+    }
+
+    virtual void abort() {
+      throw Error("Aborted by driver");
+    }
+
+    void waitForTask() {
+      while (!done && !hasTask) {
+        protocol->nextEvent();
+      }
+    }
+
+    bool nextKey() {
+      if (reader == NULL) {
+        while (!isNewKey) {
+          nextValue();
+          if (done) {
+            return false;
+          }
+        }
+        key = *newKey;
+      } else {
+        if (!reader->next(key, const_cast<string&>(*value))) {
+          pthread_mutex_lock(&mutexDone);
+          done = true;
+          pthread_mutex_unlock(&mutexDone);
+          return false;
+        }
+        progressFloat = reader->getProgress();
+      }
+      isNewKey = false;
+      if (mapper != NULL) {
+        mapper->map(*this);
+      } else {
+        reducer->reduce(*this);
+      }
+      return true;
+    }
+
+    /**
+     * Advance to the next value.
+     */
+    virtual bool nextValue() {
+      if (isNewKey || done) {
+        return false;
+      }
+      isNewValue = false;
+      progress();
+      protocol->nextEvent();
+      return isNewValue;
+    }
+
+    /**
+     * Get the JobConf for the current task.
+     */
+    virtual JobConf* getJobConf() {
+      return jobConf;
+    }
+
+    /**
+     * Get the current key. 
+     * @return the current key or NULL if called before the first map or reduce
+     */
+    virtual const string& getInputKey() {
+      return key;
+    }
+
+    /**
+     * Get the current value. 
+     * @return the current value or NULL if called before the first map or 
+     *    reduce
+     */
+    virtual const string& getInputValue() {
+      return *value;
+    }
+
+    /**
+     * Mark your task as having made progress without changing the status 
+     * message.
+     */
+    virtual void progress() {
+      if (uplink != 0) {
+        uint64_t now = getCurrentMillis();
+        if (now - lastProgress > 1000) {
+          lastProgress = now;
+          if (statusSet) {
+            uplink->status(status);
+            statusSet = false;
+          }
+          uplink->progress(progressFloat);
+        }
+      }
+    }
+
+    /**
+     * Set the status message and call progress.
+     */
+    virtual void setStatus(const string& status) {
+      this->status = status;
+      statusSet = true;
+      progress();
+    }
+
+    /**
+     * Get the name of the key class of the input to this task.
+     */
+    virtual const string& getInputKeyClass() {
+      return *inputKeyClass;
+    }
+
+    /**
+     * Get the name of the value class of the input to this task.
+     */
+    virtual const string& getInputValueClass() {
+      return *inputValueClass;
+    }
+
+    /**
+     * Access the InputSplit of the mapper.
+     */
+    virtual const std::string& getInputSplit() {
+      return *inputSplit;
+    }
+
+    virtual void emit(const string& key, const string& value) {
+      progress();
+      if (writer != NULL) {
+        writer->emit(key, value);
+      } else if (partitioner != NULL) {
+        int part = partitioner->partition(key, numReduces);
+        uplink->partitionedOutput(part, key, value);
+      } else {
+        uplink->output(key, value);
+      }
+    }
+
+    /**
+     * Register a counter with the given group and name.
+     */
+    virtual Counter* getCounter(const std::string& group, 
+                               const std::string& name) {
+      int id = registeredCounterIds.size();
+      registeredCounterIds.push_back(id);
+      uplink->registerCounter(id, group, name);
+      return new Counter(id);
+    }
+
+    /**
+     * Increment the value of the counter with the given amount.
+     */
+    virtual void incrementCounter(const Counter* counter, uint64_t amount) {
+      uplink->incrementCounter(counter, amount); 
+    }
+
+    void closeAll() {
+      if (reader) {
+        reader->close();
+      }
+      if (mapper) {
+        mapper->close();
+      }
+      if (reducer) {
+        reducer->close();
+      }
+      if (writer) {
+        writer->close();
+      }
+    }
+
+    virtual ~TaskContextImpl() {
+      delete jobConf;
+      delete inputKeyClass;
+      delete inputValueClass;
+      delete inputSplit;
+      if (reader) {
+        delete value;
+      }
+      delete reader;
+      delete mapper;
+      delete reducer;
+      delete writer;
+      delete partitioner;
+      pthread_mutex_destroy(&mutexDone);
+    }
+  };
+
+  /**
+   * Ping the parent every 5 seconds to know if it is alive 
+   */
+  void* ping(void* ptr) {
+    TaskContextImpl* context = (TaskContextImpl*) ptr;
+    char* portStr = getenv("hadoop.pipes.command.port");
+    int MAX_RETRIES = 3;
+    int remaining_retries = MAX_RETRIES;
+    while (!context->isDone()) {
+      try{
+        sleep(5);
+        int sock = -1;
+        if (portStr) {
+          sock = socket(PF_INET, SOCK_STREAM, 0);
+          HADOOP_ASSERT(sock != - 1,
+                        string("problem creating socket: ") + strerror(errno));
+          sockaddr_in addr;
+          addr.sin_family = AF_INET;
+          addr.sin_port = htons(toInt(portStr));
+          addr.sin_addr.s_addr = htonl(INADDR_LOOPBACK);
+          HADOOP_ASSERT(connect(sock, (sockaddr*) &addr, sizeof(addr)) == 0,
+                        string("problem connecting command socket: ") +
+                        strerror(errno));
+
+        }
+        if (sock != -1) {
+          int result = shutdown(sock, SHUT_RDWR);
+          HADOOP_ASSERT(result == 0, "problem shutting socket");
+          result = close(sock);
+          HADOOP_ASSERT(result == 0, "problem closing socket");
+        }
+        remaining_retries = MAX_RETRIES;
+      } catch (Error& err) {
+        if (!context->isDone()) {
+          fprintf(stderr, "Hadoop Pipes Exception: in ping %s\n", 
+                err.getMessage().c_str());
+          remaining_retries -= 1;
+          if (remaining_retries == 0) {
+            exit(1);
+          }
+        } else {
+          return NULL;
+        }
+      }
+    }
+    return NULL;
+  }
+
+  /**
+   * Run the assigned task in the framework.
+   * The user's main function should set the various functions using the 
+   * set* functions above and then call this.
+   * @return true, if the task succeeded.
+   */
+  bool runTask(const Factory& factory) {
+    try {
+      TaskContextImpl* context = new TaskContextImpl(factory);
+      Protocol* connection;
+      char* portStr = getenv("hadoop.pipes.command.port");
+      int sock = -1;
+      FILE* stream = NULL;
+      FILE* outStream = NULL;
+      char *bufin = NULL;
+      char *bufout = NULL;
+      if (portStr) {
+        sock = socket(PF_INET, SOCK_STREAM, 0);
+        HADOOP_ASSERT(sock != - 1,
+                      string("problem creating socket: ") + strerror(errno));
+        sockaddr_in addr;
+        addr.sin_family = AF_INET;
+        addr.sin_port = htons(toInt(portStr));
+        addr.sin_addr.s_addr = htonl(INADDR_LOOPBACK);
+        HADOOP_ASSERT(connect(sock, (sockaddr*) &addr, sizeof(addr)) == 0,
+                      string("problem connecting command socket: ") +
+                      strerror(errno));
+
+        stream = fdopen(sock, "r");
+        outStream = fdopen(sock, "w");
+
+        // increase buffer size
+        int bufsize = 128*1024;
+        int setbuf;
+        bufin = new char[bufsize];
+        bufout = new char[bufsize];
+        setbuf = setvbuf(stream, bufin, _IOFBF, bufsize);
+        HADOOP_ASSERT(setbuf == 0, string("problem with setvbuf for inStream: ")
+                                     + strerror(errno));
+        setbuf = setvbuf(outStream, bufout, _IOFBF, bufsize);
+        HADOOP_ASSERT(setbuf == 0, string("problem with setvbuf for outStream: ")
+                                     + strerror(errno));
+        connection = new BinaryProtocol(stream, context, outStream);
+      } else if (getenv("hadoop.pipes.command.file")) {
+        char* filename = getenv("hadoop.pipes.command.file");
+        string outFilename = filename;
+        outFilename += ".out";
+        stream = fopen(filename, "r");
+        outStream = fopen(outFilename.c_str(), "w");
+        connection = new BinaryProtocol(stream, context, outStream);
+      } else {
+        connection = new TextProtocol(stdin, context, stdout);
+      }
+      context->setProtocol(connection, connection->getUplink());
+      pthread_t pingThread;
+      pthread_create(&pingThread, NULL, ping, (void*)(context));
+      context->waitForTask();
+      while (!context->isDone()) {
+        context->nextKey();
+      }
+      context->closeAll();
+      connection->getUplink()->done();
+      pthread_join(pingThread,NULL);
+      delete context;
+      delete connection;
+      if (stream != NULL) {
+        fflush(stream);
+      }
+      if (outStream != NULL) {
+        fflush(outStream);
+      }
+      fflush(stdout);
+      if (sock != -1) {
+        int result = shutdown(sock, SHUT_RDWR);
+        HADOOP_ASSERT(result == 0, "problem shutting socket");
+        result = close(sock);
+        HADOOP_ASSERT(result == 0, "problem closing socket");
+      }
+      if (stream != NULL) {
+        //fclose(stream);
+      }
+      if (outStream != NULL) {
+        //fclose(outStream);
+      } 
+      delete bufin;
+      delete bufout;
+      return true;
+    } catch (Error& err) {
+      fprintf(stderr, "Hadoop Pipes Exception: %s\n", 
+              err.getMessage().c_str());
+      return false;
+    }
+  }
+}
+
diff -urN src/hadoop-1.0.4/utils/impl/SerialUtils.cc src/hadoop-1.0.4.patched/utils/impl/SerialUtils.cc
--- src/hadoop-1.0.4/utils/impl/SerialUtils.cc	2012-12-03 19:17:41.474765871 +0100
+++ src/hadoop-1.0.4.patched/utils/impl/SerialUtils.cc	2013-02-13 23:15:48.687084967 +0100
@@ -19,6 +19,7 @@
 #include "hadoop/StringUtils.hh"
 
 #include <errno.h>
+#include <stdint.h>
 #include <rpc/types.h>
 #include <rpc/xdr.h>
 #include <string>
@@ -252,13 +253,15 @@
     stream.write(buf, sizeof(float));
   }
 
-  void deserializeFloat(float& t, InStream& stream)
+  float deserializeFloat(InStream& stream)
   {
+    float t;
     char buf[sizeof(float)];
     stream.read(buf, sizeof(float));
     XDR xdrs;
     xdrmem_create(&xdrs, buf, sizeof(float), XDR_DECODE);
     xdr_float(&xdrs, &t);
+    return t;
   }
 
   void serializeString(const std::string& t, OutStream& stream)
